{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a261348-3abc-49da-aa9e-773b5b78c024",
   "metadata": {},
   "source": [
    "### General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2130f56d-dd15-4519-ae64-7905fa9ddf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import abs1\n",
    "import itertools\n",
    "import os\n",
    "import more_itertools\n",
    "import sys\n",
    "import time\n",
    "from absl import flags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7057deeb-ce61-4126-b15a-21a10684817b",
   "metadata": {},
   "source": [
    "## Tensorflow and keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de4a64c1-14b1-40d1-a40c-cd23c1ecde6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import keras\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "tf.compat.v1.enable_v2_behavior()\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub \n",
    "from keras.applications.resnet_v2 import ResNet101V2\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.layers import Input, GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import (Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, UpSampling2D, \n",
    "                          concatenate, GlobalAveragePooling2D, Input)\n",
    "from keras.regularizers import l2, l1_l2\n",
    "from keras.constraints import MaxNorm\n",
    "from keras.models import load_model, Sequential, Model\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet_v2 import ResNet50V2, ResNet152V2\n",
    "from keras.applications.xception import Xception\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.callbacks import Callback, EarlyStopping, LearningRateScheduler, ModelCheckpoint, LearningRateScheduler, TensorBoard\n",
    "import sklearn as sk \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fae050-4b87-41a3-872c-5fa21bda5b21",
   "metadata": {},
   "source": [
    "## SimCLRv2 colab fine tuning code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26fece07-762f-493d-9947-b16426dd8f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS_color_jitter_strength = 0.3\n",
    "CROP_PROPORTION = 0.875  # Standard for ImageNet.\n",
    "\n",
    "\n",
    "def random_apply(func, p, x):\n",
    "  \"\"\"Randomly apply function func to x with probability p.\"\"\"\n",
    "  return tf.cond(\n",
    "      tf.less(tf.random_uniform([], minval=0, maxval=1, dtype=tf.float32),\n",
    "              tf.cast(p, tf.float32)),\n",
    "      lambda: func(x),\n",
    "      lambda: x)\n",
    "\n",
    "\n",
    "def random_brightness(image, max_delta, impl='simclrv2'):\n",
    "  \"\"\"A multiplicative vs additive change of brightness.\"\"\"\n",
    "  if impl == 'simclrv2':\n",
    "    factor = tf.random_uniform(\n",
    "        [], tf.maximum(1.0 - max_delta, 0), 1.0 + max_delta)\n",
    "    image = image * factor\n",
    "  elif impl == 'simclrv1':\n",
    "    image = random_brightness(image, max_delta=max_delta)\n",
    "  else:\n",
    "    raise ValueError('Unknown impl {} for random brightness.'.format(impl))\n",
    "  return image\n",
    "\n",
    "\n",
    "def to_grayscale(image, keep_channels=True):\n",
    "  image = tf.image.rgb_to_grayscale(image)\n",
    "  if keep_channels:\n",
    "    image = tf.tile(image, [1, 1, 3])\n",
    "  return image\n",
    "\n",
    "\n",
    "def color_jitter(image,\n",
    "                 strength,\n",
    "                 random_order=True):\n",
    "  \"\"\"Distorts the color of the image.\n",
    "  Args:\n",
    "    image: The input image tensor.\n",
    "    strength: the floating number for the strength of the color augmentation.\n",
    "    random_order: A bool, specifying whether to randomize the jittering order.\n",
    "  Returns:\n",
    "    The distorted image tensor.\n",
    "  \"\"\"\n",
    "  brightness = 0.8 * strength\n",
    "  contrast = 0.8 * strength\n",
    "  saturation = 0.8 * strength\n",
    "  hue = 0.2 * strength\n",
    "  if random_order:\n",
    "    return color_jitter_rand(image, brightness, contrast, saturation, hue)\n",
    "  else:\n",
    "    return color_jitter_nonrand(image, brightness, contrast, saturation, hue)\n",
    "\n",
    "\n",
    "def color_jitter_nonrand(image, brightness=0, contrast=0, saturation=0, hue=0):\n",
    "  \"\"\"Distorts the color of the image (jittering order is fixed).\n",
    "  Args:\n",
    "    image: The input image tensor.\n",
    "    brightness: A float, specifying the brightness for color jitter.\n",
    "    contrast: A float, specifying the contrast for color jitter.\n",
    "    saturation: A float, specifying the saturation for color jitter.\n",
    "    hue: A float, specifying the hue for color jitter.\n",
    "  Returns:\n",
    "    The distorted image tensor.\n",
    "  \"\"\"\n",
    "  with tf.name_scope('distort_color'):\n",
    "    def apply_transform(i, x, brightness, contrast, saturation, hue):\n",
    "      \"\"\"Apply the i-th transformation.\"\"\"\n",
    "      if brightness != 0 and i == 0:\n",
    "        x = random_brightness(x, max_delta=brightness)\n",
    "      elif contrast != 0 and i == 1:\n",
    "        x = tf.image.random_contrast(\n",
    "            x, lower=1-contrast, upper=1+contrast)\n",
    "      elif saturation != 0 and i == 2:\n",
    "        x = tf.image.random_saturation(\n",
    "            x, lower=1-saturation, upper=1+saturation)\n",
    "      elif hue != 0:\n",
    "        x = tf.image.random_hue(x, max_delta=hue)\n",
    "      return x\n",
    "\n",
    "    for i in range(4):\n",
    "      image = apply_transform(i, image, brightness, contrast, saturation, hue)\n",
    "      image = tf.clip_by_value(image, 0., 1.)\n",
    "    return image\n",
    "\n",
    "\n",
    "def color_jitter_rand(image, brightness=0, contrast=0, saturation=0, hue=0):\n",
    "  \"\"\"Distorts the color of the image (jittering order is random).\n",
    "  Args:\n",
    "    image: The input image tensor.\n",
    "    brightness: A float, specifying the brightness for color jitter.\n",
    "    contrast: A float, specifying the contrast for color jitter.\n",
    "    saturation: A float, specifying the saturation for color jitter.\n",
    "    hue: A float, specifying the hue for color jitter.\n",
    "  Returns:\n",
    "    The distorted image tensor.\n",
    "  \"\"\"\n",
    "  with tf.name_scope('distort_color'):\n",
    "    def apply_transform(i, x):\n",
    "      \"\"\"Apply the i-th transformation.\"\"\"\n",
    "      def brightness_foo():\n",
    "        if brightness == 0:\n",
    "          return x\n",
    "        else:\n",
    "          return random_brightness(x, max_delta=brightness)\n",
    "      def contrast_foo():\n",
    "        if contrast == 0:\n",
    "          return x\n",
    "        else:\n",
    "          return tf.image.random_contrast(x, lower=1-contrast, upper=1+contrast)\n",
    "      def saturation_foo():\n",
    "        if saturation == 0:\n",
    "          return x\n",
    "        else:\n",
    "          return tf.image.random_saturation(\n",
    "              x, lower=1-saturation, upper=1+saturation)\n",
    "      def hue_foo():\n",
    "        if hue == 0:\n",
    "          return x\n",
    "        else:\n",
    "          return tf.image.random_hue(x, max_delta=hue)\n",
    "      x = tf.cond(tf.less(i, 2),\n",
    "                  lambda: tf.cond(tf.less(i, 1), brightness_foo, contrast_foo),\n",
    "                  lambda: tf.cond(tf.less(i, 3), saturation_foo, hue_foo))\n",
    "      return x\n",
    "\n",
    "    perm = tf.random_shuffle(tf.range(4))\n",
    "    for i in range(4):\n",
    "      image = apply_transform(perm[i], image)\n",
    "      image = tf.clip_by_value(image, 0., 1.)\n",
    "    return image\n",
    "\n",
    "\n",
    "def _compute_crop_shape(\n",
    "    image_height, image_width, aspect_ratio, crop_proportion):\n",
    "  \"\"\"Compute aspect ratio-preserving shape for central crop.\n",
    "  The resulting shape retains `crop_proportion` along one side and a proportion\n",
    "  less than or equal to `crop_proportion` along the other side.\n",
    "  Args:\n",
    "    image_height: Height of image to be cropped.\n",
    "    image_width: Width of image to be cropped.\n",
    "    aspect_ratio: Desired aspect ratio (width / height) of output.\n",
    "    crop_proportion: Proportion of image to retain along the less-cropped side.\n",
    "  Returns:\n",
    "    crop_height: Height of image after cropping.\n",
    "    crop_width: Width of image after cropping.\n",
    "  \"\"\"\n",
    "  image_width_float = tf.cast(image_width, tf.float32)\n",
    "  image_height_float = tf.cast(image_height, tf.float32)\n",
    "\n",
    "  def _requested_aspect_ratio_wider_than_image():\n",
    "    crop_height = tf.cast(tf.math.rint(\n",
    "        crop_proportion / aspect_ratio * image_width_float), tf.int32)\n",
    "    crop_width = tf.cast(tf.math.rint(\n",
    "        crop_proportion * image_width_float), tf.int32)\n",
    "    return crop_height, crop_width\n",
    "\n",
    "  def _image_wider_than_requested_aspect_ratio():\n",
    "    crop_height = tf.cast(\n",
    "        tf.math.rint(crop_proportion * image_height_float), tf.int32)\n",
    "    crop_width = tf.cast(tf.math.rint(\n",
    "        crop_proportion * aspect_ratio *\n",
    "        image_height_float), tf.int32)\n",
    "    return crop_height, crop_width\n",
    "\n",
    "  return tf.cond(\n",
    "      aspect_ratio > image_width_float / image_height_float,\n",
    "      _requested_aspect_ratio_wider_than_image,\n",
    "      _image_wider_than_requested_aspect_ratio)\n",
    "\n",
    "\n",
    "def center_crop(image, height, width, crop_proportion):\n",
    "  \"\"\"Crops to center of image and rescales to desired size.\n",
    "  Args:\n",
    "    image: Image Tensor to crop.\n",
    "    height: Height of image to be cropped.\n",
    "    width: Width of image to be cropped.\n",
    "    crop_proportion: Proportion of image to retain along the less-cropped side.\n",
    "  Returns:\n",
    "    A `height` x `width` x channels Tensor holding a central crop of `image`.\n",
    "  \"\"\"\n",
    "  shape = tf.shape(image)\n",
    "  image_height = shape[0]\n",
    "  image_width = shape[1]\n",
    "  crop_height, crop_width = _compute_crop_shape(\n",
    "      image_height, image_width, height / width, crop_proportion)\n",
    "  offset_height = ((image_height - crop_height) + 1) // 2\n",
    "  offset_width = ((image_width - crop_width) + 1) // 2\n",
    "  image = tf.image.crop_to_bounding_box(\n",
    "      image, offset_height, offset_width, crop_height, crop_width)\n",
    "\n",
    "  image = tf.compat.v1.image.resize_bicubic([image], [height, width])[0]\n",
    "\n",
    "  return image\n",
    "\n",
    "\n",
    "def distorted_bounding_box_crop(image,\n",
    "                                bbox,\n",
    "                                min_object_covered=0.1,\n",
    "                                aspect_ratio_range=(0.75, 1.33),\n",
    "                                area_range=(0.05, 1.0),\n",
    "                                max_attempts=100,\n",
    "                                scope=None):\n",
    "  \"\"\"Generates cropped_image using one of the bboxes randomly distorted.\n",
    "  See `tf.image.sample_distorted_bounding_box` for more documentation.\n",
    "  Args:\n",
    "    image: `Tensor` of image data.\n",
    "    bbox: `Tensor` of bounding boxes arranged `[1, num_boxes, coords]`\n",
    "        where each coordinate is [0, 1) and the coordinates are arranged\n",
    "        as `[ymin, xmin, ymax, xmax]`. If num_boxes is 0 then use the whole\n",
    "        image.\n",
    "    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n",
    "        area of the image must contain at least this fraction of any bounding\n",
    "        box supplied.\n",
    "    aspect_ratio_range: An optional list of `float`s. The cropped area of the\n",
    "        image must have an aspect ratio = width / height within this range.\n",
    "    area_range: An optional list of `float`s. The cropped area of the image\n",
    "        must contain a fraction of the supplied image within in this range.\n",
    "    max_attempts: An optional `int`. Number of attempts at generating a cropped\n",
    "        region of the image of the specified constraints. After `max_attempts`\n",
    "        failures, return the entire image.\n",
    "    scope: Optional `str` for name scope.\n",
    "  Returns:\n",
    "    (cropped image `Tensor`, distorted bbox `Tensor`).\n",
    "  \"\"\"\n",
    "  with tf.name_scope(scope, 'distorted_bounding_box_crop', [image, bbox]):\n",
    "    shape = tf.shape(image)\n",
    "    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n",
    "        shape,\n",
    "        bounding_boxes=bbox,\n",
    "        min_object_covered=min_object_covered,\n",
    "        aspect_ratio_range=aspect_ratio_range,\n",
    "        area_range=area_range,\n",
    "        max_attempts=max_attempts,\n",
    "        use_image_if_no_bounding_boxes=True)\n",
    "    bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n",
    "\n",
    "    # Crop the image to the specified bounding box.\n",
    "    offset_y, offset_x, _ = tf.unstack(bbox_begin)\n",
    "    target_height, target_width, _ = tf.unstack(bbox_size)\n",
    "    image = tf.image.crop_to_bounding_box(\n",
    "        image, offset_y, offset_x, target_height, target_width)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def crop_and_resize(image, height, width):\n",
    "  \"\"\"Make a random crop and resize it to height `height` and width `width`.\n",
    "  Args:\n",
    "    image: Tensor representing the image.\n",
    "    height: Desired image height.\n",
    "    width: Desired image width.\n",
    "  Returns:\n",
    "    A `height` x `width` x channels Tensor holding a random crop of `image`.\n",
    "  \"\"\"\n",
    "  bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])\n",
    "  aspect_ratio = width / height\n",
    "  image = distorted_bounding_box_crop(\n",
    "      image,\n",
    "      bbox,\n",
    "      min_object_covered=0.1,\n",
    "      aspect_ratio_range=(3. / 4 * aspect_ratio, 4. / 3. * aspect_ratio),\n",
    "      area_range=(0.08, 1.0),\n",
    "      max_attempts=100,\n",
    "      scope=None)\n",
    "  return tf.compat.v1.image.resize_bicubic([image], [height, width])[0]\n",
    "\n",
    "\n",
    "def gaussian_blur(image, kernel_size, sigma, padding='SAME'):\n",
    "  \"\"\"Blurs the given image with separable convolution.\n",
    "  Args:\n",
    "    image: Tensor of shape [height, width, channels] and dtype float to blur.\n",
    "    kernel_size: Integer Tensor for the size of the blur kernel. This is should\n",
    "      be an odd number. If it is an even number, the actual kernel size will be\n",
    "      size + 1.\n",
    "    sigma: Sigma value for gaussian operator.\n",
    "    padding: Padding to use for the convolution. Typically 'SAME' or 'VALID'.\n",
    "  Returns:\n",
    "    A Tensor representing the blurred image.\n",
    "  \"\"\"\n",
    "  radius = tf.to_int32(kernel_size / 2)\n",
    "  kernel_size = radius * 2 + 1\n",
    "  x = tf.to_float(tf.range(-radius, radius + 1))\n",
    "  blur_filter = tf.exp(\n",
    "      -tf.pow(x, 2.0) / (2.0 * tf.pow(tf.to_float(sigma), 2.0)))\n",
    "  blur_filter /= tf.reduce_sum(blur_filter)\n",
    "  # One vertical and one horizontal filter.\n",
    "  blur_v = tf.reshape(blur_filter, [kernel_size, 1, 1, 1])\n",
    "  blur_h = tf.reshape(blur_filter, [1, kernel_size, 1, 1])\n",
    "  num_channels = tf.shape(image)[-1]\n",
    "  blur_h = tf.tile(blur_h, [1, 1, num_channels, 1])\n",
    "  blur_v = tf.tile(blur_v, [1, 1, num_channels, 1])\n",
    "  expand_batch_dim = image.shape.ndims == 3\n",
    "  if expand_batch_dim:\n",
    "    # Tensorflow requires batched input to convolutions, which we can fake with\n",
    "    # an extra dimension.\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "  blurred = tf.nn.depthwise_conv2d(\n",
    "      image, blur_h, strides=[1, 1, 1, 1], padding=padding)\n",
    "  blurred = tf.nn.depthwise_conv2d(\n",
    "      blurred, blur_v, strides=[1, 1, 1, 1], padding=padding)\n",
    "  if expand_batch_dim:\n",
    "    blurred = tf.squeeze(blurred, axis=0)\n",
    "  return blurred\n",
    "\n",
    "\n",
    "def random_crop_with_resize(image, height, width, p=1.0):\n",
    "  \"\"\"Randomly crop and resize an image.\n",
    "  Args:\n",
    "    image: `Tensor` representing an image of arbitrary size.\n",
    "    height: Height of output image.\n",
    "    width: Width of output image.\n",
    "    p: Probability of applying this transformation.\n",
    "  Returns:\n",
    "    A preprocessed image `Tensor`.\n",
    "  \"\"\"\n",
    "  def _transform(image):  # pylint: disable=missing-docstring\n",
    "    image = crop_and_resize(image, height, width)\n",
    "    return image\n",
    "  return random_apply(_transform, p=p, x=image)\n",
    "\n",
    "\n",
    "def random_color_jitter(image, p=1.0):\n",
    "  def _transform(image):\n",
    "    color_jitter_t = functools.partial(\n",
    "        color_jitter, strength=FLAGS_color_jitter_strength)\n",
    "    image = random_apply(color_jitter_t, p=0.8, x=image)\n",
    "    return random_apply(to_grayscale, p=0.2, x=image)\n",
    "  return random_apply(_transform, p=p, x=image)\n",
    "\n",
    "\n",
    "def random_blur(image, height, width, p=1.0):\n",
    "  \"\"\"Randomly blur an image.\n",
    "  Args:\n",
    "    image: `Tensor` representing an image of arbitrary size.\n",
    "    height: Height of output image.\n",
    "    width: Width of output image.\n",
    "    p: probability of applying this transformation.\n",
    "  Returns:\n",
    "    A preprocessed image `Tensor`.\n",
    "  \"\"\"\n",
    "  del width\n",
    "  def _transform(image):\n",
    "    sigma = tf.random.uniform([], 0.1, 2.0, dtype=tf.float32)\n",
    "    return gaussian_blur(\n",
    "        image, kernel_size=height//10, sigma=sigma, padding='SAME')\n",
    "  return random_apply(_transform, p=p, x=image)\n",
    "\n",
    "\n",
    "def batch_random_blur(images_list, height, width, blur_probability=0.5):\n",
    "  \"\"\"Apply efficient batch data transformations.\n",
    "  Args:\n",
    "    images_list: a list of image tensors.\n",
    "    height: the height of image.\n",
    "    width: the width of image.\n",
    "    blur_probability: the probaility to apply the blur operator.\n",
    "  Returns:\n",
    "    Preprocessed feature list.\n",
    "  \"\"\"\n",
    "  def generate_selector(p, bsz):\n",
    "    shape = [bsz, 1, 1, 1]\n",
    "    selector = tf.cast(\n",
    "        tf.less(tf.random_uniform(shape, 0, 1, dtype=tf.float32), p),\n",
    "        tf.float32)\n",
    "    return selector\n",
    "\n",
    "  new_images_list = []\n",
    "  for images in images_list:\n",
    "    images_new = random_blur(images, height, width, p=1.)\n",
    "    selector = generate_selector(blur_probability, tf.shape(images)[0])\n",
    "    images = images_new * selector + images * (1 - selector)\n",
    "    images = tf.clip_by_value(images, 0., 1.)\n",
    "    new_images_list.append(images)\n",
    "\n",
    "  return new_images_list\n",
    "\n",
    "\n",
    "def preprocess_for_train(image, height, width,\n",
    "                         color_distort=True, crop=True, flip=True):\n",
    "  \"\"\"Preprocesses the given image for training.\n",
    "  Args:\n",
    "    image: `Tensor` representing an image of arbitrary size.\n",
    "    height: Height of output image.\n",
    "    width: Width of output image.\n",
    "    color_distort: Whether to apply the color distortion.\n",
    "    crop: Whether to crop the image.\n",
    "    flip: Whether or not to flip left and right of an image.\n",
    "  Returns:\n",
    "    A preprocessed image `Tensor`.\n",
    "  \"\"\"\n",
    "  if crop:\n",
    "    image = random_crop_with_resize(image, height, width)\n",
    "  if flip:\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "  if color_distort:\n",
    "    image = random_color_jitter(image)\n",
    "  image = tf.reshape(image, [height, width, 3])\n",
    "  image = tf.clip_by_value(image, 0., 1.)\n",
    "  return image\n",
    "\n",
    "\n",
    "def preprocess_for_eval(image, height, width, crop=True):\n",
    "  \"\"\"Preprocesses the given image for evaluation.\n",
    "  Args:\n",
    "    image: `Tensor` representing an image of arbitrary size.\n",
    "    height: Height of output image.\n",
    "    width: Width of output image.\n",
    "    crop: Whether or not to (center) crop the test images.\n",
    "  Returns:\n",
    "    A preprocessed image `Tensor`.\n",
    "  \"\"\"\n",
    "  if crop:\n",
    "    image = center_crop(image, height, width, crop_proportion=CROP_PROPORTION)\n",
    "  image = tf.reshape(image, [height, width, 3])\n",
    "  image = tf.clip_by_value(image, 0., 1.)\n",
    "  return image\n",
    "\n",
    "\n",
    "def preprocess_image(image, height, width, is_training=False,\n",
    "                     color_distort=True, test_crop=True):\n",
    "  \"\"\"Preprocesses the given image.\n",
    "  Args:\n",
    "    image: `Tensor` representing an image of arbitrary size.\n",
    "    height: Height of output image.\n",
    "    width: Width of output image.\n",
    "    is_training: `bool` for whether the preprocessing is for training.\n",
    "    color_distort: whether to apply the color distortion.\n",
    "    test_crop: whether or not to extract a central crop of the images\n",
    "        (as for standard ImageNet evaluation) during the evaluation.\n",
    "  Returns:\n",
    "    A preprocessed image `Tensor` of range [0, 1].\n",
    "  \"\"\"\n",
    "  image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "  if is_training:\n",
    "    return preprocess_for_train(image, height, width, color_distort)\n",
    "  else:\n",
    "    return preprocess_for_eval(image, height, width, test_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e26e4cbc-3686-4f65-9dac-13625e8d8351",
   "metadata": {},
   "outputs": [],
   "source": [
    "EETA_DEFAULT = 0.001\n",
    "\n",
    "\n",
    "class LARSOptimizer(tf.keras.optimizers.Optimizer):\n",
    "  \"\"\"Layer-wise Adaptive Rate Scaling for large batch training.\n",
    "\n",
    "  Introduced by \"Large Batch Training of Convolutional Networks\" by Y. You,\n",
    "  I. Gitman, and B. Ginsburg. (https://arxiv.org/abs/1708.03888)\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               learning_rate,\n",
    "               momentum=0.9,\n",
    "               use_nesterov=False,\n",
    "               weight_decay=0.0,\n",
    "               exclude_from_weight_decay=None,\n",
    "               exclude_from_layer_adaptation=None,\n",
    "               classic_momentum=True,\n",
    "               eeta=EETA_DEFAULT,\n",
    "               name=\"LARSOptimizer\"):\n",
    "    \"\"\"Constructs a LARSOptimizer.\n",
    "\n",
    "    Args:\n",
    "      learning_rate: A `float` for learning rate.\n",
    "      momentum: A `float` for momentum.\n",
    "      use_nesterov: A 'Boolean' for whether to use nesterov momentum.\n",
    "      weight_decay: A `float` for weight decay.\n",
    "      exclude_from_weight_decay: A list of `string` for variable screening, if\n",
    "          any of the string appears in a variable's name, the variable will be\n",
    "          excluded for computing weight decay. For example, one could specify\n",
    "          the list like ['batch_normalization', 'bias'] to exclude BN and bias\n",
    "          from weight decay.\n",
    "      exclude_from_layer_adaptation: Similar to exclude_from_weight_decay, but\n",
    "          for layer adaptation. If it is None, it will be defaulted the same as\n",
    "          exclude_from_weight_decay.\n",
    "      classic_momentum: A `boolean` for whether to use classic (or popular)\n",
    "          momentum. The learning rate is applied during momeuntum update in\n",
    "          classic momentum, but after momentum for popular momentum.\n",
    "      eeta: A `float` for scaling of learning rate when computing trust ratio.\n",
    "      name: The name for the scope.\n",
    "    \"\"\"\n",
    "    super(LARSOptimizer, self).__init__(name)\n",
    "\n",
    "    self._set_hyper(\"learning_rate\", learning_rate)\n",
    "    self.momentum = momentum\n",
    "    self.weight_decay = weight_decay\n",
    "    self.use_nesterov = use_nesterov\n",
    "    self.classic_momentum = classic_momentum\n",
    "    self.eeta = eeta\n",
    "    self.exclude_from_weight_decay = exclude_from_weight_decay\n",
    "    # exclude_from_layer_adaptation is set to exclude_from_weight_decay if the\n",
    "    # arg is None.\n",
    "    if exclude_from_layer_adaptation:\n",
    "      self.exclude_from_layer_adaptation = exclude_from_layer_adaptation\n",
    "    else:\n",
    "      self.exclude_from_layer_adaptation = exclude_from_weight_decay\n",
    "\n",
    "  def _create_slots(self, var_list):\n",
    "    for v in var_list:\n",
    "      self.add_slot(v, \"Momentum\")\n",
    "\n",
    "  def _resource_apply_dense(self, grad, param, apply_state=None):\n",
    "    if grad is None or param is None:\n",
    "      return tf.no_op()\n",
    "\n",
    "    var_device, var_dtype = param.device, param.dtype.base_dtype\n",
    "    coefficients = ((apply_state or {}).get((var_device, var_dtype)) or\n",
    "                    self._fallback_apply_state(var_device, var_dtype))\n",
    "    learning_rate = coefficients[\"lr_t\"]\n",
    "\n",
    "    param_name = param.name\n",
    "\n",
    "    v = self.get_slot(param, \"Momentum\")\n",
    "\n",
    "    if self._use_weight_decay(param_name):\n",
    "      grad += self.weight_decay * param\n",
    "\n",
    "    if self.classic_momentum:\n",
    "      trust_ratio = 1.0\n",
    "      if self._do_layer_adaptation(param_name):\n",
    "        w_norm = tf.norm(param, ord=2)\n",
    "        g_norm = tf.norm(grad, ord=2)\n",
    "        trust_ratio = tf.where(\n",
    "            tf.greater(w_norm, 0),\n",
    "            tf.where(tf.greater(g_norm, 0), (self.eeta * w_norm / g_norm), 1.0),\n",
    "            1.0)\n",
    "      scaled_lr = learning_rate * trust_ratio\n",
    "\n",
    "      next_v = tf.multiply(self.momentum, v) + scaled_lr * grad\n",
    "      if self.use_nesterov:\n",
    "        update = tf.multiply(self.momentum, next_v) + scaled_lr * grad\n",
    "      else:\n",
    "        update = next_v\n",
    "      next_param = param - update\n",
    "    else:\n",
    "      next_v = tf.multiply(self.momentum, v) + grad\n",
    "      if self.use_nesterov:\n",
    "        update = tf.multiply(self.momentum, next_v) + grad\n",
    "      else:\n",
    "        update = next_v\n",
    "\n",
    "      trust_ratio = 1.0\n",
    "      if self._do_layer_adaptation(param_name):\n",
    "        w_norm = tf.norm(param, ord=2)\n",
    "        v_norm = tf.norm(update, ord=2)\n",
    "        trust_ratio = tf.where(\n",
    "            tf.greater(w_norm, 0),\n",
    "            tf.where(tf.greater(v_norm, 0), (self.eeta * w_norm / v_norm), 1.0),\n",
    "            1.0)\n",
    "      scaled_lr = trust_ratio * learning_rate\n",
    "      next_param = param - scaled_lr * update\n",
    "\n",
    "    return tf.group(*[\n",
    "        param.assign(next_param, use_locking=False),\n",
    "        v.assign(next_v, use_locking=False)\n",
    "    ])\n",
    "\n",
    "  def _use_weight_decay(self, param_name):\n",
    "    \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n",
    "    if not self.weight_decay:\n",
    "      return False\n",
    "    if self.exclude_from_weight_decay:\n",
    "      for r in self.exclude_from_weight_decay:\n",
    "        if re.search(r, param_name) is not None:\n",
    "          return False\n",
    "    return True\n",
    "\n",
    "  def _do_layer_adaptation(self, param_name):\n",
    "    \"\"\"Whether to do layer-wise learning rate adaptation for `param_name`.\"\"\"\n",
    "    if self.exclude_from_layer_adaptation:\n",
    "      for r in self.exclude_from_layer_adaptation:\n",
    "        if re.search(r, param_name) is not None:\n",
    "          return False\n",
    "    return True\n",
    "\n",
    "  def get_config(self):\n",
    "    config = super(LARSOptimizer, self).get_config()\n",
    "    config.update({\n",
    "        \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n",
    "        \"momentum\": self.momentum,\n",
    "        \"classic_momentum\": self.classic_momentum,\n",
    "        \"weight_decay\": self.weight_decay,\n",
    "        \"eeta\": self.eeta,\n",
    "        \"use_nesterov\": self.use_nesterov,\n",
    "    })\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f29e85b-6a63-4e05-b430-3390cdc00104",
   "metadata": {},
   "source": [
    "## Image libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af31c0f8-1776-48f8-b37f-514b210b49f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "# from matplotlib. import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420d7755-321f-4af3-bd84-3b284d59cdc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3a362bf-8bf0-4904-ba92-9fa610e1aa38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CNV', 'DME', 'DRUSEN', 'NORMAL']\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = './OCT2017/all_data'\n",
    "data_listing = os.listdir(DATA_PATH)\n",
    "print(data_listing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4649bdff-1d92-40c8-a7a7-0a0b264a8638",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir= DATA_PATH\n",
    "CNV_PATH = os.path.join(train_dir, 'CNV')\n",
    "DME_PATH = os.path.join(train_dir, 'DME')\n",
    "NORMAL_PATH = os.path.join(train_dir, 'NORMAL')\n",
    "DRUSEN_PATH = os.path.join(train_dir, 'DRUSEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2feed38-44b2-4f6f-aa1f-5ad11304f637",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnv = os.listdir(CNV_PATH)\n",
    "dme = os.listdir(DME_PATH)\n",
    "normal = os.listdir(NORMAL_PATH)\n",
    "drusen = os.listdir(DRUSEN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c1e5d92-be42-4531-b1a6-6dd4cc904f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37205\n",
      "11348\n",
      "26315\n",
      "8616\n"
     ]
    }
   ],
   "source": [
    "print(len(cnv))\n",
    "print(len(dme))\n",
    "print(len(normal))\n",
    "print(len(drusen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8623bff1-0817-4ea3-9457-668fa82676e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 66788 images belonging to 4 classes.\n",
      "Found 16696 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "epochs=6\n",
    "batch_size=16\n",
    "steps=60000//batch_size\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255., data_format=\"channels_last\", \n",
    "                                   validation_split=0.20,\n",
    "                                   horizontal_flip=True,\n",
    "                                   rotation_range=37,\n",
    "                                   vertical_flip=True)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    # target_size=(256, 256),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    subset='training',\n",
    "                                                    class_mode='binary',\n",
    "                                                    color_mode='rgb')\n",
    "\n",
    "\n",
    "valid_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    # target_size=(96, 96), \n",
    "                                                    batch_size=batch_size,\n",
    "                                                    shuffle=False,\n",
    "                                                    subset='validation',\n",
    "                                                    class_mode='binary',\n",
    "                                                    color_mode='rgb')\n",
    "\n",
    "\n",
    "\n",
    "best_path='./best_val_acc_'+str(int(time.time()))\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=best_path,\n",
    "    monitor='val_acc',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    initial_value_threshold=0.92)\n",
    "               \n",
    "       \n",
    "\n",
    "def _lrs(epoch, lr):\n",
    "    return lr\n",
    "    \n",
    "lrs = LearningRateScheduler(_lrs) \n",
    " \n",
    "stop = EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        min_delta=0.0001,\n",
    "        patience=10,\n",
    "        verbose=1,\n",
    "        mode='auto',\n",
    "        baseline=None,\n",
    "        restore_best_weights=True\n",
    ")\n",
    "\n",
    "stop_acc = EarlyStopping(\n",
    "            monitor='acc',\n",
    "            min_delta=0.001,\n",
    "            patience=10,\n",
    "            verbose=1,\n",
    "            mode='auto',\n",
    "            baseline=None,\n",
    "            restore_best_weights=True)\n",
    "\n",
    "logdir='./logs'\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12cdb3fd-ef9d-4ca3-9561-38ca24a66640",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "weight_decay = 0.\n",
    "num_classes = 4\n",
    "class Model(tf.keras.Model):\n",
    "  def __init__(self, path):\n",
    "    super(Model, self).__init__()\n",
    "    self.saved_model = tf.saved_model.load(path)\n",
    "    self.dense_layer = tf.keras.layers.Dense(units=num_classes, name=\"head_supervised_new\")\n",
    "    self.optimizer = LARSOptimizer(\n",
    "      learning_rate,\n",
    "      momentum=momentum,\n",
    "      weight_decay=weight_decay,\n",
    "      exclude_from_weight_decay=['batch_normalization', 'bias', 'head_supervised'])\n",
    "\n",
    "  def call(self, x, training=True):\n",
    "    with tf.GradientTape() as tape:\n",
    "      outputs = self.saved_model(x[0], trainable=False)\n",
    "      print(outputs)\n",
    "      logits_t = self.dense_layer(outputs['final_avg_pool'])\n",
    "      loss_t = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels = tf.one_hot(x[1], num_classes), logits=logits_t))\n",
    "      if training:\n",
    "          dense_layer_weights = self.dense_layer.trainable_weights\n",
    "          print('Variables to train:', dense_layer_weights)\n",
    "          grads = tape.gradient(loss_t, dense_layer_weights)\n",
    "          self.optimizer.apply_gradients(zip(grads, dense_layer_weights))\n",
    "    return loss_t, x[0], logits_t, x[1]\n",
    "\n",
    "# model = Model(\"gs://simclr-checkpoints-tf2/simclrv2/finetuned_100pct/r50_1x_sk0/saved_model/\")\n",
    "\n",
    "# Remove this for debugging.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78d85efa-6825-4d67-bbd0-c71147ef5259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(m, generator, epochs=10, training=True):\n",
    "    steps = len(generator.filenames)//generator.batch_size \n",
    "    for e in range(epochs):\n",
    "        total_correct = 0 \n",
    "        total_processed = 0 \n",
    "        total_loss = 0\n",
    "        epoch_start_time = time.time()\n",
    "        for it in range(steps):\n",
    "            x = next(generator)\n",
    "            # x[0] = preprocess_image(\n",
    "            #      x[0], 256, 256, is_training=False, color_distort=False)\n",
    "            xx = (x[0], np.int32(x[1]))\n",
    "            loss, image, logits, labels = train_step(m, xx, training=training)\n",
    "            logits = logits.numpy()\n",
    "            labels = labels.numpy()\n",
    "            pred = logits.argmax(-1)\n",
    "            correct = np.sum(pred == labels)\n",
    "            total = labels.size\n",
    "            total_processed += total\n",
    "            total_loss += loss\n",
    "            total_correct += correct\n",
    "            if (it+1) % 300 == 0:\n",
    "                print(\"[Iter {}] Loss: {} Top 1: {}\".format(it+1, total_loss/total_processed, total_correct/float(total_processed)))\n",
    "        run_time = (time.time() - epoch_start_time)/60 \n",
    "        print(f\"Loss: {total_loss/total_processed} Top 1: {total_correct/total_processed}\")\n",
    "        print(f\"Total time: {run_time} in minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e316fad-dff5-4e65-9848-090acf242d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "977e3f72-b302-4ecc-b771-21bae17b49b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(m, x, training=True):\n",
    "  return m(x, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f868baa-e328-4e6d-9850-6ae74e65b9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 06:20:18.494656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-27 06:20:18.499596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-27 06:20:18.499759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-27 06:20:18.500095: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-27 06:20:18.500989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-27 06:20:18.501122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-27 06:20:18.501246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-27 06:20:18.818160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-27 06:20:18.818344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-27 06:20:18.818474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-27 06:20:18.818590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10241 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:05:00.0, compute capability: 8.6\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_28_layer_call_and_return_conditional_losses_59016) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_177_layer_call_and_return_conditional_losses_134414) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_45_layer_call_and_return_conditional_losses_62688) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_19_layer_call_and_return_conditional_losses_57072) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_133_layer_call_and_return_conditional_losses_81708) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_181_layer_call_and_return_conditional_losses_92076) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_169_layer_call_and_return_conditional_losses_133934) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_63_layer_call_and_return_conditional_losses_139694) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_106_layer_call_and_return_conditional_losses_130214) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_94_layer_call_and_return_conditional_losses_129494) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_200_layer_call_and_return_conditional_losses_96192) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_32_layer_call_and_return_conditional_losses_59880) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_90_layer_call_and_return_conditional_losses_72420) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_26_layer_call_and_return_conditional_losses_137534) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_194_layer_call_and_return_conditional_losses_135494) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_38_layer_call_and_return_conditional_losses_61176) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_37_layer_call_and_return_conditional_losses_126014) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference___call___116255) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_117_layer_call_and_return_conditional_losses_78252) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_72_layer_call_and_return_conditional_losses_68532) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_115_layer_call_and_return_conditional_losses_142814) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_60_layer_call_and_return_conditional_losses_139574) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_33_layer_call_and_return_conditional_losses_125774) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_189_layer_call_and_return_conditional_losses_93804) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_135_layer_call_and_return_conditional_losses_144014) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_5_layer_call_and_return_conditional_losses_136334) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_80_layer_call_and_return_conditional_losses_70260) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_56_layer_call_and_return_conditional_losses_65076) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_79_layer_call_and_return_conditional_losses_70044) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_142_layer_call_and_return_conditional_losses_83652) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_163_layer_call_and_return_conditional_losses_88188) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_105_layer_call_and_return_conditional_losses_75660) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_130_layer_call_and_return_conditional_losses_131654) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_39_layer_call_and_return_conditional_losses_138374) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_203_layer_call_and_return_conditional_losses_96840) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_208_layer_call_and_return_conditional_losses_123633) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_168_layer_call_and_return_conditional_losses_89268) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_209_layer_call_and_return_conditional_losses_98064) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_162_layer_call_and_return_conditional_losses_133574) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_65_layer_call_and_return_conditional_losses_127694) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_126_layer_call_and_return_conditional_losses_80196) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_202_layer_call_and_return_conditional_losses_135974) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_41_layer_call_and_return_conditional_losses_126254) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_199_layer_call_and_return_conditional_losses_135854) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_193_layer_call_and_return_conditional_losses_135374) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_3_layer_call_and_return_conditional_losses_53604) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_164_layer_call_and_return_conditional_losses_145814) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_112_layer_call_and_return_conditional_losses_77172) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_53_layer_call_and_return_conditional_losses_64428) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_86_layer_call_and_return_conditional_losses_71556) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_16_layer_call_and_return_conditional_losses_56424) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_8_layer_call_and_return_conditional_losses_124214) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_43_layer_call_and_return_conditional_losses_62256) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_125_layer_call_and_return_conditional_losses_131294) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_105_layer_call_and_return_conditional_losses_130094) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_159_layer_call_and_return_conditional_losses_145454) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_61_layer_call_and_return_conditional_losses_127454) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_186_layer_call_and_return_conditional_losses_135014) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_100_layer_call_and_return_conditional_losses_141974) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_59_layer_call_and_return_conditional_losses_139454) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_150_layer_call_and_return_conditional_losses_85380) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_157_layer_call_and_return_conditional_losses_86892) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_14_layer_call_and_return_conditional_losses_55980) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_157_layer_call_and_return_conditional_losses_133214) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_34_layer_call_and_return_conditional_losses_138014) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_98_layer_call_and_return_conditional_losses_129734) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_58_layer_call_and_return_conditional_losses_65508) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_9_layer_call_and_return_conditional_losses_136574) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_99_layer_call_and_return_conditional_losses_74364) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_57_layer_call_and_return_conditional_losses_127214) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_layer_call_and_return_conditional_losses_123175) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_92_layer_call_and_return_conditional_losses_72852) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_112_layer_call_and_return_conditional_losses_142694) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_179_layer_call_and_return_conditional_losses_91644) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_107_layer_call_and_return_conditional_losses_142334) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_170_layer_call_and_return_conditional_losses_134054) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_44_layer_call_and_return_conditional_losses_126374) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_21_layer_call_and_return_conditional_losses_57504) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_51_layer_call_and_return_conditional_losses_63996) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_144_layer_call_and_return_conditional_losses_144614) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_206_layer_call_and_return_conditional_losses_136214) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_16_layer_call_and_return_conditional_losses_124694) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_5_layer_call_and_return_conditional_losses_54036) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_159_layer_call_and_return_conditional_losses_87324) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_167_layer_call_and_return_conditional_losses_89052) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_75_layer_call_and_return_conditional_losses_69180) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_84_layer_call_and_return_conditional_losses_71124) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_136_layer_call_and_return_conditional_losses_144134) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_133_layer_call_and_return_conditional_losses_131774) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_192_layer_call_and_return_conditional_losses_94452) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_137_layer_call_and_return_conditional_losses_82572) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_174_layer_call_and_return_conditional_losses_90564) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_76_layer_call_and_return_conditional_losses_140534) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_96_layer_call_and_return_conditional_losses_141734) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_172_layer_call_and_return_conditional_losses_146294) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_142_layer_call_and_return_conditional_losses_132374) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_71_layer_call_and_return_conditional_losses_68316) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_136_layer_call_and_return_conditional_losses_82356) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_50_layer_call_and_return_conditional_losses_126854) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_201_layer_call_and_return_conditional_losses_96408) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_22_layer_call_and_return_conditional_losses_57720) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_36_layer_call_and_return_conditional_losses_60744) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_49_layer_call_and_return_conditional_losses_126734) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_84_layer_call_and_return_conditional_losses_141014) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_158_layer_call_and_return_conditional_losses_87108) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_153_layer_call_and_return_conditional_losses_86028) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_31_layer_call_and_return_conditional_losses_59664) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_67_layer_call_and_return_conditional_losses_139934) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_143_layer_call_and_return_conditional_losses_83868) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_172_layer_call_and_return_conditional_losses_90132) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_131_layer_call_and_return_conditional_losses_81276) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_39_layer_call_and_return_conditional_losses_61392) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_10_layer_call_and_return_conditional_losses_136694) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_208_layer_call_and_return_conditional_losses_97876) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_7_layer_call_and_return_conditional_losses_124094) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_178_layer_call_and_return_conditional_losses_134534) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_195_layer_call_and_return_conditional_losses_135614) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_42_layer_call_and_return_conditional_losses_138494) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_163_layer_call_and_return_conditional_losses_145694) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_146_layer_call_and_return_conditional_losses_84516) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_137_layer_call_and_return_conditional_losses_132014) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_37_layer_call_and_return_conditional_losses_60960) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_156_layer_call_and_return_conditional_losses_86676) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_32_layer_call_and_return_conditional_losses_125654) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_78_layer_call_and_return_conditional_losses_69828) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_74_layer_call_and_return_conditional_losses_128294) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_88_layer_call_and_return_conditional_losses_141254) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_50_layer_call_and_return_conditional_losses_63780) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_73_layer_call_and_return_conditional_losses_68748) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_91_layer_call_and_return_conditional_losses_141374) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_149_layer_call_and_return_conditional_losses_132734) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_111_layer_call_and_return_conditional_losses_142574) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_90_layer_call_and_return_conditional_losses_129254) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_4_layer_call_and_return_conditional_losses_53820) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_110_layer_call_and_return_conditional_losses_76740) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_35_layer_call_and_return_conditional_losses_60528) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_183_layer_call_and_return_conditional_losses_92508) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_75_layer_call_and_return_conditional_losses_140414) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_113_layer_call_and_return_conditional_losses_130574) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_29_layer_call_and_return_conditional_losses_59232) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_48_layer_call_and_return_conditional_losses_63336) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_17_layer_call_and_return_conditional_losses_124814) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_30_layer_call_and_return_conditional_losses_59448) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_64_layer_call_and_return_conditional_losses_139814) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_94_layer_call_and_return_conditional_losses_73284) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_25_layer_call_and_return_conditional_losses_58368) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_87_layer_call_and_return_conditional_losses_71772) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_185_layer_call_and_return_conditional_losses_92940) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_140_layer_call_and_return_conditional_losses_144374) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_162_layer_call_and_return_conditional_losses_87972) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_19_layer_call_and_return_conditional_losses_137174) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_111_layer_call_and_return_conditional_losses_76956) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_4_layer_call_and_return_conditional_losses_123974) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_88_layer_call_and_return_conditional_losses_71988) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_128_layer_call_and_return_conditional_losses_80628) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_114_layer_call_and_return_conditional_losses_77604) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_99_layer_call_and_return_conditional_losses_141854) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_179_layer_call_and_return_conditional_losses_146654) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_79_layer_call_and_return_conditional_losses_140654) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_70_layer_call_and_return_conditional_losses_68100) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_69_layer_call_and_return_conditional_losses_67884) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_29_layer_call_and_return_conditional_losses_125534) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_169_layer_call_and_return_conditional_losses_89484) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_8_layer_call_and_return_conditional_losses_54684) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_132_layer_call_and_return_conditional_losses_143894) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_108_layer_call_and_return_conditional_losses_142454) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_68_layer_call_and_return_conditional_losses_67668) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_206_layer_call_and_return_conditional_losses_97488) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_85_layer_call_and_return_conditional_losses_71340) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_83_layer_call_and_return_conditional_losses_70908) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_11_layer_call_and_return_conditional_losses_55332) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_116_layer_call_and_return_conditional_losses_142934) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_66_layer_call_and_return_conditional_losses_67236) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_87_layer_call_and_return_conditional_losses_141134) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_199_layer_call_and_return_conditional_losses_95976) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_28_layer_call_and_return_conditional_losses_125414) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_81_layer_call_and_return_conditional_losses_128654) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_201_layer_call_and_return_conditional_losses_147974) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_127_layer_call_and_return_conditional_losses_80412) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_1_layer_call_and_return_conditional_losses_53148) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_209_layer_call_and_return_conditional_losses_123740) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_119_layer_call_and_return_conditional_losses_143054) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_55_layer_call_and_return_conditional_losses_64860) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_115_layer_call_and_return_conditional_losses_77820) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_18_layer_call_and_return_conditional_losses_137054) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_101_layer_call_and_return_conditional_losses_129854) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_10_layer_call_and_return_conditional_losses_55116) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_45_layer_call_and_return_conditional_losses_126494) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_156_layer_call_and_return_conditional_losses_145334) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_198_layer_call_and_return_conditional_losses_135734) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_173_layer_call_and_return_conditional_losses_90348) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_117_layer_call_and_return_conditional_losses_130814) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_151_layer_call_and_return_conditional_losses_85596) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_61_layer_call_and_return_conditional_losses_66156) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_170_layer_call_and_return_conditional_losses_89700) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_191_layer_call_and_return_conditional_losses_147374) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_46_layer_call_and_return_conditional_losses_62904) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_23_layer_call_and_return_conditional_losses_137414) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_49_layer_call_and_return_conditional_losses_63564) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_155_layer_call_and_return_conditional_losses_86460) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_123_layer_call_and_return_conditional_losses_79548) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_144_layer_call_and_return_conditional_losses_84084) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_56_layer_call_and_return_conditional_losses_139334) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_93_layer_call_and_return_conditional_losses_73068) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_120_layer_call_and_return_conditional_losses_143174) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_152_layer_call_and_return_conditional_losses_85812) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_12_layer_call_and_return_conditional_losses_124454) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_80_layer_call_and_return_conditional_losses_140774) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_41_layer_call_and_return_conditional_losses_61824) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_92_layer_call_and_return_conditional_losses_141494) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_47_layer_call_and_return_conditional_losses_138854) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_181_layer_call_and_return_conditional_losses_134654) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_177_layer_call_and_return_conditional_losses_91212) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_35_layer_call_and_return_conditional_losses_138134) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_161_layer_call_and_return_conditional_losses_133454) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_24_layer_call_and_return_conditional_losses_125174) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_138_layer_call_and_return_conditional_losses_82788) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_189_layer_call_and_return_conditional_losses_135134) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_130_layer_call_and_return_conditional_losses_81060) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_174_layer_call_and_return_conditional_losses_134294) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_107_layer_call_and_return_conditional_losses_76092) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_171_layer_call_and_return_conditional_losses_146174) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_96_layer_call_and_return_conditional_losses_73716) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_65_layer_call_and_return_conditional_losses_67020) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_131_layer_call_and_return_conditional_losses_143774) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_102_layer_call_and_return_conditional_losses_129974) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_85_layer_call_and_return_conditional_losses_128894) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_62_layer_call_and_return_conditional_losses_66372) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_122_layer_call_and_return_conditional_losses_79332) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_38_layer_call_and_return_conditional_losses_138254) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_24_layer_call_and_return_conditional_losses_58152) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_6_layer_call_and_return_conditional_losses_136454) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_114_layer_call_and_return_conditional_losses_130694) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_109_layer_call_and_return_conditional_losses_130334) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_196_layer_call_and_return_conditional_losses_95328) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_166_layer_call_and_return_conditional_losses_133814) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_20_layer_call_and_return_conditional_losses_57288) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_191_layer_call_and_return_conditional_losses_94236) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_12_layer_call_and_return_conditional_losses_55548) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_30_layer_call_and_return_conditional_losses_137774) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_158_layer_call_and_return_conditional_losses_133334) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_154_layer_call_and_return_conditional_losses_86244) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_116_layer_call_and_return_conditional_losses_78036) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_125_layer_call_and_return_conditional_losses_79980) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_140_layer_call_and_return_conditional_losses_83220) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_layer_call_and_return_conditional_losses_52932) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_187_layer_call_and_return_conditional_losses_147134) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_197_layer_call_and_return_conditional_losses_95544) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_74_layer_call_and_return_conditional_losses_68964) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_57_layer_call_and_return_conditional_losses_65292) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_160_layer_call_and_return_conditional_losses_145574) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_173_layer_call_and_return_conditional_losses_134174) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_54_layer_call_and_return_conditional_losses_64644) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_207_layer_call_and_return_conditional_losses_123524) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_33_layer_call_and_return_conditional_losses_60096) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_21_layer_call_and_return_conditional_losses_125054) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_171_layer_call_and_return_conditional_losses_89916) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_55_layer_call_and_return_conditional_losses_139214) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_138_layer_call_and_return_conditional_losses_132134) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_187_layer_call_and_return_conditional_losses_93372) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_109_layer_call_and_return_conditional_losses_76524) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_141_layer_call_and_return_conditional_losses_83436) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_103_layer_call_and_return_conditional_losses_75228) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_188_layer_call_and_return_conditional_losses_147254) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_118_layer_call_and_return_conditional_losses_78468) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_121_layer_call_and_return_conditional_losses_131054) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_184_layer_call_and_return_conditional_losses_147014) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_152_layer_call_and_return_conditional_losses_145094) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_149_layer_call_and_return_conditional_losses_85164) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_126_layer_call_and_return_conditional_losses_131414) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_91_layer_call_and_return_conditional_losses_72636) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_44_layer_call_and_return_conditional_losses_62472) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_166_layer_call_and_return_conditional_losses_88836) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_150_layer_call_and_return_conditional_losses_132854) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_185_layer_call_and_return_conditional_losses_134894) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_155_layer_call_and_return_conditional_losses_145214) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_89_layer_call_and_return_conditional_losses_129134) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_161_layer_call_and_return_conditional_losses_87756) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_123_layer_call_and_return_conditional_losses_143294) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_9_layer_call_and_return_conditional_losses_54900) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_27_layer_call_and_return_conditional_losses_58800) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_164_layer_call_and_return_conditional_losses_88404) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_129_layer_call_and_return_conditional_losses_80844) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_3_layer_call_and_return_conditional_losses_123854) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_82_layer_call_and_return_conditional_losses_128774) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_124_layer_call_and_return_conditional_losses_143414) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_13_layer_call_and_return_conditional_losses_136814) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_145_layer_call_and_return_conditional_losses_132494) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_113_layer_call_and_return_conditional_losses_77388) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_103_layer_call_and_return_conditional_losses_142094) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_52_layer_call_and_return_conditional_losses_64212) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_97_layer_call_and_return_conditional_losses_73932) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_200_layer_call_and_return_conditional_losses_147854) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_66_layer_call_and_return_conditional_losses_127814) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_71_layer_call_and_return_conditional_losses_140174) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_83_layer_call_and_return_conditional_losses_140894) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_124_layer_call_and_return_conditional_losses_79764) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_76_layer_call_and_return_conditional_losses_69396) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_73_layer_call_and_return_conditional_losses_128174) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_134_layer_call_and_return_conditional_losses_131894) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_18_layer_call_and_return_conditional_losses_56856) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_146_layer_call_and_return_conditional_losses_132614) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_48_layer_call_and_return_conditional_losses_126614) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_52_layer_call_and_return_conditional_losses_139094) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_197_layer_call_and_return_conditional_losses_147734) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_93_layer_call_and_return_conditional_losses_129374) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_180_layer_call_and_return_conditional_losses_146774) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_196_layer_call_and_return_conditional_losses_147614) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_59_layer_call_and_return_conditional_losses_65724) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_34_layer_call_and_return_conditional_losses_60312) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_86_layer_call_and_return_conditional_losses_129014) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_78_layer_call_and_return_conditional_losses_128534) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_67_layer_call_and_return_conditional_losses_67452) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_147_layer_call_and_return_conditional_losses_144734) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_108_layer_call_and_return_conditional_losses_76308) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_62_layer_call_and_return_conditional_losses_127574) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_101_layer_call_and_return_conditional_losses_74796) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_1_layer_call_and_return_conditional_losses_123295) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_81_layer_call_and_return_conditional_losses_70476) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_192_layer_call_and_return_conditional_losses_147494) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_143_layer_call_and_return_conditional_losses_144494) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_20_layer_call_and_return_conditional_losses_124934) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_118_layer_call_and_return_conditional_losses_130934) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_148_layer_call_and_return_conditional_losses_84948) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_203_layer_call_and_return_conditional_losses_136094) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_186_layer_call_and_return_conditional_losses_93156) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_42_layer_call_and_return_conditional_losses_62040) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_154_layer_call_and_return_conditional_losses_133094) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_15_layer_call_and_return_conditional_losses_56196) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_207_layer_call_and_return_conditional_losses_97682) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_2_layer_call_and_return_conditional_losses_53364) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_27_layer_call_and_return_conditional_losses_137654) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_145_layer_call_and_return_conditional_losses_84300) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_168_layer_call_and_return_conditional_losses_146054) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_132_layer_call_and_return_conditional_losses_81492) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_176_layer_call_and_return_conditional_losses_90996) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_182_layer_call_and_return_conditional_losses_134774) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_204_layer_call_and_return_conditional_losses_148094) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_72_layer_call_and_return_conditional_losses_140294) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_95_layer_call_and_return_conditional_losses_141614) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_26_layer_call_and_return_conditional_losses_58584) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_135_layer_call_and_return_conditional_losses_82140) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_102_layer_call_and_return_conditional_losses_75012) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_160_layer_call_and_return_conditional_losses_87540) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_127_layer_call_and_return_conditional_losses_143534) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_184_layer_call_and_return_conditional_losses_92724) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_98_layer_call_and_return_conditional_losses_74148) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_15_layer_call_and_return_conditional_losses_124574) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_25_layer_call_and_return_conditional_losses_125294) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_139_layer_call_and_return_conditional_losses_144254) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_51_layer_call_and_return_conditional_losses_138974) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_120_layer_call_and_return_conditional_losses_78900) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_6_layer_call_and_return_conditional_losses_54252) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_141_layer_call_and_return_conditional_losses_132254) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_47_layer_call_and_return_conditional_losses_63120) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_22_layer_call_and_return_conditional_losses_137294) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_183_layer_call_and_return_conditional_losses_146894) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_82_layer_call_and_return_conditional_losses_70692) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_58_layer_call_and_return_conditional_losses_127334) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_106_layer_call_and_return_conditional_losses_75876) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_167_layer_call_and_return_conditional_losses_145934) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_11_layer_call_and_return_conditional_losses_124334) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_202_layer_call_and_return_conditional_losses_96624) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_129_layer_call_and_return_conditional_losses_131534) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_54_layer_call_and_return_conditional_losses_127094) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_188_layer_call_and_return_conditional_losses_93588) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_151_layer_call_and_return_conditional_losses_144974) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_193_layer_call_and_return_conditional_losses_94668) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_175_layer_call_and_return_conditional_losses_146414) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_204_layer_call_and_return_conditional_losses_97056) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_89_layer_call_and_return_conditional_losses_72204) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_43_layer_call_and_return_conditional_losses_138614) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_148_layer_call_and_return_conditional_losses_144854) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_153_layer_call_and_return_conditional_losses_132974) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_205_layer_call_and_return_conditional_losses_97272) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_122_layer_call_and_return_conditional_losses_131174) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_2_layer_call_and_return_conditional_losses_123415) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_175_layer_call_and_return_conditional_losses_90780) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_190_layer_call_and_return_conditional_losses_94020) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_104_layer_call_and_return_conditional_losses_75444) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_7_layer_call_and_return_conditional_losses_54468) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_121_layer_call_and_return_conditional_losses_79116) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_182_layer_call_and_return_conditional_losses_92292) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_64_layer_call_and_return_conditional_losses_66804) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_36_layer_call_and_return_conditional_losses_125894) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_178_layer_call_and_return_conditional_losses_91428) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_40_layer_call_and_return_conditional_losses_61608) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_31_layer_call_and_return_conditional_losses_137894) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_70_layer_call_and_return_conditional_losses_128054) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_180_layer_call_and_return_conditional_losses_91860) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_60_layer_call_and_return_conditional_losses_65940) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_95_layer_call_and_return_conditional_losses_73500) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_40_layer_call_and_return_conditional_losses_126134) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_139_layer_call_and_return_conditional_losses_83004) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_14_layer_call_and_return_conditional_losses_136934) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_165_layer_call_and_return_conditional_losses_133694) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_195_layer_call_and_return_conditional_losses_95112) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_23_layer_call_and_return_conditional_losses_57936) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_165_layer_call_and_return_conditional_losses_88620) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_69_layer_call_and_return_conditional_losses_127934) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_77_layer_call_and_return_conditional_losses_128414) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_77_layer_call_and_return_conditional_losses_69612) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_190_layer_call_and_return_conditional_losses_135254) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_128_layer_call_and_return_conditional_losses_143654) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_176_layer_call_and_return_conditional_losses_146534) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_198_layer_call_and_return_conditional_losses_95760) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_205_layer_call_and_return_conditional_losses_148214) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_104_layer_call_and_return_conditional_losses_142214) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_134_layer_call_and_return_conditional_losses_81924) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_147_layer_call_and_return_conditional_losses_84732) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_63_layer_call_and_return_conditional_losses_66588) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_119_layer_call_and_return_conditional_losses_78684) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_68_layer_call_and_return_conditional_losses_140054) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_100_layer_call_and_return_conditional_losses_74580) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_194_layer_call_and_return_conditional_losses_94896) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_13_layer_call_and_return_conditional_losses_55764) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_97_layer_call_and_return_conditional_losses_129614) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_46_layer_call_and_return_conditional_losses_138734) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_53_layer_call_and_return_conditional_losses_126974) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_17_layer_call_and_return_conditional_losses_56640) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_110_layer_call_and_return_conditional_losses_130454) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    }
   ],
   "source": [
    "model_r152_10 = Model(\"./10pct/r152_3x_sk1/saved_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca4816b0-5960-4008-b340-8f6df2b32ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer=keras.optimizers.adam_v2.Adam(learning_rate=0.000001), loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "model_r152_10.compile(loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31526744-2c56-46a5-859d-cbe8c28b27d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CNV': 0, 'DME': 1, 'DRUSEN': 2, 'NORMAL': 3}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20b31431-d5c0-4373-a44b-2caafda2612d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3750\n",
      "6\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(steps)\n",
    "print(epochs)\n",
    "print(batch_size)\n",
    "# res101_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "425613e8-3d69-4330-9534-51bcef3c2cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'initial_conv': <tf.Tensor 'model/StatefulPartitionedCall:5' shape=(16, 128, 128, 192) dtype=float32>, 'block_group2': <tf.Tensor 'model/StatefulPartitionedCall:1' shape=(16, 32, 32, 1536) dtype=float32>, 'logits_sup': <tf.Tensor 'model/StatefulPartitionedCall:7' shape=(16, 1000) dtype=float32>, 'block_group3': <tf.Tensor 'model/StatefulPartitionedCall:2' shape=(16, 16, 16, 3072) dtype=float32>, 'block_group4': <tf.Tensor 'model/StatefulPartitionedCall:3' shape=(16, 8, 8, 6144) dtype=float32>, 'block_group1': <tf.Tensor 'model/StatefulPartitionedCall:0' shape=(16, 64, 64, 768) dtype=float32>, 'initial_max_pool': <tf.Tensor 'model/StatefulPartitionedCall:6' shape=(16, 64, 64, 192) dtype=float32>, 'final_avg_pool': <tf.Tensor 'model/StatefulPartitionedCall:4' shape=(16, 6144) dtype=float32>}\n",
      "Variables to train: [<tf.Variable 'model/head_supervised_new/kernel:0' shape=(6144, 4) dtype=float32>, <tf.Variable 'model/head_supervised_new/bias:0' shape=(4,) dtype=float32>]\n",
      "{'initial_conv': <tf.Tensor 'model/StatefulPartitionedCall:5' shape=(16, 128, 128, 192) dtype=float32>, 'block_group2': <tf.Tensor 'model/StatefulPartitionedCall:1' shape=(16, 32, 32, 1536) dtype=float32>, 'logits_sup': <tf.Tensor 'model/StatefulPartitionedCall:7' shape=(16, 1000) dtype=float32>, 'block_group3': <tf.Tensor 'model/StatefulPartitionedCall:2' shape=(16, 16, 16, 3072) dtype=float32>, 'block_group4': <tf.Tensor 'model/StatefulPartitionedCall:3' shape=(16, 8, 8, 6144) dtype=float32>, 'block_group1': <tf.Tensor 'model/StatefulPartitionedCall:0' shape=(16, 64, 64, 768) dtype=float32>, 'initial_max_pool': <tf.Tensor 'model/StatefulPartitionedCall:6' shape=(16, 64, 64, 192) dtype=float32>, 'final_avg_pool': <tf.Tensor 'model/StatefulPartitionedCall:4' shape=(16, 6144) dtype=float32>}\n",
      "Variables to train: [<tf.Variable 'model/head_supervised_new/kernel:0' shape=(6144, 4) dtype=float32>, <tf.Variable 'model/head_supervised_new/bias:0' shape=(4,) dtype=float32>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 06:20:48.613252: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8202\n",
      "2022-04-27 06:20:52.554813: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 300] Loss: 0.053216464817523956 Top 1: 0.6902083333333333\n",
      "[Iter 600] Loss: 0.04602234810590744 Top 1: 0.734375\n",
      "[Iter 900] Loss: 0.04211752116680145 Top 1: 0.7575\n",
      "[Iter 1200] Loss: 0.03962055221199989 Top 1: 0.7730208333333334\n",
      "[Iter 1500] Loss: 0.0375613234937191 Top 1: 0.7844583333333334\n",
      "[Iter 1800] Loss: 0.035720761865377426 Top 1: 0.7965972222222222\n",
      "[Iter 2100] Loss: 0.03432842716574669 Top 1: 0.8048809523809524\n",
      "[Iter 2400] Loss: 0.03325456753373146 Top 1: 0.811484375\n",
      "[Iter 2700] Loss: 0.03232881799340248 Top 1: 0.8171990740740741\n",
      "[Iter 3000] Loss: 0.03152655437588692 Top 1: 0.82125\n",
      "[Iter 3300] Loss: 0.030921664088964462 Top 1: 0.8248484848484848\n",
      "[Iter 3600] Loss: 0.030318528413772583 Top 1: 0.8282465277777777\n",
      "[Iter 3900] Loss: 0.02981603518128395 Top 1: 0.8312660256410257\n",
      "Loss: 0.02934441715478897 Top 1: 0.8341668663152851\n",
      "Total time: 67.44114385843277 in minutes\n",
      "{'initial_conv': <tf.Tensor 'model/StatefulPartitionedCall:5' shape=(4, 128, 128, 192) dtype=float32>, 'block_group2': <tf.Tensor 'model/StatefulPartitionedCall:1' shape=(4, 32, 32, 1536) dtype=float32>, 'logits_sup': <tf.Tensor 'model/StatefulPartitionedCall:7' shape=(4, 1000) dtype=float32>, 'block_group3': <tf.Tensor 'model/StatefulPartitionedCall:2' shape=(4, 16, 16, 3072) dtype=float32>, 'block_group4': <tf.Tensor 'model/StatefulPartitionedCall:3' shape=(4, 8, 8, 6144) dtype=float32>, 'block_group1': <tf.Tensor 'model/StatefulPartitionedCall:0' shape=(4, 64, 64, 768) dtype=float32>, 'initial_max_pool': <tf.Tensor 'model/StatefulPartitionedCall:6' shape=(4, 64, 64, 192) dtype=float32>, 'final_avg_pool': <tf.Tensor 'model/StatefulPartitionedCall:4' shape=(4, 6144) dtype=float32>}\n",
      "Variables to train: [<tf.Variable 'model/head_supervised_new/kernel:0' shape=(6144, 4) dtype=float32>, <tf.Variable 'model/head_supervised_new/bias:0' shape=(4,) dtype=float32>]\n",
      "[Iter 300] Loss: 0.022816283628344536 Top 1: 0.8717627401837929\n",
      "[Iter 600] Loss: 0.02234603837132454 Top 1: 0.8742177722152691\n",
      "[Iter 900] Loss: 0.022201431915163994 Top 1: 0.876424798443147\n",
      "[Iter 1200] Loss: 0.021964281797409058 Top 1: 0.8775276214300605\n",
      "[Iter 1500] Loss: 0.02174057811498642 Top 1: 0.877397031849258\n",
      "[Iter 1800] Loss: 0.021670013666152954 Top 1: 0.8785257746283174\n",
      "[Iter 2100] Loss: 0.021685702726244926 Top 1: 0.8779325949743956\n",
      "[Iter 2400] Loss: 0.021537531167268753 Top 1: 0.8782171511930812\n",
      "[Iter 2700] Loss: 0.021431535482406616 Top 1: 0.8788089284060387\n",
      "[Iter 3000] Loss: 0.021387431770563126 Top 1: 0.8788655497207635\n",
      "[Iter 3300] Loss: 0.02124258317053318 Top 1: 0.8795370159884822\n",
      "[Iter 3600] Loss: 0.02108910121023655 Top 1: 0.8805133013822324\n",
      "[Iter 3900] Loss: 0.02094242163002491 Top 1: 0.8815156760915561\n",
      "Loss: 0.020861519500613213 Top 1: 0.8820613430779368\n",
      "Total time: 67.21941953500112 in minutes\n",
      "[Iter 300] Loss: 0.019712265580892563 Top 1: 0.8920217209690894\n",
      "[Iter 600] Loss: 0.0194482933729887 Top 1: 0.890488110137672\n",
      "[Iter 900] Loss: 0.01988467387855053 Top 1: 0.888101195440645\n",
      "[Iter 1200] Loss: 0.019441954791545868 Top 1: 0.8898269751928288\n",
      "[Iter 1500] Loss: 0.019342143088579178 Top 1: 0.8911955977988995\n",
      "[Iter 1800] Loss: 0.019344689324498177 Top 1: 0.8911699319160762\n",
      "[Iter 2100] Loss: 0.019292235374450684 Top 1: 0.8912111468381565\n",
      "[Iter 2400] Loss: 0.019225716590881348 Top 1: 0.891216005001563\n",
      "[Iter 2700] Loss: 0.019225073978304863 Top 1: 0.8914281744929147\n",
      "[Iter 3000] Loss: 0.019265087321400642 Top 1: 0.8913686755022089\n",
      "[Iter 3300] Loss: 0.019276266917586327 Top 1: 0.8914904902629386\n",
      "[Iter 3600] Loss: 0.019254039973020554 Top 1: 0.8914530805028825\n",
      "[Iter 3900] Loss: 0.019177937880158424 Top 1: 0.8921427197537988\n",
      "Loss: 0.0191299207508564 Top 1: 0.8923500868627569\n",
      "Total time: 67.13137815793355 in minutes\n",
      "[Iter 300] Loss: 0.020190680399537086 Top 1: 0.883249791144528\n",
      "[Iter 600] Loss: 0.01918807253241539 Top 1: 0.8891322486441385\n",
      "[Iter 900] Loss: 0.018922485411167145 Top 1: 0.892132332499305\n",
      "[Iter 1200] Loss: 0.01877215877175331 Top 1: 0.893266624973942\n",
      "[Iter 1500] Loss: 0.018474237993359566 Top 1: 0.8947807236951809\n",
      "[Iter 1800] Loss: 0.018487367779016495 Top 1: 0.8949909684590802\n",
      "[Iter 2100] Loss: 0.018433324992656708 Top 1: 0.895200666904847\n",
      "[Iter 2400] Loss: 0.018355775624513626 Top 1: 0.8956965718453683\n",
      "[Iter 2700] Loss: 0.018338248133659363 Top 1: 0.8962906362878578\n",
      "[Iter 3000] Loss: 0.018329977989196777 Top 1: 0.8964532799866634\n",
      "[Iter 3300] Loss: 0.01826467178761959 Top 1: 0.8962453587936652\n",
      "[Iter 3600] Loss: 0.018321681767702103 Top 1: 0.8959505452524832\n",
      "[Iter 3900] Loss: 0.018186820670962334 Top 1: 0.8964223889209464\n",
      "Loss: 0.01815229468047619 Top 1: 0.8970526568022524\n",
      "Total time: 67.22440288066863 in minutes\n",
      "[Iter 300] Loss: 0.01686950773000717 Top 1: 0.9068504594820385\n",
      "[Iter 600] Loss: 0.01678672805428505 Top 1: 0.9063412599082186\n",
      "[Iter 900] Loss: 0.017213255167007446 Top 1: 0.904642757853767\n",
      "[Iter 1200] Loss: 0.017016785219311714 Top 1: 0.9054096310193871\n",
      "[Iter 1500] Loss: 0.017148643732070923 Top 1: 0.9042437885609471\n",
      "[Iter 1800] Loss: 0.017253704369068146 Top 1: 0.9030498818952342\n",
      "[Iter 2100] Loss: 0.017296968027949333 Top 1: 0.9027331189710611\n",
      "[Iter 2400] Loss: 0.01738665997982025 Top 1: 0.9015317286652079\n",
      "[Iter 2700] Loss: 0.01741330325603485 Top 1: 0.9015467259423914\n",
      "[Iter 3000] Loss: 0.017547959461808205 Top 1: 0.9008502125531382\n",
      "[Iter 3300] Loss: 0.017519785091280937 Top 1: 0.9012275517162992\n",
      "[Iter 3600] Loss: 0.01752300001680851 Top 1: 0.9014030700840453\n",
      "[Iter 3900] Loss: 0.017569860443472862 Top 1: 0.9013111495800474\n",
      "Loss: 0.017569202929735184 Top 1: 0.9015605343557179\n",
      "Total time: 67.19487046400705 in minutes\n",
      "[Iter 300] Loss: 0.018923500552773476 Top 1: 0.8918128654970761\n",
      "[Iter 600] Loss: 0.01817484013736248 Top 1: 0.8952857738840216\n",
      "[Iter 900] Loss: 0.0180373378098011 Top 1: 0.8980400333611342\n",
      "[Iter 1200] Loss: 0.01759030856192112 Top 1: 0.9001459245361685\n",
      "[Iter 1500] Loss: 0.017458489164710045 Top 1: 0.9006586626646657\n",
      "[Iter 1800] Loss: 0.017351938411593437 Top 1: 0.9012088370154231\n",
      "[Iter 2100] Loss: 0.017217349261045456 Top 1: 0.9021078956770275\n",
      "[Iter 2400] Loss: 0.017168521881103516 Top 1: 0.9032249661352506\n",
      "[Iter 2700] Loss: 0.017224542796611786 Top 1: 0.9028433824210429\n",
      "[Iter 3000] Loss: 0.017111118882894516 Top 1: 0.9032258064516129\n",
      "[Iter 3300] Loss: 0.017136162146925926 Top 1: 0.9029324846556035\n",
      "[Iter 3600] Loss: 0.01720750704407692 Top 1: 0.9027922483850802\n",
      "[Iter 3900] Loss: 0.017254048958420753 Top 1: 0.902577418734372\n",
      "Loss: 0.01726376824080944 Top 1: 0.9023542802372252\n",
      "Total time: 67.15560124317805 in minutes\n",
      "[Iter 300] Loss: 0.01638507843017578 Top 1: 0.9058061821219716\n",
      "[Iter 600] Loss: 0.017105648294091225 Top 1: 0.9003963287442637\n",
      "[Iter 900] Loss: 0.017144639045000076 Top 1: 0.8998470948012233\n",
      "[Iter 1200] Loss: 0.017020106315612793 Top 1: 0.9009797790285595\n",
      "[Iter 1500] Loss: 0.017281081527471542 Top 1: 0.8998249124562281\n",
      "[Iter 1800] Loss: 0.017068829387426376 Top 1: 0.901278310407114\n",
      "[Iter 2100] Loss: 0.017048798501491547 Top 1: 0.9013933547695605\n",
      "[Iter 2400] Loss: 0.017092138528823853 Top 1: 0.901844326351985\n",
      "[Iter 2700] Loss: 0.01709257997572422 Top 1: 0.9017782717421506\n",
      "[Iter 3000] Loss: 0.017097942531108856 Top 1: 0.9020588480453446\n",
      "[Iter 3300] Loss: 0.017082665115594864 Top 1: 0.9023262862771843\n",
      "[Iter 3600] Loss: 0.017112785950303078 Top 1: 0.9021497534208516\n",
      "[Iter 3900] Loss: 0.017047621309757233 Top 1: 0.9028659357568763\n",
      "Loss: 0.01709204725921154 Top 1: 0.9029533337327024\n",
      "Total time: 67.279114929835 in minutes\n",
      "[Iter 300] Loss: 0.0169249027967453 Top 1: 0.9035087719298246\n",
      "[Iter 600] Loss: 0.016841180622577667 Top 1: 0.9033166458072591\n",
      "[Iter 900] Loss: 0.016785170882940292 Top 1: 0.9037392271337226\n",
      "[Iter 1200] Loss: 0.01658736728131771 Top 1: 0.9053575151136126\n",
      "[Iter 1500] Loss: 0.01660386100411415 Top 1: 0.9055777888944472\n",
      "[Iter 1800] Loss: 0.016622960567474365 Top 1: 0.9048561900791997\n",
      "[Iter 2100] Loss: 0.016698645427823067 Top 1: 0.9046683339287841\n",
      "[Iter 2400] Loss: 0.016582820564508438 Top 1: 0.9055433989788475\n",
      "[Iter 2700] Loss: 0.016607893630862236 Top 1: 0.9054830045382977\n",
      "[Iter 3000] Loss: 0.01668626442551613 Top 1: 0.9053721763774277\n",
      "[Iter 3300] Loss: 0.016581321135163307 Top 1: 0.9056414336591649\n",
      "[Iter 3600] Loss: 0.01661834679543972 Top 1: 0.905622699173439\n",
      "[Iter 3900] Loss: 0.016629280522465706 Top 1: 0.9060235942809515\n",
      "Loss: 0.016702312976121902 Top 1: 0.9054244294015455\n",
      "Total time: 67.20805159807205 in minutes\n",
      "[Iter 300] Loss: 0.017112573608756065 Top 1: 0.9072681704260651\n",
      "[Iter 600] Loss: 0.017260925844311714 Top 1: 0.9047768043387567\n",
      "[Iter 900] Loss: 0.016877004876732826 Top 1: 0.9054072838476508\n",
      "[Iter 1200] Loss: 0.01673515886068344 Top 1: 0.9061913696060038\n",
      "[Iter 1500] Loss: 0.016832562163472176 Top 1: 0.9052026013006503\n",
      "[Iter 1800] Loss: 0.01678570546209812 Top 1: 0.9054119772127275\n",
      "[Iter 2100] Loss: 0.01670924574136734 Top 1: 0.9057401452899845\n",
      "[Iter 2400] Loss: 0.016663528978824615 Top 1: 0.9055954985933105\n",
      "[Iter 2700] Loss: 0.01669238694012165 Top 1: 0.9056682411781051\n",
      "[Iter 3000] Loss: 0.016699690371751785 Top 1: 0.9054972076352421\n",
      "[Iter 3300] Loss: 0.016643445938825607 Top 1: 0.9057740395544442\n",
      "[Iter 3600] Loss: 0.016725972294807434 Top 1: 0.9048586511078697\n",
      "[Iter 3900] Loss: 0.01673123612999916 Top 1: 0.9045329229980125\n",
      "Loss: 0.016696790233254433 Top 1: 0.9046905888695861\n",
      "Total time: 67.23537561496099 in minutes\n",
      "[Iter 300] Loss: 0.016720080748200417 Top 1: 0.9070593149540518\n",
      "[Iter 600] Loss: 0.016459768638014793 Top 1: 0.9087400917813934\n",
      "[Iter 900] Loss: 0.01617518439888954 Top 1: 0.9098554350847928\n",
      "[Iter 1200] Loss: 0.016075994819402695 Top 1: 0.909631019387117\n",
      "[Iter 1500] Loss: 0.016017718240618706 Top 1: 0.9100383525095881\n",
      "[Iter 1800] Loss: 0.016236446797847748 Top 1: 0.9095109073224955\n",
      "[Iter 2100] Loss: 0.01622275449335575 Top 1: 0.9095808026676194\n",
      "[Iter 2400] Loss: 0.01651676371693611 Top 1: 0.9079139314369074\n",
      "[Iter 2700] Loss: 0.016515681520104408 Top 1: 0.9077058442159859\n",
      "[Iter 3000] Loss: 0.016489660367369652 Top 1: 0.9080811869634076\n",
      "[Iter 3300] Loss: 0.01649582013487816 Top 1: 0.9074789724937485\n",
      "[Iter 3600] Loss: 0.016545886173844337 Top 1: 0.9070466069319997\n",
      "[Iter 3900] Loss: 0.016505923122167587 Top 1: 0.9075142655638905\n",
      "Loss: 0.016618341207504272 Top 1: 0.9069220631402384\n",
      "Total time: 67.20187231302262 in minutes\n",
      "[Iter 300] Loss: 0.016260864213109016 Top 1: 0.9047619047619048\n",
      "[Iter 600] Loss: 0.0161293875426054 Top 1: 0.9063412599082186\n",
      "[Iter 900] Loss: 0.016030989587306976 Top 1: 0.9072838476508202\n",
      "[Iter 1200] Loss: 0.01589764468371868 Top 1: 0.9092662080466959\n",
      "[Iter 1500] Loss: 0.01622902788221836 Top 1: 0.9084959146239786\n",
      "[Iter 1800] Loss: 0.016345500946044922 Top 1: 0.9089551201889676\n",
      "[Iter 2100] Loss: 0.01651398092508316 Top 1: 0.9086280814576635\n",
      "[Iter 2400] Loss: 0.01641644723713398 Top 1: 0.9084870271959987\n",
      "[Iter 2700] Loss: 0.016411887481808662 Top 1: 0.908261554135408\n",
      "[Iter 3000] Loss: 0.016362661495804787 Top 1: 0.9081437025923147\n",
      "[Iter 3300] Loss: 0.016464676707983017 Top 1: 0.9076684094870047\n",
      "[Iter 3600] Loss: 0.016550084576010704 Top 1: 0.9075501840661249\n",
      "[Iter 3900] Loss: 0.016566773876547813 Top 1: 0.9074982368404181\n",
      "Loss: 0.016604715958237648 Top 1: 0.9073264242496855\n",
      "Total time: 67.17925796508788 in minutes\n",
      "[Iter 300] Loss: 0.01756901852786541 Top 1: 0.9062238930659984\n",
      "[Iter 600] Loss: 0.017181815579533577 Top 1: 0.9037338339591156\n",
      "[Iter 900] Loss: 0.01708686351776123 Top 1: 0.9040172365860439\n",
      "[Iter 1200] Loss: 0.017202001065015793 Top 1: 0.9043151969981238\n",
      "[Iter 1500] Loss: 0.01702829636633396 Top 1: 0.9057862264465566\n",
      "[Iter 1800] Loss: 0.01673051342368126 Top 1: 0.907496178963457\n",
      "[Iter 2100] Loss: 0.016551412642002106 Top 1: 0.9083005835417411\n",
      "[Iter 2400] Loss: 0.016521621495485306 Top 1: 0.9079399812441388\n",
      "[Iter 2700] Loss: 0.01644853688776493 Top 1: 0.9078447716958414\n",
      "[Iter 3000] Loss: 0.016384726390242577 Top 1: 0.9077686088188713\n",
      "[Iter 3300] Loss: 0.01631908304989338 Top 1: 0.9079146775782375\n",
      "[Iter 3600] Loss: 0.016292966902256012 Top 1: 0.9081058553865389\n",
      "[Iter 3900] Loss: 0.016335930675268173 Top 1: 0.9080912996088991\n",
      "Loss: 0.0162971094250679 Top 1: 0.9086143892649614\n",
      "Total time: 67.09314799706141 in minutes\n",
      "[Iter 300] Loss: 0.015716342255473137 Top 1: 0.908312447786132\n",
      "[Iter 600] Loss: 0.016334867104887962 Top 1: 0.9067584480600751\n",
      "[Iter 900] Loss: 0.016302142292261124 Top 1: 0.9073533500139005\n",
      "[Iter 1200] Loss: 0.016167616471648216 Top 1: 0.908223889931207\n",
      "[Iter 1500] Loss: 0.0162366796284914 Top 1: 0.9085792896448224\n",
      "[Iter 1800] Loss: 0.016159219667315483 Top 1: 0.9087119633180492\n",
      "[Iter 2100] Loss: 0.016319628804922104 Top 1: 0.9081814933904966\n",
      "[Iter 2400] Loss: 0.016278615221381187 Top 1: 0.9085130770032301\n",
      "[Iter 2700] Loss: 0.016257191076874733 Top 1: 0.9089098823747337\n",
      "[Iter 3000] Loss: 0.01623198203742504 Top 1: 0.9089355672251396\n",
      "[Iter 3300] Loss: 0.0162567850202322 Top 1: 0.9090891869364249\n",
      "[Iter 3600] Loss: 0.01636548340320587 Top 1: 0.9088178092658192\n",
      "[Iter 3900] Loss: 0.01637040078639984 Top 1: 0.9086362762069629\n",
      "Loss: 0.01647293195128441 Top 1: 0.9085694602528006\n",
      "Total time: 67.02431059281031 in minutes\n",
      "[Iter 300] Loss: 0.01564917340874672 Top 1: 0.910609857978279\n",
      "[Iter 600] Loss: 0.015562480315566063 Top 1: 0.9122861910721736\n",
      "[Iter 900] Loss: 0.016213348135352135 Top 1: 0.9106199610786767\n",
      "[Iter 1200] Loss: 0.016165997833013535 Top 1: 0.9111423806545758\n",
      "[Iter 1500] Loss: 0.016141675412654877 Top 1: 0.910413540103385\n",
      "[Iter 1800] Loss: 0.01612486131489277 Top 1: 0.9105530081978602\n",
      "[Iter 2100] Loss: 0.016300899907946587 Top 1: 0.9102655710372752\n",
      "[Iter 2400] Loss: 0.016466420143842697 Top 1: 0.9101281650515786\n",
      "[Iter 2700] Loss: 0.01647823676466942 Top 1: 0.9101139205334815\n",
      "[Iter 3000] Loss: 0.016504326835274696 Top 1: 0.9099774943735934\n",
      "[Iter 3300] Loss: 0.01641627959907055 Top 1: 0.910093203000682\n",
      "[Iter 3600] Loss: 0.0163470096886158 Top 1: 0.9102243522956172\n",
      "[Iter 3900] Loss: 0.016358569264411926 Top 1: 0.9096621145092005\n",
      "Loss: 0.01637505181133747 Top 1: 0.9095728748577248\n",
      "Total time: 66.94646321932474 in minutes\n",
      "[Iter 300] Loss: 0.016062995418906212 Top 1: 0.9120718462823726\n",
      "[Iter 600] Loss: 0.01620020531117916 Top 1: 0.9105131414267835\n",
      "[Iter 900] Loss: 0.016224002465605736 Top 1: 0.9120100083402836\n",
      "[Iter 1200] Loss: 0.01584419421851635 Top 1: 0.9136960600375235\n",
      "[Iter 1500] Loss: 0.015770433470606804 Top 1: 0.9137902284475571\n",
      "[Iter 1800] Loss: 0.015902889892458916 Top 1: 0.9127066833402807\n",
      "[Iter 2100] Loss: 0.015964435413479805 Top 1: 0.912379421221865\n",
      "[Iter 2400] Loss: 0.016019493341445923 Top 1: 0.912472647702407\n",
      "[Iter 2700] Loss: 0.016081221401691437 Top 1: 0.9123367602111697\n",
      "[Iter 3000] Loss: 0.01625862717628479 Top 1: 0.9108110360923565\n",
      "[Iter 3300] Loss: 0.01620844565331936 Top 1: 0.911191937561567\n",
      "[Iter 3600] Loss: 0.01625850982964039 Top 1: 0.9107279294297423\n",
      "[Iter 3900] Loss: 0.016225814819335938 Top 1: 0.9109604411104699\n",
      "Loss: 0.01629936881363392 Top 1: 0.9108009345234529\n",
      "Total time: 67.02683962583542 in minutes\n",
      "[Iter 300] Loss: 0.016722284257411957 Top 1: 0.9087301587301587\n",
      "[Iter 600] Loss: 0.015801671892404556 Top 1: 0.911869002920317\n",
      "[Iter 900] Loss: 0.01608441211283207 Top 1: 0.9107589658048374\n",
      "[Iter 1200] Loss: 0.01598256453871727 Top 1: 0.9112987283718991\n",
      "[Iter 1500] Loss: 0.016242867335677147 Top 1: 0.9100383525095881\n",
      "[Iter 1800] Loss: 0.01629824750125408 Top 1: 0.909615117410032\n",
      "[Iter 2100] Loss: 0.01629793643951416 Top 1: 0.9096105752054305\n",
      "[Iter 2400] Loss: 0.016290249302983284 Top 1: 0.9099197665937272\n",
      "[Iter 2700] Loss: 0.016396379098296165 Top 1: 0.9096276743539872\n",
      "[Iter 3000] Loss: 0.01645122468471527 Top 1: 0.9095190464282737\n",
      "[Iter 3300] Loss: 0.016392560675740242 Top 1: 0.9096953853148443\n",
      "[Iter 3600] Loss: 0.016368843615055084 Top 1: 0.9097902340765437\n",
      "[Iter 3900] Loss: 0.01644827611744404 Top 1: 0.909229338975444\n",
      "Loss: 0.01641242578625679 Top 1: 0.9097376145689811\n",
      "Total time: 67.02988035678864 in minutes\n",
      "[Iter 300] Loss: 0.015879778191447258 Top 1: 0.9114452798663325\n",
      "[Iter 600] Loss: 0.01581566594541073 Top 1: 0.91270337922403\n",
      "[Iter 900] Loss: 0.01645706593990326 Top 1: 0.9110369752571588\n",
      "[Iter 1200] Loss: 0.016899898648262024 Top 1: 0.9083802376485304\n",
      "[Iter 1500] Loss: 0.016644863411784172 Top 1: 0.9100383525095881\n",
      "[Iter 1800] Loss: 0.01644177921116352 Top 1: 0.9105182715020147\n",
      "[Iter 2100] Loss: 0.01631731167435646 Top 1: 0.9111289746337978\n",
      "[Iter 2400] Loss: 0.01635415479540825 Top 1: 0.9109878086902157\n",
      "[Iter 2700] Loss: 0.016285333782434464 Top 1: 0.9110401037325183\n",
      "[Iter 3000] Loss: 0.016395356506109238 Top 1: 0.9105818121196966\n",
      "[Iter 3300] Loss: 0.016536641865968704 Top 1: 0.9102258088959612\n",
      "[Iter 3600] Loss: 0.016572212800383568 Top 1: 0.9099291519066472\n",
      "[Iter 3900] Loss: 0.016572944819927216 Top 1: 0.9101269474899019\n",
      "Loss: 0.016601862385869026 Top 1: 0.9100970466662673\n",
      "Total time: 67.0467766880989 in minutes\n",
      "[Iter 300] Loss: 0.01647421345114708 Top 1: 0.914578111946533\n",
      "[Iter 600] Loss: 0.01568000204861164 Top 1: 0.9162494785148102\n",
      "[Iter 900] Loss: 0.01563405431807041 Top 1: 0.9157631359466222\n",
      "[Iter 1200] Loss: 0.015917077660560608 Top 1: 0.9130706691682301\n",
      "[Iter 1500] Loss: 0.016102546826004982 Top 1: 0.9125812906453227\n",
      "[Iter 1800] Loss: 0.01626291684806347 Top 1: 0.9114214255939975\n",
      "[Iter 2100] Loss: 0.01613735780119896 Top 1: 0.9120221507681314\n",
      "[Iter 2400] Loss: 0.01620839536190033 Top 1: 0.9114567052203814\n",
      "[Iter 2700] Loss: 0.01618054509162903 Top 1: 0.9115495044919885\n",
      "[Iter 3000] Loss: 0.016221465542912483 Top 1: 0.9113528382095524\n",
      "[Iter 3300] Loss: 0.016182610765099525 Top 1: 0.9115897552474047\n",
      "[Iter 3600] Loss: 0.01623409241437912 Top 1: 0.9118045426130443\n",
      "[Iter 3900] Loss: 0.016340212896466255 Top 1: 0.9112810155799193\n",
      "Loss: 0.01636987365782261 Top 1: 0.9110405559216438\n",
      "Total time: 67.00685758590699 in minutes\n",
      "[Iter 300] Loss: 0.015948224812746048 Top 1: 0.908939014202172\n",
      "[Iter 600] Loss: 0.016325419768691063 Top 1: 0.9094701710471422\n",
      "[Iter 900] Loss: 0.01647997461259365 Top 1: 0.9099944398109536\n",
      "[Iter 1200] Loss: 0.016511445865035057 Top 1: 0.9098915989159891\n",
      "[Iter 1500] Loss: 0.01629462279379368 Top 1: 0.9109971652492913\n",
      "[Iter 1800] Loss: 0.016298066824674606 Top 1: 0.9108656384604696\n",
      "[Iter 2100] Loss: 0.016285361722111702 Top 1: 0.9111587471716089\n",
      "[Iter 2400] Loss: 0.016259368509054184 Top 1: 0.9110399083046785\n",
      "[Iter 2700] Loss: 0.016378343105316162 Top 1: 0.9101602296934334\n",
      "[Iter 3000] Loss: 0.016435567289590836 Top 1: 0.9101858798032841\n",
      "[Iter 3300] Loss: 0.01650973968207836 Top 1: 0.9098279912101235\n",
      "[Iter 3600] Loss: 0.01647801510989666 Top 1: 0.9099117871778842\n",
      "[Iter 3900] Loss: 0.01640862599015236 Top 1: 0.9103994357889338\n",
      "Loss: 0.01636427454650402 Top 1: 0.9108907925477745\n",
      "Total time: 67.01918065150579 in minutes\n",
      "[Iter 300] Loss: 0.01607237383723259 Top 1: 0.9058061821219716\n",
      "[Iter 600] Loss: 0.016613805666565895 Top 1: 0.9060283687943262\n",
      "[Iter 900] Loss: 0.01655523292720318 Top 1: 0.9089519043647484\n",
      "[Iter 1200] Loss: 0.016659511253237724 Top 1: 0.9089013967062748\n",
      "[Iter 1500] Loss: 0.016788333654403687 Top 1: 0.9092046023011505\n",
      "[Iter 1800] Loss: 0.016588781028985977 Top 1: 0.9100666944560234\n",
      "[Iter 2100] Loss: 0.01656121388077736 Top 1: 0.9099083005835418\n",
      "[Iter 2400] Loss: 0.016431400552392006 Top 1: 0.9106491611962072\n",
      "[Iter 2700] Loss: 0.016469569876790047 Top 1: 0.9103917754931925\n",
      "[Iter 3000] Loss: 0.01637275516986847 Top 1: 0.9110610986079853\n",
      "[Iter 3300] Loss: 0.01631542481482029 Top 1: 0.9112677123588694\n",
      "[Iter 3600] Loss: 0.016349706798791885 Top 1: 0.9109884003611863\n",
      "[Iter 3900] Loss: 0.016337087377905846 Top 1: 0.9108963262165801\n",
      "Loss: 0.016466941684484482 Top 1: 0.9103366680644581\n",
      "Total time: 67.06627745230993 in minutes\n",
      "[Iter 300] Loss: 0.017898255959153175 Top 1: 0.9074770258980785\n",
      "[Iter 600] Loss: 0.01775706559419632 Top 1: 0.9042553191489362\n",
      "[Iter 900] Loss: 0.017458545044064522 Top 1: 0.9060328051153739\n",
      "[Iter 1200] Loss: 0.017194002866744995 Top 1: 0.9060350218886805\n",
      "[Iter 1500] Loss: 0.016882892698049545 Top 1: 0.9085376021344005\n",
      "[Iter 1800] Loss: 0.01701412908732891 Top 1: 0.9070793386133111\n",
      "[Iter 2100] Loss: 0.016886742785573006 Top 1: 0.9081814933904966\n",
      "[Iter 2400] Loss: 0.01685521751642227 Top 1: 0.9088777743044701\n",
      "[Iter 2700] Loss: 0.016870545223355293 Top 1: 0.9084236361952395\n",
      "[Iter 3000] Loss: 0.016763564199209213 Top 1: 0.9088105359673252\n",
      "[Iter 3300] Loss: 0.016823433339595795 Top 1: 0.9085398196559824\n",
      "[Iter 3600] Loss: 0.016737110912799835 Top 1: 0.9089219976383969\n",
      "[Iter 3900] Loss: 0.016737498342990875 Top 1: 0.9090690517407194\n",
      "Loss: 0.01672685705125332 Top 1: 0.9092284190978255\n",
      "Total time: 67.0364727894465 in minutes\n",
      "[Iter 300] Loss: 0.016122806817293167 Top 1: 0.910609857978279\n",
      "[Iter 600] Loss: 0.016431158408522606 Top 1: 0.91270337922403\n",
      "[Iter 900] Loss: 0.01653263531625271 Top 1: 0.9119405059772032\n",
      "[Iter 1200] Loss: 0.016835985705256462 Top 1: 0.9102564102564102\n",
      "[Iter 1500] Loss: 0.016826581209897995 Top 1: 0.9102467900616975\n",
      "[Iter 1800] Loss: 0.016943108290433884 Top 1: 0.9098235375851049\n",
      "[Iter 2100] Loss: 0.01700401119887829 Top 1: 0.9089258068357747\n",
      "[Iter 2400] Loss: 0.016950087621808052 Top 1: 0.9093987704490987\n",
      "[Iter 2700] Loss: 0.01679094508290291 Top 1: 0.9098360655737705\n",
      "[Iter 3000] Loss: 0.016766754910349846 Top 1: 0.9099566558306244\n",
      "[Iter 3300] Loss: 0.01659119501709938 Top 1: 0.9108509509737062\n",
      "[Iter 3600] Loss: 0.016555950045585632 Top 1: 0.9111446829200528\n",
      "[Iter 3900] Loss: 0.016584422439336777 Top 1: 0.9108001538757453\n",
      "Loss: 0.016544686630368233 Top 1: 0.9108907925477745\n",
      "Total time: 67.01738397280376 in minutes\n",
      "[Iter 300] Loss: 0.01569678634405136 Top 1: 0.9181286549707602\n",
      "[Iter 600] Loss: 0.01584811508655548 Top 1: 0.9158322903629537\n",
      "[Iter 900] Loss: 0.015934299677610397 Top 1: 0.9136780650542119\n",
      "[Iter 1200] Loss: 0.015824871137738228 Top 1: 0.9139566395663956\n",
      "[Iter 1500] Loss: 0.016102507710456848 Top 1: 0.9125396031349008\n",
      "[Iter 1800] Loss: 0.01638064533472061 Top 1: 0.9120466861192164\n",
      "[Iter 2100] Loss: 0.01635172590613365 Top 1: 0.9117541979278314\n",
      "[Iter 2400] Loss: 0.016144882887601852 Top 1: 0.9123684484734813\n",
      "[Iter 2700] Loss: 0.016285140067338943 Top 1: 0.9115726590719644\n",
      "[Iter 3000] Loss: 0.01631629280745983 Top 1: 0.911248645494707\n",
      "[Iter 3300] Loss: 0.01648412086069584 Top 1: 0.9108320072743805\n",
      "[Iter 3600] Loss: 0.01657913625240326 Top 1: 0.910259081753143\n",
      "[Iter 3900] Loss: 0.016473939642310143 Top 1: 0.9110245560043598\n",
      "Loss: 0.01648668572306633 Top 1: 0.9110705085964177\n",
      "Total time: 67.05300750732422 in minutes\n",
      "[Iter 300] Loss: 0.014655851759016514 Top 1: 0.9193817878028404\n",
      "[Iter 600] Loss: 0.015910670161247253 Top 1: 0.9148936170212766\n",
      "[Iter 900] Loss: 0.016221236437559128 Top 1: 0.9129830414234084\n",
      "[Iter 1200] Loss: 0.01673547923564911 Top 1: 0.9114550760892224\n",
      "[Iter 1500] Loss: 0.016911737620830536 Top 1: 0.9102467900616975\n",
      "[Iter 1800] Loss: 0.01675250381231308 Top 1: 0.9105530081978602\n",
      "[Iter 2100] Loss: 0.01679105870425701 Top 1: 0.9101464808860307\n",
      "[Iter 2400] Loss: 0.01682008057832718 Top 1: 0.9105449619672814\n",
      "[Iter 2700] Loss: 0.016879621893167496 Top 1: 0.9100676113735297\n",
      "[Iter 3000] Loss: 0.016704238951206207 Top 1: 0.9107901975493874\n",
      "[Iter 3300] Loss: 0.016541089862585068 Top 1: 0.9115708115480791\n",
      "[Iter 3600] Loss: 0.016523776575922966 Top 1: 0.9116656247829409\n",
      "[Iter 3900] Loss: 0.016588158905506134 Top 1: 0.9112329294095018\n",
      "Loss: 0.01654447428882122 Top 1: 0.9112352483076739\n",
      "Total time: 67.0369968811671 in minutes\n",
      "[Iter 300] Loss: 0.015256653539836407 Top 1: 0.9172932330827067\n",
      "[Iter 600] Loss: 0.01527348905801773 Top 1: 0.9163537755527743\n",
      "[Iter 900] Loss: 0.016328826546669006 Top 1: 0.9136780650542119\n",
      "[Iter 1200] Loss: 0.016411127522587776 Top 1: 0.9133312486971024\n",
      "[Iter 1500] Loss: 0.01637919619679451 Top 1: 0.9134150408537602\n",
      "[Iter 1800] Loss: 0.016235405579209328 Top 1: 0.9135403640405725\n",
      "[Iter 2100] Loss: 0.01615382358431816 Top 1: 0.9134512325830654\n",
      "[Iter 2400] Loss: 0.016169892624020576 Top 1: 0.9132541419193498\n",
      "[Iter 2700] Loss: 0.016244009137153625 Top 1: 0.9130313976104474\n",
      "[Iter 3000] Loss: 0.016308123245835304 Top 1: 0.9126656664166042\n",
      "[Iter 3300] Loss: 0.01627848483622074 Top 1: 0.9127263772069409\n",
      "[Iter 3600] Loss: 0.016301322728395462 Top 1: 0.9124123081197472\n",
      "[Iter 3900] Loss: 0.016419634222984314 Top 1: 0.9119061357953453\n",
      "Loss: 0.016453169286251068 Top 1: 0.9116545857545079\n",
      "Total time: 67.0130684375763 in minutes\n",
      "[Iter 300] Loss: 0.017204253003001213 Top 1: 0.9064327485380117\n",
      "[Iter 600] Loss: 0.0173175185918808 Top 1: 0.9090529828952858\n",
      "[Iter 900] Loss: 0.016819436103105545 Top 1: 0.9105504587155964\n",
      "[Iter 1200] Loss: 0.016567567363381386 Top 1: 0.9116635397123202\n",
      "[Iter 1500] Loss: 0.016755208373069763 Top 1: 0.9104552276138069\n",
      "[Iter 1800] Loss: 0.01678348146378994 Top 1: 0.9106224815895512\n",
      "[Iter 2100] Loss: 0.016885049641132355 Top 1: 0.9101464808860307\n",
      "[Iter 2400] Loss: 0.016827978193759918 Top 1: 0.9101542148588101\n",
      "[Iter 2700] Loss: 0.01670690067112446 Top 1: 0.9109937945725665\n",
      "[Iter 3000] Loss: 0.016779758036136627 Top 1: 0.9101858798032841\n",
      "[Iter 3300] Loss: 0.01671415939927101 Top 1: 0.910282639993938\n",
      "[Iter 3600] Loss: 0.016734594479203224 Top 1: 0.9099291519066472\n",
      "[Iter 3900] Loss: 0.016778210178017616 Top 1: 0.9095819708918381\n",
      "Loss: 0.01678634062409401 Top 1: 0.9096178038698856\n",
      "Total time: 66.91076761484146 in minutes\n",
      "[Iter 300] Loss: 0.01847965642809868 Top 1: 0.9064327485380117\n",
      "[Iter 600] Loss: 0.018092304468154907 Top 1: 0.906654151022111\n",
      "[Iter 900] Loss: 0.0175793394446373 Top 1: 0.9092299138170697\n",
      "[Iter 1200] Loss: 0.017226416617631912 Top 1: 0.9090056285178236\n",
      "[Iter 1500] Loss: 0.01717044785618782 Top 1: 0.9097048524262131\n",
      "[Iter 1800] Loss: 0.016905561089515686 Top 1: 0.9101709045435599\n",
      "[Iter 2100] Loss: 0.01696217991411686 Top 1: 0.9099380731213529\n",
      "[Iter 2400] Loss: 0.016986574977636337 Top 1: 0.9100760654371157\n",
      "[Iter 2700] Loss: 0.01694735698401928 Top 1: 0.9099981476336019\n",
      "[Iter 3000] Loss: 0.01683512143790722 Top 1: 0.9108735517212636\n",
      "[Iter 3300] Loss: 0.016839435324072838 Top 1: 0.91018792149731\n",
      "[Iter 3600] Loss: 0.01679283380508423 Top 1: 0.9103111759394318\n",
      "[Iter 3900] Loss: 0.01671857014298439 Top 1: 0.9105757517471309\n",
      "Loss: 0.016740674152970314 Top 1: 0.9106361948121967\n",
      "Total time: 66.91813269456227 in minutes\n",
      "[Iter 300] Loss: 0.01724272593855858 Top 1: 0.9114452798663325\n",
      "[Iter 600] Loss: 0.016801048070192337 Top 1: 0.9125990821860659\n",
      "[Iter 900] Loss: 0.016757167875766754 Top 1: 0.9128440366972477\n",
      "[Iter 1200] Loss: 0.016651449725031853 Top 1: 0.9123410464873879\n",
      "[Iter 1500] Loss: 0.016730384901165962 Top 1: 0.9118726029681508\n",
      "[Iter 1800] Loss: 0.016699383035302162 Top 1: 0.9114561622898429\n",
      "[Iter 2100] Loss: 0.01665971241891384 Top 1: 0.9119328331546981\n",
      "[Iter 2400] Loss: 0.016832828521728516 Top 1: 0.9112222569552986\n",
      "[Iter 2700] Loss: 0.01680356077849865 Top 1: 0.910276002593313\n",
      "[Iter 3000] Loss: 0.01680433191359043 Top 1: 0.9099358172876553\n",
      "[Iter 3300] Loss: 0.01676815189421177 Top 1: 0.9102258088959612\n",
      "[Iter 3600] Loss: 0.016770590096712112 Top 1: 0.9100507050079878\n",
      "[Iter 3900] Loss: 0.016769135370850563 Top 1: 0.9100307751490672\n",
      "Loss: 0.0167151540517807 Top 1: 0.9101419756784281\n",
      "Total time: 66.95407494306565 in minutes\n",
      "[Iter 300] Loss: 0.01730281300842762 Top 1: 0.9072681704260651\n",
      "[Iter 600] Loss: 0.017193669453263283 Top 1: 0.9102002503128911\n",
      "[Iter 900] Loss: 0.016876092180609703 Top 1: 0.9111759799833195\n",
      "[Iter 1200] Loss: 0.016782373189926147 Top 1: 0.9110381488430269\n",
      "[Iter 1500] Loss: 0.016726428642868996 Top 1: 0.9119976654994164\n",
      "[Iter 1800] Loss: 0.016569359228014946 Top 1: 0.913158260386272\n",
      "[Iter 2100] Loss: 0.016484489664435387 Top 1: 0.9125580564487317\n",
      "[Iter 2400] Loss: 0.01679926924407482 Top 1: 0.9118214025216214\n",
      "[Iter 2700] Loss: 0.016792459413409233 Top 1: 0.9116652773918681\n",
      "[Iter 3000] Loss: 0.016661373898386955 Top 1: 0.91208218721347\n",
      "[Iter 3300] Loss: 0.01667553000152111 Top 1: 0.9117791922406607\n",
      "[Iter 3600] Loss: 0.016754858195781708 Top 1: 0.9116829895117038\n",
      "[Iter 3900] Loss: 0.016758156940340996 Top 1: 0.9116977623902033\n",
      "Loss: 0.016733014956116676 Top 1: 0.9115048223806386\n",
      "Total time: 67.01392015218735 in minutes\n",
      "[Iter 300] Loss: 0.016590869054198265 Top 1: 0.9122807017543859\n",
      "[Iter 600] Loss: 0.017397090792655945 Top 1: 0.9075928243637881\n",
      "[Iter 900] Loss: 0.017691390588879585 Top 1: 0.9073533500139005\n",
      "[Iter 1200] Loss: 0.017519468441605568 Top 1: 0.9084323535543047\n",
      "[Iter 1500] Loss: 0.017230430617928505 Top 1: 0.9096631649157912\n",
      "[Iter 1800] Loss: 0.017123838886618614 Top 1: 0.9103445880227873\n",
      "[Iter 2100] Loss: 0.017123019322752953 Top 1: 0.9092533047516971\n",
      "[Iter 2400] Loss: 0.017194265499711037 Top 1: 0.9091903719912473\n",
      "[Iter 2700] Loss: 0.017120273783802986 Top 1: 0.9094424377141799\n",
      "[Iter 3000] Loss: 0.017131177708506584 Top 1: 0.9096024006001501\n",
      "[Iter 3300] Loss: 0.017007173970341682 Top 1: 0.9097901038114723\n",
      "[Iter 3600] Loss: 0.01693646050989628 Top 1: 0.9102069875668543\n",
      "[Iter 3900] Loss: 0.016927406191825867 Top 1: 0.9104795794062961\n",
      "Loss: 0.016848450526595116 Top 1: 0.9109656742347092\n",
      "Total time: 66.9670607884725 in minutes\n"
     ]
    }
   ],
   "source": [
    "run_model(model_r152_10, train_generator, epochs=30, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a46409e-4d9a-4a68-a084-314d63a32d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'initial_conv': <tf.Tensor 'model/StatefulPartitionedCall:5' shape=(16, 128, 128, 192) dtype=float32>, 'block_group2': <tf.Tensor 'model/StatefulPartitionedCall:1' shape=(16, 32, 32, 1536) dtype=float32>, 'logits_sup': <tf.Tensor 'model/StatefulPartitionedCall:7' shape=(16, 1000) dtype=float32>, 'block_group3': <tf.Tensor 'model/StatefulPartitionedCall:2' shape=(16, 16, 16, 3072) dtype=float32>, 'block_group4': <tf.Tensor 'model/StatefulPartitionedCall:3' shape=(16, 8, 8, 6144) dtype=float32>, 'block_group1': <tf.Tensor 'model/StatefulPartitionedCall:0' shape=(16, 64, 64, 768) dtype=float32>, 'initial_max_pool': <tf.Tensor 'model/StatefulPartitionedCall:6' shape=(16, 64, 64, 192) dtype=float32>, 'final_avg_pool': <tf.Tensor 'model/StatefulPartitionedCall:4' shape=(16, 6144) dtype=float32>}\n",
      "[Iter 300] Loss: 0.005186954513192177 Top 1: 0.9745833333333334\n",
      "[Iter 600] Loss: 0.012424014508724213 Top 1: 0.9392708333333334\n",
      "[Iter 900] Loss: 0.020019512623548508 Top 1: 0.9013888888888889\n",
      "Loss: 0.018082132562994957 Top 1: 0.9104745925215724\n",
      "Total time: 17.272078434626263 in minutes\n"
     ]
    }
   ],
   "source": [
    "run_model(model_r152_10, valid_generator, epochs=1, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526b2ea1-4506-482d-a7ed-e7840dc80a06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
