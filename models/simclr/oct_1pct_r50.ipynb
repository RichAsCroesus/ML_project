{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a261348-3abc-49da-aa9e-773b5b78c024",
   "metadata": {},
   "source": [
    "### General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2130f56d-dd15-4519-ae64-7905fa9ddf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import abs1\n",
    "import itertools\n",
    "import os\n",
    "import more_itertools\n",
    "import sys\n",
    "import time\n",
    "from absl import flags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7057deeb-ce61-4126-b15a-21a10684817b",
   "metadata": {},
   "source": [
    "## Tensorflow and keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de4a64c1-14b1-40d1-a40c-cd23c1ecde6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import keras\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "tf.compat.v1.enable_v2_behavior()\n",
    "# import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub \n",
    "from keras.applications.resnet_v2 import ResNet101V2\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.layers import Input, GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import (Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, UpSampling2D, \n",
    "                          concatenate, GlobalAveragePooling2D, Input)\n",
    "from keras.regularizers import l2, l1_l2\n",
    "from keras.constraints import MaxNorm\n",
    "from keras.models import load_model, Sequential, Model\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet_v2 import ResNet50V2, ResNet152V2\n",
    "from keras.applications.xception import Xception\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.callbacks import Callback, EarlyStopping, LearningRateScheduler, ModelCheckpoint, LearningRateScheduler, TensorBoard\n",
    "import sklearn as sk \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fae050-4b87-41a3-872c-5fa21bda5b21",
   "metadata": {},
   "source": [
    "## SimCLRv2 colab fine tuning code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26fece07-762f-493d-9947-b16426dd8f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS_color_jitter_strength = 0.3\n",
    "CROP_PROPORTION = 0.875  # Standard for ImageNet.\n",
    "\n",
    "\n",
    "def random_apply(func, p, x):\n",
    "  \"\"\"Randomly apply function func to x with probability p.\"\"\"\n",
    "  return tf.cond(\n",
    "      tf.less(tf.random_uniform([], minval=0, maxval=1, dtype=tf.float32),\n",
    "              tf.cast(p, tf.float32)),\n",
    "      lambda: func(x),\n",
    "      lambda: x)\n",
    "\n",
    "\n",
    "def random_brightness(image, max_delta, impl='simclrv2'):\n",
    "  \"\"\"A multiplicative vs additive change of brightness.\"\"\"\n",
    "  if impl == 'simclrv2':\n",
    "    factor = tf.random_uniform(\n",
    "        [], tf.maximum(1.0 - max_delta, 0), 1.0 + max_delta)\n",
    "    image = image * factor\n",
    "  elif impl == 'simclrv1':\n",
    "    image = random_brightness(image, max_delta=max_delta)\n",
    "  else:\n",
    "    raise ValueError('Unknown impl {} for random brightness.'.format(impl))\n",
    "  return image\n",
    "\n",
    "\n",
    "def to_grayscale(image, keep_channels=True):\n",
    "  image = tf.image.rgb_to_grayscale(image)\n",
    "  if keep_channels:\n",
    "    image = tf.tile(image, [1, 1, 3])\n",
    "  return image\n",
    "\n",
    "\n",
    "def color_jitter(image,\n",
    "                 strength,\n",
    "                 random_order=True):\n",
    "  \"\"\"Distorts the color of the image.\n",
    "  Args:\n",
    "    image: The input image tensor.\n",
    "    strength: the floating number for the strength of the color augmentation.\n",
    "    random_order: A bool, specifying whether to randomize the jittering order.\n",
    "  Returns:\n",
    "    The distorted image tensor.\n",
    "  \"\"\"\n",
    "  brightness = 0.8 * strength\n",
    "  contrast = 0.8 * strength\n",
    "  saturation = 0.8 * strength\n",
    "  hue = 0.2 * strength\n",
    "  if random_order:\n",
    "    return color_jitter_rand(image, brightness, contrast, saturation, hue)\n",
    "  else:\n",
    "    return color_jitter_nonrand(image, brightness, contrast, saturation, hue)\n",
    "\n",
    "\n",
    "def color_jitter_nonrand(image, brightness=0, contrast=0, saturation=0, hue=0):\n",
    "  \"\"\"Distorts the color of the image (jittering order is fixed).\n",
    "  Args:\n",
    "    image: The input image tensor.\n",
    "    brightness: A float, specifying the brightness for color jitter.\n",
    "    contrast: A float, specifying the contrast for color jitter.\n",
    "    saturation: A float, specifying the saturation for color jitter.\n",
    "    hue: A float, specifying the hue for color jitter.\n",
    "  Returns:\n",
    "    The distorted image tensor.\n",
    "  \"\"\"\n",
    "  with tf.name_scope('distort_color'):\n",
    "    def apply_transform(i, x, brightness, contrast, saturation, hue):\n",
    "      \"\"\"Apply the i-th transformation.\"\"\"\n",
    "      if brightness != 0 and i == 0:\n",
    "        x = random_brightness(x, max_delta=brightness)\n",
    "      elif contrast != 0 and i == 1:\n",
    "        x = tf.image.random_contrast(\n",
    "            x, lower=1-contrast, upper=1+contrast)\n",
    "      elif saturation != 0 and i == 2:\n",
    "        x = tf.image.random_saturation(\n",
    "            x, lower=1-saturation, upper=1+saturation)\n",
    "      elif hue != 0:\n",
    "        x = tf.image.random_hue(x, max_delta=hue)\n",
    "      return x\n",
    "\n",
    "    for i in range(4):\n",
    "      image = apply_transform(i, image, brightness, contrast, saturation, hue)\n",
    "      image = tf.clip_by_value(image, 0., 1.)\n",
    "    return image\n",
    "\n",
    "\n",
    "def color_jitter_rand(image, brightness=0, contrast=0, saturation=0, hue=0):\n",
    "  \"\"\"Distorts the color of the image (jittering order is random).\n",
    "  Args:\n",
    "    image: The input image tensor.\n",
    "    brightness: A float, specifying the brightness for color jitter.\n",
    "    contrast: A float, specifying the contrast for color jitter.\n",
    "    saturation: A float, specifying the saturation for color jitter.\n",
    "    hue: A float, specifying the hue for color jitter.\n",
    "  Returns:\n",
    "    The distorted image tensor.\n",
    "  \"\"\"\n",
    "  with tf.name_scope('distort_color'):\n",
    "    def apply_transform(i, x):\n",
    "      \"\"\"Apply the i-th transformation.\"\"\"\n",
    "      def brightness_foo():\n",
    "        if brightness == 0:\n",
    "          return x\n",
    "        else:\n",
    "          return random_brightness(x, max_delta=brightness)\n",
    "      def contrast_foo():\n",
    "        if contrast == 0:\n",
    "          return x\n",
    "        else:\n",
    "          return tf.image.random_contrast(x, lower=1-contrast, upper=1+contrast)\n",
    "      def saturation_foo():\n",
    "        if saturation == 0:\n",
    "          return x\n",
    "        else:\n",
    "          return tf.image.random_saturation(\n",
    "              x, lower=1-saturation, upper=1+saturation)\n",
    "      def hue_foo():\n",
    "        if hue == 0:\n",
    "          return x\n",
    "        else:\n",
    "          return tf.image.random_hue(x, max_delta=hue)\n",
    "      x = tf.cond(tf.less(i, 2),\n",
    "                  lambda: tf.cond(tf.less(i, 1), brightness_foo, contrast_foo),\n",
    "                  lambda: tf.cond(tf.less(i, 3), saturation_foo, hue_foo))\n",
    "      return x\n",
    "\n",
    "    perm = tf.random_shuffle(tf.range(4))\n",
    "    for i in range(4):\n",
    "      image = apply_transform(perm[i], image)\n",
    "      image = tf.clip_by_value(image, 0., 1.)\n",
    "    return image\n",
    "\n",
    "\n",
    "def _compute_crop_shape(\n",
    "    image_height, image_width, aspect_ratio, crop_proportion):\n",
    "  \"\"\"Compute aspect ratio-preserving shape for central crop.\n",
    "  The resulting shape retains `crop_proportion` along one side and a proportion\n",
    "  less than or equal to `crop_proportion` along the other side.\n",
    "  Args:\n",
    "    image_height: Height of image to be cropped.\n",
    "    image_width: Width of image to be cropped.\n",
    "    aspect_ratio: Desired aspect ratio (width / height) of output.\n",
    "    crop_proportion: Proportion of image to retain along the less-cropped side.\n",
    "  Returns:\n",
    "    crop_height: Height of image after cropping.\n",
    "    crop_width: Width of image after cropping.\n",
    "  \"\"\"\n",
    "  image_width_float = tf.cast(image_width, tf.float32)\n",
    "  image_height_float = tf.cast(image_height, tf.float32)\n",
    "\n",
    "  def _requested_aspect_ratio_wider_than_image():\n",
    "    crop_height = tf.cast(tf.math.rint(\n",
    "        crop_proportion / aspect_ratio * image_width_float), tf.int32)\n",
    "    crop_width = tf.cast(tf.math.rint(\n",
    "        crop_proportion * image_width_float), tf.int32)\n",
    "    return crop_height, crop_width\n",
    "\n",
    "  def _image_wider_than_requested_aspect_ratio():\n",
    "    crop_height = tf.cast(\n",
    "        tf.math.rint(crop_proportion * image_height_float), tf.int32)\n",
    "    crop_width = tf.cast(tf.math.rint(\n",
    "        crop_proportion * aspect_ratio *\n",
    "        image_height_float), tf.int32)\n",
    "    return crop_height, crop_width\n",
    "\n",
    "  return tf.cond(\n",
    "      aspect_ratio > image_width_float / image_height_float,\n",
    "      _requested_aspect_ratio_wider_than_image,\n",
    "      _image_wider_than_requested_aspect_ratio)\n",
    "\n",
    "\n",
    "def center_crop(image, height, width, crop_proportion):\n",
    "  \"\"\"Crops to center of image and rescales to desired size.\n",
    "  Args:\n",
    "    image: Image Tensor to crop.\n",
    "    height: Height of image to be cropped.\n",
    "    width: Width of image to be cropped.\n",
    "    crop_proportion: Proportion of image to retain along the less-cropped side.\n",
    "  Returns:\n",
    "    A `height` x `width` x channels Tensor holding a central crop of `image`.\n",
    "  \"\"\"\n",
    "  shape = tf.shape(image)\n",
    "  image_height = shape[0]\n",
    "  image_width = shape[1]\n",
    "  crop_height, crop_width = _compute_crop_shape(\n",
    "      image_height, image_width, height / width, crop_proportion)\n",
    "  offset_height = ((image_height - crop_height) + 1) // 2\n",
    "  offset_width = ((image_width - crop_width) + 1) // 2\n",
    "  image = tf.image.crop_to_bounding_box(\n",
    "      image, offset_height, offset_width, crop_height, crop_width)\n",
    "\n",
    "  image = tf.compat.v1.image.resize_bicubic([image], [height, width])[0]\n",
    "\n",
    "  return image\n",
    "\n",
    "\n",
    "def distorted_bounding_box_crop(image,\n",
    "                                bbox,\n",
    "                                min_object_covered=0.1,\n",
    "                                aspect_ratio_range=(0.75, 1.33),\n",
    "                                area_range=(0.05, 1.0),\n",
    "                                max_attempts=100,\n",
    "                                scope=None):\n",
    "  \"\"\"Generates cropped_image using one of the bboxes randomly distorted.\n",
    "  See `tf.image.sample_distorted_bounding_box` for more documentation.\n",
    "  Args:\n",
    "    image: `Tensor` of image data.\n",
    "    bbox: `Tensor` of bounding boxes arranged `[1, num_boxes, coords]`\n",
    "        where each coordinate is [0, 1) and the coordinates are arranged\n",
    "        as `[ymin, xmin, ymax, xmax]`. If num_boxes is 0 then use the whole\n",
    "        image.\n",
    "    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n",
    "        area of the image must contain at least this fraction of any bounding\n",
    "        box supplied.\n",
    "    aspect_ratio_range: An optional list of `float`s. The cropped area of the\n",
    "        image must have an aspect ratio = width / height within this range.\n",
    "    area_range: An optional list of `float`s. The cropped area of the image\n",
    "        must contain a fraction of the supplied image within in this range.\n",
    "    max_attempts: An optional `int`. Number of attempts at generating a cropped\n",
    "        region of the image of the specified constraints. After `max_attempts`\n",
    "        failures, return the entire image.\n",
    "    scope: Optional `str` for name scope.\n",
    "  Returns:\n",
    "    (cropped image `Tensor`, distorted bbox `Tensor`).\n",
    "  \"\"\"\n",
    "  with tf.name_scope(scope, 'distorted_bounding_box_crop', [image, bbox]):\n",
    "    shape = tf.shape(image)\n",
    "    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n",
    "        shape,\n",
    "        bounding_boxes=bbox,\n",
    "        min_object_covered=min_object_covered,\n",
    "        aspect_ratio_range=aspect_ratio_range,\n",
    "        area_range=area_range,\n",
    "        max_attempts=max_attempts,\n",
    "        use_image_if_no_bounding_boxes=True)\n",
    "    bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n",
    "\n",
    "    # Crop the image to the specified bounding box.\n",
    "    offset_y, offset_x, _ = tf.unstack(bbox_begin)\n",
    "    target_height, target_width, _ = tf.unstack(bbox_size)\n",
    "    image = tf.image.crop_to_bounding_box(\n",
    "        image, offset_y, offset_x, target_height, target_width)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def crop_and_resize(image, height, width):\n",
    "  \"\"\"Make a random crop and resize it to height `height` and width `width`.\n",
    "  Args:\n",
    "    image: Tensor representing the image.\n",
    "    height: Desired image height.\n",
    "    width: Desired image width.\n",
    "  Returns:\n",
    "    A `height` x `width` x channels Tensor holding a random crop of `image`.\n",
    "  \"\"\"\n",
    "  bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])\n",
    "  aspect_ratio = width / height\n",
    "  image = distorted_bounding_box_crop(\n",
    "      image,\n",
    "      bbox,\n",
    "      min_object_covered=0.1,\n",
    "      aspect_ratio_range=(3. / 4 * aspect_ratio, 4. / 3. * aspect_ratio),\n",
    "      area_range=(0.08, 1.0),\n",
    "      max_attempts=100,\n",
    "      scope=None)\n",
    "  return tf.compat.v1.image.resize_bicubic([image], [height, width])[0]\n",
    "\n",
    "\n",
    "def gaussian_blur(image, kernel_size, sigma, padding='SAME'):\n",
    "  \"\"\"Blurs the given image with separable convolution.\n",
    "  Args:\n",
    "    image: Tensor of shape [height, width, channels] and dtype float to blur.\n",
    "    kernel_size: Integer Tensor for the size of the blur kernel. This is should\n",
    "      be an odd number. If it is an even number, the actual kernel size will be\n",
    "      size + 1.\n",
    "    sigma: Sigma value for gaussian operator.\n",
    "    padding: Padding to use for the convolution. Typically 'SAME' or 'VALID'.\n",
    "  Returns:\n",
    "    A Tensor representing the blurred image.\n",
    "  \"\"\"\n",
    "  radius = tf.to_int32(kernel_size / 2)\n",
    "  kernel_size = radius * 2 + 1\n",
    "  x = tf.to_float(tf.range(-radius, radius + 1))\n",
    "  blur_filter = tf.exp(\n",
    "      -tf.pow(x, 2.0) / (2.0 * tf.pow(tf.to_float(sigma), 2.0)))\n",
    "  blur_filter /= tf.reduce_sum(blur_filter)\n",
    "  # One vertical and one horizontal filter.\n",
    "  blur_v = tf.reshape(blur_filter, [kernel_size, 1, 1, 1])\n",
    "  blur_h = tf.reshape(blur_filter, [1, kernel_size, 1, 1])\n",
    "  num_channels = tf.shape(image)[-1]\n",
    "  blur_h = tf.tile(blur_h, [1, 1, num_channels, 1])\n",
    "  blur_v = tf.tile(blur_v, [1, 1, num_channels, 1])\n",
    "  expand_batch_dim = image.shape.ndims == 3\n",
    "  if expand_batch_dim:\n",
    "    # Tensorflow requires batched input to convolutions, which we can fake with\n",
    "    # an extra dimension.\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "  blurred = tf.nn.depthwise_conv2d(\n",
    "      image, blur_h, strides=[1, 1, 1, 1], padding=padding)\n",
    "  blurred = tf.nn.depthwise_conv2d(\n",
    "      blurred, blur_v, strides=[1, 1, 1, 1], padding=padding)\n",
    "  if expand_batch_dim:\n",
    "    blurred = tf.squeeze(blurred, axis=0)\n",
    "  return blurred\n",
    "\n",
    "\n",
    "def random_crop_with_resize(image, height, width, p=1.0):\n",
    "  \"\"\"Randomly crop and resize an image.\n",
    "  Args:\n",
    "    image: `Tensor` representing an image of arbitrary size.\n",
    "    height: Height of output image.\n",
    "    width: Width of output image.\n",
    "    p: Probability of applying this transformation.\n",
    "  Returns:\n",
    "    A preprocessed image `Tensor`.\n",
    "  \"\"\"\n",
    "  def _transform(image):  # pylint: disable=missing-docstring\n",
    "    image = crop_and_resize(image, height, width)\n",
    "    return image\n",
    "  return random_apply(_transform, p=p, x=image)\n",
    "\n",
    "\n",
    "def random_color_jitter(image, p=1.0):\n",
    "  def _transform(image):\n",
    "    color_jitter_t = functools.partial(\n",
    "        color_jitter, strength=FLAGS_color_jitter_strength)\n",
    "    image = random_apply(color_jitter_t, p=0.8, x=image)\n",
    "    return random_apply(to_grayscale, p=0.2, x=image)\n",
    "  return random_apply(_transform, p=p, x=image)\n",
    "\n",
    "\n",
    "def random_blur(image, height, width, p=1.0):\n",
    "  \"\"\"Randomly blur an image.\n",
    "  Args:\n",
    "    image: `Tensor` representing an image of arbitrary size.\n",
    "    height: Height of output image.\n",
    "    width: Width of output image.\n",
    "    p: probability of applying this transformation.\n",
    "  Returns:\n",
    "    A preprocessed image `Tensor`.\n",
    "  \"\"\"\n",
    "  del width\n",
    "  def _transform(image):\n",
    "    sigma = tf.random.uniform([], 0.1, 2.0, dtype=tf.float32)\n",
    "    return gaussian_blur(\n",
    "        image, kernel_size=height//10, sigma=sigma, padding='SAME')\n",
    "  return random_apply(_transform, p=p, x=image)\n",
    "\n",
    "\n",
    "def batch_random_blur(images_list, height, width, blur_probability=0.5):\n",
    "  \"\"\"Apply efficient batch data transformations.\n",
    "  Args:\n",
    "    images_list: a list of image tensors.\n",
    "    height: the height of image.\n",
    "    width: the width of image.\n",
    "    blur_probability: the probaility to apply the blur operator.\n",
    "  Returns:\n",
    "    Preprocessed feature list.\n",
    "  \"\"\"\n",
    "  def generate_selector(p, bsz):\n",
    "    shape = [bsz, 1, 1, 1]\n",
    "    selector = tf.cast(\n",
    "        tf.less(tf.random_uniform(shape, 0, 1, dtype=tf.float32), p),\n",
    "        tf.float32)\n",
    "    return selector\n",
    "\n",
    "  new_images_list = []\n",
    "  for images in images_list:\n",
    "    images_new = random_blur(images, height, width, p=1.)\n",
    "    selector = generate_selector(blur_probability, tf.shape(images)[0])\n",
    "    images = images_new * selector + images * (1 - selector)\n",
    "    images = tf.clip_by_value(images, 0., 1.)\n",
    "    new_images_list.append(images)\n",
    "\n",
    "  return new_images_list\n",
    "\n",
    "\n",
    "def preprocess_for_train(image, height, width,\n",
    "                         color_distort=True, crop=True, flip=True):\n",
    "  \"\"\"Preprocesses the given image for training.\n",
    "  Args:\n",
    "    image: `Tensor` representing an image of arbitrary size.\n",
    "    height: Height of output image.\n",
    "    width: Width of output image.\n",
    "    color_distort: Whether to apply the color distortion.\n",
    "    crop: Whether to crop the image.\n",
    "    flip: Whether or not to flip left and right of an image.\n",
    "  Returns:\n",
    "    A preprocessed image `Tensor`.\n",
    "  \"\"\"\n",
    "  if crop:\n",
    "    image = random_crop_with_resize(image, height, width)\n",
    "  if flip:\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "  if color_distort:\n",
    "    image = random_color_jitter(image)\n",
    "  image = tf.reshape(image, [height, width, 3])\n",
    "  image = tf.clip_by_value(image, 0., 1.)\n",
    "  return image\n",
    "\n",
    "\n",
    "def preprocess_for_eval(image, height, width, crop=True):\n",
    "  \"\"\"Preprocesses the given image for evaluation.\n",
    "  Args:\n",
    "    image: `Tensor` representing an image of arbitrary size.\n",
    "    height: Height of output image.\n",
    "    width: Width of output image.\n",
    "    crop: Whether or not to (center) crop the test images.\n",
    "  Returns:\n",
    "    A preprocessed image `Tensor`.\n",
    "  \"\"\"\n",
    "  if crop:\n",
    "    image = center_crop(image, height, width, crop_proportion=CROP_PROPORTION)\n",
    "  image = tf.reshape(image, [height, width, 3])\n",
    "  image = tf.clip_by_value(image, 0., 1.)\n",
    "  return image\n",
    "\n",
    "\n",
    "def preprocess_image(image, height, width, is_training=False,\n",
    "                     color_distort=True, test_crop=True):\n",
    "  \"\"\"Preprocesses the given image.\n",
    "  Args:\n",
    "    image: `Tensor` representing an image of arbitrary size.\n",
    "    height: Height of output image.\n",
    "    width: Width of output image.\n",
    "    is_training: `bool` for whether the preprocessing is for training.\n",
    "    color_distort: whether to apply the color distortion.\n",
    "    test_crop: whether or not to extract a central crop of the images\n",
    "        (as for standard ImageNet evaluation) during the evaluation.\n",
    "  Returns:\n",
    "    A preprocessed image `Tensor` of range [0, 1].\n",
    "  \"\"\"\n",
    "  image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "  if is_training:\n",
    "    return preprocess_for_train(image, height, width, color_distort)\n",
    "  else:\n",
    "    return preprocess_for_eval(image, height, width, test_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e26e4cbc-3686-4f65-9dac-13625e8d8351",
   "metadata": {},
   "outputs": [],
   "source": [
    "EETA_DEFAULT = 0.001\n",
    "\n",
    "\n",
    "class LARSOptimizer(tf.keras.optimizers.Optimizer):\n",
    "  \"\"\"Layer-wise Adaptive Rate Scaling for large batch training.\n",
    "\n",
    "  Introduced by \"Large Batch Training of Convolutional Networks\" by Y. You,\n",
    "  I. Gitman, and B. Ginsburg. (https://arxiv.org/abs/1708.03888)\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               learning_rate,\n",
    "               momentum=0.9,\n",
    "               use_nesterov=False,\n",
    "               weight_decay=0.0,\n",
    "               exclude_from_weight_decay=None,\n",
    "               exclude_from_layer_adaptation=None,\n",
    "               classic_momentum=True,\n",
    "               eeta=EETA_DEFAULT,\n",
    "               name=\"LARSOptimizer\"):\n",
    "    \"\"\"Constructs a LARSOptimizer.\n",
    "\n",
    "    Args:\n",
    "      learning_rate: A `float` for learning rate.\n",
    "      momentum: A `float` for momentum.\n",
    "      use_nesterov: A 'Boolean' for whether to use nesterov momentum.\n",
    "      weight_decay: A `float` for weight decay.\n",
    "      exclude_from_weight_decay: A list of `string` for variable screening, if\n",
    "          any of the string appears in a variable's name, the variable will be\n",
    "          excluded for computing weight decay. For example, one could specify\n",
    "          the list like ['batch_normalization', 'bias'] to exclude BN and bias\n",
    "          from weight decay.\n",
    "      exclude_from_layer_adaptation: Similar to exclude_from_weight_decay, but\n",
    "          for layer adaptation. If it is None, it will be defaulted the same as\n",
    "          exclude_from_weight_decay.\n",
    "      classic_momentum: A `boolean` for whether to use classic (or popular)\n",
    "          momentum. The learning rate is applied during momeuntum update in\n",
    "          classic momentum, but after momentum for popular momentum.\n",
    "      eeta: A `float` for scaling of learning rate when computing trust ratio.\n",
    "      name: The name for the scope.\n",
    "    \"\"\"\n",
    "    super(LARSOptimizer, self).__init__(name)\n",
    "\n",
    "    self._set_hyper(\"learning_rate\", learning_rate)\n",
    "    self.momentum = momentum\n",
    "    self.weight_decay = weight_decay\n",
    "    self.use_nesterov = use_nesterov\n",
    "    self.classic_momentum = classic_momentum\n",
    "    self.eeta = eeta\n",
    "    self.exclude_from_weight_decay = exclude_from_weight_decay\n",
    "    # exclude_from_layer_adaptation is set to exclude_from_weight_decay if the\n",
    "    # arg is None.\n",
    "    if exclude_from_layer_adaptation:\n",
    "      self.exclude_from_layer_adaptation = exclude_from_layer_adaptation\n",
    "    else:\n",
    "      self.exclude_from_layer_adaptation = exclude_from_weight_decay\n",
    "\n",
    "  def _create_slots(self, var_list):\n",
    "    for v in var_list:\n",
    "      self.add_slot(v, \"Momentum\")\n",
    "\n",
    "  def _resource_apply_dense(self, grad, param, apply_state=None):\n",
    "    if grad is None or param is None:\n",
    "      return tf.no_op()\n",
    "\n",
    "    var_device, var_dtype = param.device, param.dtype.base_dtype\n",
    "    coefficients = ((apply_state or {}).get((var_device, var_dtype)) or\n",
    "                    self._fallback_apply_state(var_device, var_dtype))\n",
    "    learning_rate = coefficients[\"lr_t\"]\n",
    "\n",
    "    param_name = param.name\n",
    "\n",
    "    v = self.get_slot(param, \"Momentum\")\n",
    "\n",
    "    if self._use_weight_decay(param_name):\n",
    "      grad += self.weight_decay * param\n",
    "\n",
    "    if self.classic_momentum:\n",
    "      trust_ratio = 1.0\n",
    "      if self._do_layer_adaptation(param_name):\n",
    "        w_norm = tf.norm(param, ord=2)\n",
    "        g_norm = tf.norm(grad, ord=2)\n",
    "        trust_ratio = tf.where(\n",
    "            tf.greater(w_norm, 0),\n",
    "            tf.where(tf.greater(g_norm, 0), (self.eeta * w_norm / g_norm), 1.0),\n",
    "            1.0)\n",
    "      scaled_lr = learning_rate * trust_ratio\n",
    "\n",
    "      next_v = tf.multiply(self.momentum, v) + scaled_lr * grad\n",
    "      if self.use_nesterov:\n",
    "        update = tf.multiply(self.momentum, next_v) + scaled_lr * grad\n",
    "      else:\n",
    "        update = next_v\n",
    "      next_param = param - update\n",
    "    else:\n",
    "      next_v = tf.multiply(self.momentum, v) + grad\n",
    "      if self.use_nesterov:\n",
    "        update = tf.multiply(self.momentum, next_v) + grad\n",
    "      else:\n",
    "        update = next_v\n",
    "\n",
    "      trust_ratio = 1.0\n",
    "      if self._do_layer_adaptation(param_name):\n",
    "        w_norm = tf.norm(param, ord=2)\n",
    "        v_norm = tf.norm(update, ord=2)\n",
    "        trust_ratio = tf.where(\n",
    "            tf.greater(w_norm, 0),\n",
    "            tf.where(tf.greater(v_norm, 0), (self.eeta * w_norm / v_norm), 1.0),\n",
    "            1.0)\n",
    "      scaled_lr = trust_ratio * learning_rate\n",
    "      next_param = param - scaled_lr * update\n",
    "\n",
    "    return tf.group(*[\n",
    "        param.assign(next_param, use_locking=False),\n",
    "        v.assign(next_v, use_locking=False)\n",
    "    ])\n",
    "\n",
    "  def _use_weight_decay(self, param_name):\n",
    "    \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n",
    "    if not self.weight_decay:\n",
    "      return False\n",
    "    if self.exclude_from_weight_decay:\n",
    "      for r in self.exclude_from_weight_decay:\n",
    "        if re.search(r, param_name) is not None:\n",
    "          return False\n",
    "    return True\n",
    "\n",
    "  def _do_layer_adaptation(self, param_name):\n",
    "    \"\"\"Whether to do layer-wise learning rate adaptation for `param_name`.\"\"\"\n",
    "    if self.exclude_from_layer_adaptation:\n",
    "      for r in self.exclude_from_layer_adaptation:\n",
    "        if re.search(r, param_name) is not None:\n",
    "          return False\n",
    "    return True\n",
    "\n",
    "  def get_config(self):\n",
    "    config = super(LARSOptimizer, self).get_config()\n",
    "    config.update({\n",
    "        \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n",
    "        \"momentum\": self.momentum,\n",
    "        \"classic_momentum\": self.classic_momentum,\n",
    "        \"weight_decay\": self.weight_decay,\n",
    "        \"eeta\": self.eeta,\n",
    "        \"use_nesterov\": self.use_nesterov,\n",
    "    })\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f29e85b-6a63-4e05-b430-3390cdc00104",
   "metadata": {},
   "source": [
    "## Image libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af31c0f8-1776-48f8-b37f-514b210b49f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "# from matplotlib. import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91eeb22a-491c-4652-b469-8d377c9ac59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/OCT'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420d7755-321f-4af3-bd84-3b284d59cdc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3a362bf-8bf0-4904-ba92-9fa610e1aa38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CNV', 'DME', 'DRUSEN', 'NORMAL']\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = './OCT2017/all_data'\n",
    "data_listing = os.listdir(DATA_PATH)\n",
    "print(data_listing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4649bdff-1d92-40c8-a7a7-0a0b264a8638",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir= DATA_PATH\n",
    "CNV_PATH = os.path.join(train_dir, 'CNV')\n",
    "DME_PATH = os.path.join(train_dir, 'DME')\n",
    "NORMAL_PATH = os.path.join(train_dir, 'NORMAL')\n",
    "DRUSEN_PATH = os.path.join(train_dir, 'DRUSEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2feed38-44b2-4f6f-aa1f-5ad11304f637",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnv = os.listdir(CNV_PATH)\n",
    "dme = os.listdir(DME_PATH)\n",
    "normal = os.listdir(NORMAL_PATH)\n",
    "drusen = os.listdir(DRUSEN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c1e5d92-be42-4531-b1a6-6dd4cc904f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37205\n",
      "11348\n",
      "26315\n",
      "8616\n"
     ]
    }
   ],
   "source": [
    "print(len(cnv))\n",
    "print(len(dme))\n",
    "print(len(normal))\n",
    "print(len(drusen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8623bff1-0817-4ea3-9457-668fa82676e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 66788 images belonging to 4 classes.\n",
      "Found 16696 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "epochs=6\n",
    "batch_size=16\n",
    "steps=60000//batch_size\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255., data_format=\"channels_last\", \n",
    "                                   validation_split=0.20,\n",
    "                                   horizontal_flip=True,\n",
    "                                   rotation_range=37,\n",
    "                                   vertical_flip=True)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    # target_size=(256, 256),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    subset='training',\n",
    "                                                    class_mode='binary',\n",
    "                                                    color_mode='rgb')\n",
    "\n",
    "\n",
    "valid_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    # target_size=(96, 96), \n",
    "                                                    batch_size=batch_size,\n",
    "                                                    shuffle=False,\n",
    "                                                    subset='validation',\n",
    "                                                    class_mode='binary',\n",
    "                                                    color_mode='rgb')\n",
    "\n",
    "\n",
    "\n",
    "best_path='./best_val_acc_'+str(int(time.time()))\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=best_path,\n",
    "    monitor='val_acc',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    initial_value_threshold=0.92)\n",
    "               \n",
    "       \n",
    "\n",
    "def _lrs(epoch, lr):\n",
    "    return lr\n",
    "    \n",
    "lrs = LearningRateScheduler(_lrs) \n",
    " \n",
    "stop = EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        min_delta=0.0001,\n",
    "        patience=10,\n",
    "        verbose=1,\n",
    "        mode='auto',\n",
    "        baseline=None,\n",
    "        restore_best_weights=True\n",
    ")\n",
    "\n",
    "stop_acc = EarlyStopping(\n",
    "            monitor='acc',\n",
    "            min_delta=0.001,\n",
    "            patience=10,\n",
    "            verbose=1,\n",
    "            mode='auto',\n",
    "            baseline=None,\n",
    "            restore_best_weights=True)\n",
    "\n",
    "logdir='./logs'\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12cdb3fd-ef9d-4ca3-9561-38ca24a66640",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "weight_decay = 0.\n",
    "num_classes = 4\n",
    "class Model(tf.keras.Model):\n",
    "  def __init__(self, path):\n",
    "    super(Model, self).__init__()\n",
    "    self.saved_model = tf.saved_model.load(path)\n",
    "    self.dense_layer = tf.keras.layers.Dense(units=num_classes, name=\"head_supervised_new\")\n",
    "    self.optimizer = LARSOptimizer(\n",
    "      learning_rate,\n",
    "      momentum=momentum,\n",
    "      weight_decay=weight_decay,\n",
    "      exclude_from_weight_decay=['batch_normalization', 'bias', 'head_supervised'])\n",
    "\n",
    "  def call(self, x, training=True):\n",
    "    with tf.GradientTape() as tape:\n",
    "      outputs = self.saved_model(x[0], trainable=False)\n",
    "      print(outputs)\n",
    "      logits_t = self.dense_layer(outputs['final_avg_pool'])\n",
    "      loss_t = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels = tf.one_hot(x[1], num_classes), logits=logits_t))\n",
    "      if training:\n",
    "          dense_layer_weights = self.dense_layer.trainable_weights\n",
    "          print('Variables to train:', dense_layer_weights)\n",
    "          grads = tape.gradient(loss_t, dense_layer_weights)\n",
    "          self.optimizer.apply_gradients(zip(grads, dense_layer_weights))\n",
    "    return loss_t, x[0], logits_t, x[1]\n",
    "\n",
    "# model = Model(\"gs://simclr-checkpoints-tf2/simclrv2/finetuned_100pct/r50_1x_sk0/saved_model/\")\n",
    "\n",
    "# Remove this for debugging.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78d85efa-6825-4d67-bbd0-c71147ef5259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(m, generator, epochs=10, training=True):\n",
    "    steps = len(generator.filenames)//generator.batch_size \n",
    "    for e in range(epochs):\n",
    "        print(f'Starting epoch: {e+1}')\n",
    "        total_correct = 0 \n",
    "        total_processed = 0 \n",
    "        total_loss = 0\n",
    "        epoch_start_time = time.time()\n",
    "        for it in range(steps):\n",
    "            x = next(generator)\n",
    "            # x[0] = preprocess_image(\n",
    "            #      x[0], 256, 256, is_training=False, color_distort=False)\n",
    "            xx = (x[0], np.int32(x[1]))\n",
    "            loss, image, logits, labels = train_step(m, xx, training=training)\n",
    "            logits = logits.numpy()\n",
    "            labels = labels.numpy()\n",
    "            pred = logits.argmax(-1)\n",
    "            correct = np.sum(pred == labels)\n",
    "            total = labels.size\n",
    "            total_processed += total\n",
    "            total_loss += loss\n",
    "            total_correct += correct\n",
    "            if (it+1) % 300 == 0:\n",
    "                print(\"[Iter {}] Loss: {} Top 1: {}\".format(it+1, total_loss/total_processed, total_correct/float(total_processed)))\n",
    "        run_time = (time.time() - epoch_start_time)/60 \n",
    "        print(f\"Loss: {total_loss/total_processed} Top 1: {total_correct/total_processed}\")\n",
    "        print(f\"Total time: {run_time} in minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e316fad-dff5-4e65-9848-090acf242d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "977e3f72-b302-4ecc-b771-21bae17b49b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(m, x, training=True):\n",
    "  return m(x, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f868baa-e328-4e6d-9850-6ae74e65b9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 20:55:35.858060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-26 20:55:35.877262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-26 20:55:35.877376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-26 20:55:35.877864: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-26 20:55:35.878434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-26 20:55:35.878548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-26 20:55:35.878642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-26 20:55:36.144639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-26 20:55:36.144774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-26 20:55:36.144874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-26 20:55:36.144964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10248 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:08:00.0, compute capability: 8.6\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_9_layer_call_and_return_conditional_losses_20526) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_54_layer_call_and_return_conditional_losses_30270) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_60_layer_call_and_return_conditional_losses_31578) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_15_layer_call_and_return_conditional_losses_44504) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_35_layer_call_and_return_conditional_losses_26166) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_31_layer_call_and_return_conditional_losses_49664) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_20_layer_call_and_return_conditional_losses_22914) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_70_layer_call_and_return_conditional_losses_47984) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_47_layer_call_and_return_conditional_losses_28758) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_24_layer_call_and_return_conditional_losses_45104) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_43_layer_call_and_return_conditional_losses_50264) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_19_layer_call_and_return_conditional_losses_22698) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_5_layer_call_and_return_conditional_losses_48104) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_36_layer_call_and_return_conditional_losses_26382) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_3_layer_call_and_return_conditional_losses_43784) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_23_layer_call_and_return_conditional_losses_49184) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_30_layer_call_and_return_conditional_losses_49544) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_29_layer_call_and_return_conditional_losses_45464) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_33_layer_call_and_return_conditional_losses_45704) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_4_layer_call_and_return_conditional_losses_19446) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_73_layer_call_and_return_conditional_losses_43670) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_17_layer_call_and_return_conditional_losses_22266) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_54_layer_call_and_return_conditional_losses_47024) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_26_layer_call_and_return_conditional_losses_24210) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_39_layer_call_and_return_conditional_losses_50024) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_11_layer_call_and_return_conditional_losses_20958) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_46_layer_call_and_return_conditional_losses_28542) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_13_layer_call_and_return_conditional_losses_21390) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_26_layer_call_and_return_conditional_losses_49304) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_63_layer_call_and_return_conditional_losses_32226) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_67_layer_call_and_return_conditional_losses_47864) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_61_layer_call_and_return_conditional_losses_51344) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_55_layer_call_and_return_conditional_losses_30486) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_3_layer_call_and_return_conditional_losses_19230) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_51_layer_call_and_return_conditional_losses_50744) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_31_layer_call_and_return_conditional_losses_25290) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_57_layer_call_and_return_conditional_losses_47144) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_42_layer_call_and_return_conditional_losses_27678) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_35_layer_call_and_return_conditional_losses_49784) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_39_layer_call_and_return_conditional_losses_27030) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_2_layer_call_and_return_conditional_losses_18990) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_53_layer_call_and_return_conditional_losses_46904) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_73_layer_call_and_return_conditional_losses_34314) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_1_layer_call_and_return_conditional_losses_43225) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_layer_call_and_return_conditional_losses_43105) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_24_layer_call_and_return_conditional_losses_23778) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_33_layer_call_and_return_conditional_losses_25734) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_49_layer_call_and_return_conditional_losses_46664) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_14_layer_call_and_return_conditional_losses_48704) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_34_layer_call_and_return_conditional_losses_25950) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_25_layer_call_and_return_conditional_losses_23994) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_55_layer_call_and_return_conditional_losses_50984) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_71_layer_call_and_return_conditional_losses_43454) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_53_layer_call_and_return_conditional_losses_30054) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_56_layer_call_and_return_conditional_losses_51104) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_32_layer_call_and_return_conditional_losses_45584) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_38_layer_call_and_return_conditional_losses_46064) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_46_layer_call_and_return_conditional_losses_46544) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_23_layer_call_and_return_conditional_losses_23562) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_45_layer_call_and_return_conditional_losses_28326) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_29_layer_call_and_return_conditional_losses_24858) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_28_layer_call_and_return_conditional_losses_24642) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_10_layer_call_and_return_conditional_losses_20742) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_40_layer_call_and_return_conditional_losses_50144) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_62_layer_call_and_return_conditional_losses_32010) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_19_layer_call_and_return_conditional_losses_48944) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_72_layer_call_and_return_conditional_losses_34126) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_21_layer_call_and_return_conditional_losses_23130) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_52_layer_call_and_return_conditional_losses_50864) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_61_layer_call_and_return_conditional_losses_31794) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_16_layer_call_and_return_conditional_losses_22050) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_40_layer_call_and_return_conditional_losses_27246) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_6_layer_call_and_return_conditional_losses_48224) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_41_layer_call_and_return_conditional_losses_46184) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_65_layer_call_and_return_conditional_losses_32658) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_36_layer_call_and_return_conditional_losses_49904) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_layer_call_and_return_conditional_losses_18558) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference___call___40673) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_5_layer_call_and_return_conditional_losses_19662) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_27_layer_call_and_return_conditional_losses_24426) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_13_layer_call_and_return_conditional_losses_48584) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_12_layer_call_and_return_conditional_losses_44384) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_60_layer_call_and_return_conditional_losses_51224) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_22_layer_call_and_return_conditional_losses_23346) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_12_layer_call_and_return_conditional_losses_21174) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_70_layer_call_and_return_conditional_losses_33738) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_66_layer_call_and_return_conditional_losses_32874) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_10_layer_call_and_return_conditional_losses_48464) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_38_layer_call_and_return_conditional_losses_26814) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_6_layer_call_and_return_conditional_losses_19878) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_30_layer_call_and_return_conditional_losses_25074) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_42_layer_call_and_return_conditional_losses_46304) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_67_layer_call_and_return_conditional_losses_33090) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_64_layer_call_and_return_conditional_losses_51464) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_25_layer_call_and_return_conditional_losses_45224) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_44_layer_call_and_return_conditional_losses_28110) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_68_layer_call_and_return_conditional_losses_51704) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_48_layer_call_and_return_conditional_losses_28974) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_17_layer_call_and_return_conditional_losses_44744) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_71_layer_call_and_return_conditional_losses_33932) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_16_layer_call_and_return_conditional_losses_44624) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_50_layer_call_and_return_conditional_losses_29406) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_28_layer_call_and_return_conditional_losses_45344) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_4_layer_call_and_return_conditional_losses_43904) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_15_layer_call_and_return_conditional_losses_21822) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_62_layer_call_and_return_conditional_losses_47504) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_8_layer_call_and_return_conditional_losses_44144) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_7_layer_call_and_return_conditional_losses_20094) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_63_layer_call_and_return_conditional_losses_47624) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_44_layer_call_and_return_conditional_losses_50384) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_57_layer_call_and_return_conditional_losses_30918) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_34_layer_call_and_return_conditional_losses_45824) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_37_layer_call_and_return_conditional_losses_26598) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_49_layer_call_and_return_conditional_losses_29190) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_14_layer_call_and_return_conditional_losses_21606) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_66_layer_call_and_return_conditional_losses_47744) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_47_layer_call_and_return_conditional_losses_50504) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_64_layer_call_and_return_conditional_losses_32442) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_11_layer_call_and_return_conditional_losses_44264) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_69_layer_call_and_return_conditional_losses_33522) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_58_layer_call_and_return_conditional_losses_31146) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_41_layer_call_and_return_conditional_losses_27462) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_2_layer_call_and_return_conditional_losses_43345) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_37_layer_call_and_return_conditional_losses_45944) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_43_layer_call_and_return_conditional_losses_27894) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_45_layer_call_and_return_conditional_losses_46424) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_18_layer_call_and_return_conditional_losses_48824) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_8_layer_call_and_return_conditional_losses_20310) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_21_layer_call_and_return_conditional_losses_44984) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_68_layer_call_and_return_conditional_losses_33306) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_27_layer_call_and_return_conditional_losses_49424) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_50_layer_call_and_return_conditional_losses_46784) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_69_layer_call_and_return_conditional_losses_51824) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_59_layer_call_and_return_conditional_losses_47384) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_52_layer_call_and_return_conditional_losses_29838) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_72_layer_call_and_return_conditional_losses_43563) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_48_layer_call_and_return_conditional_losses_50624) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_9_layer_call_and_return_conditional_losses_48344) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_65_layer_call_and_return_conditional_losses_51584) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_58_layer_call_and_return_conditional_losses_47264) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_59_layer_call_and_return_conditional_losses_31362) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_32_layer_call_and_return_conditional_losses_25506) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_1_layer_call_and_return_conditional_losses_18774) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_56_layer_call_and_return_conditional_losses_30702) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_22_layer_call_and_return_conditional_losses_49064) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_20_layer_call_and_return_conditional_losses_44864) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_51_layer_call_and_return_conditional_losses_29622) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_7_layer_call_and_return_conditional_losses_44024) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_18_layer_call_and_return_conditional_losses_22482) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    }
   ],
   "source": [
    "model_r50 = Model(\"/OCT/1pct/r50_2x_sk1/saved_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca4816b0-5960-4008-b340-8f6df2b32ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer=keras.optimizers.adam_v2.Adam(learning_rate=0.000001), loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "model_r50.compile(loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31526744-2c56-46a5-859d-cbe8c28b27d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CNV': 0, 'DME': 1, 'DRUSEN': 2, 'NORMAL': 3}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20b31431-d5c0-4373-a44b-2caafda2612d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3750\n",
      "6\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(steps)\n",
    "print(epochs)\n",
    "print(batch_size)\n",
    "# res101_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425613e8-3d69-4330-9534-51bcef3c2cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d1734d0-2c69-4f8e-bb2d-7c1e29833ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'block_group3': <tf.Tensor 'model/StatefulPartitionedCall:2' shape=(16, 16, 16, 2048) dtype=float32>, 'block_group1': <tf.Tensor 'model/StatefulPartitionedCall:0' shape=(16, 64, 64, 512) dtype=float32>, 'logits_sup': <tf.Tensor 'model/StatefulPartitionedCall:7' shape=(16, 1000) dtype=float32>, 'block_group4': <tf.Tensor 'model/StatefulPartitionedCall:3' shape=(16, 8, 8, 4096) dtype=float32>, 'final_avg_pool': <tf.Tensor 'model/StatefulPartitionedCall:4' shape=(16, 4096) dtype=float32>, 'initial_max_pool': <tf.Tensor 'model/StatefulPartitionedCall:6' shape=(16, 64, 64, 128) dtype=float32>, 'block_group2': <tf.Tensor 'model/StatefulPartitionedCall:1' shape=(16, 32, 32, 1024) dtype=float32>, 'initial_conv': <tf.Tensor 'model/StatefulPartitionedCall:5' shape=(16, 128, 128, 128) dtype=float32>}\n",
      "Variables to train: [<tf.Variable 'model/head_supervised_new/kernel:0' shape=(4096, 4) dtype=float32>, <tf.Variable 'model/head_supervised_new/bias:0' shape=(4,) dtype=float32>]\n",
      "{'block_group3': <tf.Tensor 'model/StatefulPartitionedCall:2' shape=(16, 16, 16, 2048) dtype=float32>, 'block_group1': <tf.Tensor 'model/StatefulPartitionedCall:0' shape=(16, 64, 64, 512) dtype=float32>, 'logits_sup': <tf.Tensor 'model/StatefulPartitionedCall:7' shape=(16, 1000) dtype=float32>, 'block_group4': <tf.Tensor 'model/StatefulPartitionedCall:3' shape=(16, 8, 8, 4096) dtype=float32>, 'final_avg_pool': <tf.Tensor 'model/StatefulPartitionedCall:4' shape=(16, 4096) dtype=float32>, 'initial_max_pool': <tf.Tensor 'model/StatefulPartitionedCall:6' shape=(16, 64, 64, 128) dtype=float32>, 'block_group2': <tf.Tensor 'model/StatefulPartitionedCall:1' shape=(16, 32, 32, 1024) dtype=float32>, 'initial_conv': <tf.Tensor 'model/StatefulPartitionedCall:5' shape=(16, 128, 128, 128) dtype=float32>}\n",
      "Variables to train: [<tf.Variable 'model/head_supervised_new/kernel:0' shape=(4096, 4) dtype=float32>, <tf.Variable 'model/head_supervised_new/bias:0' shape=(4,) dtype=float32>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 20:55:44.457201: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8302\n",
      "2022-04-26 20:55:46.702766: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 300] Loss: 0.052633561193943024 Top 1: 0.6995833333333333\n",
      "[Iter 600] Loss: 0.04515643045306206 Top 1: 0.7473958333333334\n",
      "[Iter 900] Loss: 0.04109238088130951 Top 1: 0.76875\n",
      "[Iter 1200] Loss: 0.03852386400103569 Top 1: 0.7822395833333333\n",
      "[Iter 1500] Loss: 0.03667529299855232 Top 1: 0.7924583333333334\n",
      "[Iter 1800] Loss: 0.035158079117536545 Top 1: 0.8007291666666667\n",
      "[Iter 2100] Loss: 0.03380637988448143 Top 1: 0.8081547619047619\n",
      "[Iter 2400] Loss: 0.032674264162778854 Top 1: 0.813984375\n",
      "[Iter 2700] Loss: 0.03186734393239021 Top 1: 0.8188425925925926\n",
      "[Iter 3000] Loss: 0.031061403453350067 Top 1: 0.8235208333333334\n",
      "[Iter 3300] Loss: 0.030487388372421265 Top 1: 0.82625\n",
      "[Iter 3600] Loss: 0.02986074611544609 Top 1: 0.8301736111111111\n",
      "[Iter 3900] Loss: 0.029320547357201576 Top 1: 0.8332211538461538\n",
      "Loss: 0.028898844495415688 Top 1: 0.83558936272161\n",
      "Total time: 37.35750759840012 in minutes\n",
      "{'block_group3': <tf.Tensor 'model/StatefulPartitionedCall:2' shape=(4, 16, 16, 2048) dtype=float32>, 'block_group1': <tf.Tensor 'model/StatefulPartitionedCall:0' shape=(4, 64, 64, 512) dtype=float32>, 'logits_sup': <tf.Tensor 'model/StatefulPartitionedCall:7' shape=(4, 1000) dtype=float32>, 'block_group4': <tf.Tensor 'model/StatefulPartitionedCall:3' shape=(4, 8, 8, 4096) dtype=float32>, 'final_avg_pool': <tf.Tensor 'model/StatefulPartitionedCall:4' shape=(4, 4096) dtype=float32>, 'initial_max_pool': <tf.Tensor 'model/StatefulPartitionedCall:6' shape=(4, 64, 64, 128) dtype=float32>, 'block_group2': <tf.Tensor 'model/StatefulPartitionedCall:1' shape=(4, 32, 32, 1024) dtype=float32>, 'initial_conv': <tf.Tensor 'model/StatefulPartitionedCall:5' shape=(4, 128, 128, 128) dtype=float32>}\n",
      "Variables to train: [<tf.Variable 'model/head_supervised_new/kernel:0' shape=(4096, 4) dtype=float32>, <tf.Variable 'model/head_supervised_new/bias:0' shape=(4,) dtype=float32>]\n",
      "[Iter 300] Loss: 0.021881351247429848 Top 1: 0.870718462823726\n",
      "[Iter 600] Loss: 0.022081689909100533 Top 1: 0.8718189403420943\n",
      "[Iter 900] Loss: 0.02162237837910652 Top 1: 0.8751737559077009\n",
      "[Iter 1200] Loss: 0.021373141556978226 Top 1: 0.878413591828226\n",
      "[Iter 1500] Loss: 0.021613800898194313 Top 1: 0.8771052192763048\n",
      "[Iter 1800] Loss: 0.02157345600426197 Top 1: 0.878213144365708\n",
      "[Iter 2100] Loss: 0.021369675174355507 Top 1: 0.8800166726211742\n",
      "[Iter 2400] Loss: 0.021327033638954163 Top 1: 0.8797801396269668\n",
      "[Iter 2700] Loss: 0.021370207890868187 Top 1: 0.8792488654255812\n",
      "[Iter 3000] Loss: 0.021323131397366524 Top 1: 0.8791989664082688\n",
      "[Iter 3300] Loss: 0.02131674997508526 Top 1: 0.8798780025763431\n",
      "[Iter 3600] Loss: 0.021138733252882957 Top 1: 0.880930054872543\n",
      "[Iter 3900] Loss: 0.02108972892165184 Top 1: 0.8812431877925242\n",
      "Loss: 0.02110632322728634 Top 1: 0.8809530941113042\n",
      "Total time: 21.290590063730875 in minutes\n",
      "[Iter 300] Loss: 0.019392095506191254 Top 1: 0.8886800334168755\n",
      "[Iter 600] Loss: 0.019603613764047623 Top 1: 0.8877763871506049\n",
      "[Iter 900] Loss: 0.01950087957084179 Top 1: 0.8883792048929664\n",
      "[Iter 1200] Loss: 0.019475487992167473 Top 1: 0.8882634980195956\n",
      "[Iter 1500] Loss: 0.01949935592710972 Top 1: 0.8884442221110556\n",
      "[Iter 1800] Loss: 0.01942066103219986 Top 1: 0.8890509934695012\n",
      "[Iter 2100] Loss: 0.01955532468855381 Top 1: 0.8883232106704776\n",
      "[Iter 2400] Loss: 0.01951517164707184 Top 1: 0.8885068250494946\n",
      "[Iter 2700] Loss: 0.01955680549144745 Top 1: 0.8886033157358526\n",
      "[Iter 3000] Loss: 0.01953074336051941 Top 1: 0.8888263732599817\n",
      "[Iter 3300] Loss: 0.019472554326057434 Top 1: 0.8887625975600515\n",
      "[Iter 3600] Loss: 0.019482046365737915 Top 1: 0.8888483711884421\n",
      "[Iter 3900] Loss: 0.019445352256298065 Top 1: 0.8890812335705585\n",
      "Loss: 0.019446007907390594 Top 1: 0.8888456239142155\n",
      "Total time: 21.11542615095774 in minutes\n",
      "[Iter 300] Loss: 0.018291402608156204 Top 1: 0.899749373433584\n",
      "[Iter 600] Loss: 0.018485045060515404 Top 1: 0.8952857738840216\n",
      "[Iter 900] Loss: 0.018824759870767593 Top 1: 0.8926188490408674\n",
      "[Iter 1200] Loss: 0.018860554322600365 Top 1: 0.8915989159891599\n",
      "[Iter 1500] Loss: 0.018941253423690796 Top 1: 0.8921960980490246\n",
      "[Iter 1800] Loss: 0.01868484914302826 Top 1: 0.8938793941920244\n",
      "[Iter 2100] Loss: 0.01883860118687153 Top 1: 0.8931463617958795\n",
      "[Iter 2400] Loss: 0.01874384842813015 Top 1: 0.8937167864957799\n",
      "[Iter 2700] Loss: 0.01870054192841053 Top 1: 0.89332685005094\n",
      "[Iter 3000] Loss: 0.018646318465471268 Top 1: 0.8935358839709927\n",
      "[Iter 3300] Loss: 0.018703879788517952 Top 1: 0.8932522543002197\n",
      "[Iter 3600] Loss: 0.018690668046474457 Top 1: 0.893241647565465\n",
      "[Iter 3900] Loss: 0.01882600039243698 Top 1: 0.8924632942232481\n",
      "Loss: 0.018789930269122124 Top 1: 0.8928443059965255\n",
      "Total time: 21.202166152000427 in minutes\n",
      "[Iter 300] Loss: 0.018000176176428795 Top 1: 0.8924394319131161\n",
      "[Iter 600] Loss: 0.018418511375784874 Top 1: 0.8940342094284522\n",
      "[Iter 900] Loss: 0.018058214336633682 Top 1: 0.8958854601056436\n",
      "[Iter 1200] Loss: 0.018036413937807083 Top 1: 0.8966020429435063\n",
      "[Iter 1500] Loss: 0.018008410930633545 Top 1: 0.8968234117058529\n",
      "[Iter 1800] Loss: 0.018317971378564835 Top 1: 0.8951993886341532\n",
      "[Iter 2100] Loss: 0.018359752371907234 Top 1: 0.8942181731570799\n",
      "[Iter 2400] Loss: 0.01826227456331253 Top 1: 0.8950713764718141\n",
      "[Iter 2700] Loss: 0.018156886100769043 Top 1: 0.8960822450680744\n",
      "[Iter 3000] Loss: 0.018185455352067947 Top 1: 0.8959114778694673\n",
      "[Iter 3300] Loss: 0.018265122547745705 Top 1: 0.8954686671213155\n",
      "[Iter 3600] Loss: 0.01832033507525921 Top 1: 0.8955685212196985\n",
      "[Iter 3900] Loss: 0.01834271103143692 Top 1: 0.8956690389177406\n",
      "Loss: 0.018334899097681046 Top 1: 0.8955400467261726\n",
      "Total time: 21.33469122250875 in minutes\n",
      "[Iter 300] Loss: 0.018975313752889633 Top 1: 0.8959899749373433\n",
      "[Iter 600] Loss: 0.01834600418806076 Top 1: 0.8974760116812682\n",
      "[Iter 900] Loss: 0.017990080639719963 Top 1: 0.8988740617180984\n",
      "[Iter 1200] Loss: 0.018072430044412613 Top 1: 0.8984782155513863\n",
      "[Iter 1500] Loss: 0.018010016530752182 Top 1: 0.898866099716525\n",
      "[Iter 1800] Loss: 0.01799485646188259 Top 1: 0.8986383215228567\n",
      "[Iter 2100] Loss: 0.017841912806034088 Top 1: 0.8994879123496486\n",
      "[Iter 2400] Loss: 0.0178788211196661 Top 1: 0.8987183494842138\n",
      "[Iter 2700] Loss: 0.01789626106619835 Top 1: 0.8985366305455219\n",
      "[Iter 3000] Loss: 0.017820507287979126 Top 1: 0.8987455197132617\n",
      "[Iter 3300] Loss: 0.017785878852009773 Top 1: 0.8989353640979011\n",
      "[Iter 3600] Loss: 0.017697002738714218 Top 1: 0.8995797735639369\n",
      "[Iter 3900] Loss: 0.01769324764609337 Top 1: 0.8994678463807142\n",
      "Loss: 0.01772245019674301 Top 1: 0.8992541783981309\n",
      "Total time: 21.349007006486257 in minutes\n",
      "[Iter 300] Loss: 0.018299326300621033 Top 1: 0.8991228070175439\n",
      "[Iter 600] Loss: 0.01784195750951767 Top 1: 0.8999791405924071\n",
      "[Iter 900] Loss: 0.01797749660909176 Top 1: 0.8994995829858216\n",
      "[Iter 1200] Loss: 0.017706124112010002 Top 1: 0.9010840108401084\n",
      "[Iter 1500] Loss: 0.017644701525568962 Top 1: 0.9009504752376188\n",
      "[Iter 1800] Loss: 0.01756495051085949 Top 1: 0.9015214672780325\n",
      "[Iter 2100] Loss: 0.017576368525624275 Top 1: 0.9012742646183161\n",
      "[Iter 2400] Loss: 0.017583675682544708 Top 1: 0.9012451807856622\n",
      "[Iter 2700] Loss: 0.017655368894338608 Top 1: 0.9007363156432342\n",
      "[Iter 3000] Loss: 0.01761588454246521 Top 1: 0.9008085354672002\n",
      "[Iter 3300] Loss: 0.01763554848730564 Top 1: 0.9011517769189967\n",
      "[Iter 3600] Loss: 0.017649054527282715 Top 1: 0.9009863165937348\n",
      "[Iter 3900] Loss: 0.017675673589110374 Top 1: 0.9009264602167083\n",
      "Loss: 0.017766855657100677 Top 1: 0.9004972144012461\n",
      "Total time: 21.362049758434296 in minutes\n",
      "[Iter 300] Loss: 0.018255453556776047 Top 1: 0.899749373433584\n",
      "[Iter 600] Loss: 0.017929866909980774 Top 1: 0.9012307050479766\n",
      "[Iter 900] Loss: 0.017599573358893394 Top 1: 0.9031137058659995\n",
      "[Iter 1200] Loss: 0.01762884296476841 Top 1: 0.9026996039191161\n",
      "[Iter 1500] Loss: 0.01742507703602314 Top 1: 0.9048274137068534\n",
      "[Iter 1800] Loss: 0.01749488338828087 Top 1: 0.9042309295539809\n",
      "[Iter 2100] Loss: 0.0173915084451437 Top 1: 0.904638561390973\n",
      "[Iter 2400] Loss: 0.01758882775902748 Top 1: 0.9036678128581849\n",
      "[Iter 2700] Loss: 0.017565129324793816 Top 1: 0.9037927202000555\n",
      "[Iter 3000] Loss: 0.01749499887228012 Top 1: 0.9041635408852213\n",
      "[Iter 3300] Loss: 0.01742996834218502 Top 1: 0.9038417822232325\n",
      "[Iter 3600] Loss: 0.017455844208598137 Top 1: 0.90338264916302\n",
      "[Iter 3900] Loss: 0.01751658506691456 Top 1: 0.9031384240559082\n",
      "Loss: 0.01755208894610405 Top 1: 0.9031779787935063\n",
      "Total time: 21.359281798203785 in minutes\n",
      "[Iter 300] Loss: 0.01688380166888237 Top 1: 0.9012113617376776\n",
      "[Iter 600] Loss: 0.017153704538941383 Top 1: 0.9032123487692949\n",
      "[Iter 900] Loss: 0.01763896271586418 Top 1: 0.9012371420628301\n",
      "[Iter 1200] Loss: 0.017318543046712875 Top 1: 0.9017094017094017\n",
      "[Iter 1500] Loss: 0.01751747541129589 Top 1: 0.9017842254460564\n",
      "[Iter 1800] Loss: 0.01765996403992176 Top 1: 0.9018340975406419\n",
      "[Iter 2100] Loss: 0.0175155159085989 Top 1: 0.9024949386685721\n",
      "[Iter 2400] Loss: 0.017556605860590935 Top 1: 0.9020006251953736\n",
      "[Iter 2700] Loss: 0.0174576323479414 Top 1: 0.9021487450217653\n",
      "[Iter 3000] Loss: 0.017405888065695763 Top 1: 0.9025589730766025\n",
      "[Iter 3300] Loss: 0.01730802096426487 Top 1: 0.90281882245965\n",
      "[Iter 3600] Loss: 0.017328757792711258 Top 1: 0.9031916371466278\n",
      "[Iter 3900] Loss: 0.017321977764368057 Top 1: 0.9029941655446561\n",
      "Loss: 0.017474940046668053 Top 1: 0.9025339962858683\n",
      "Total time: 21.324419446786244 in minutes\n",
      "[Iter 300] Loss: 0.017658181488513947 Top 1: 0.9001670843776107\n",
      "[Iter 600] Loss: 0.017190206795930862 Top 1: 0.9037338339591156\n",
      "[Iter 900] Loss: 0.016995593905448914 Top 1: 0.9043647484014457\n",
      "[Iter 1200] Loss: 0.017476048320531845 Top 1: 0.9025953721075672\n",
      "[Iter 1500] Loss: 0.01743961311876774 Top 1: 0.9035351008837752\n",
      "[Iter 1800] Loss: 0.017445845529437065 Top 1: 0.903675142420453\n",
      "[Iter 2100] Loss: 0.017490064725279808 Top 1: 0.9033881148029058\n",
      "[Iter 2400] Loss: 0.017376985400915146 Top 1: 0.903798061894342\n",
      "[Iter 2700] Loss: 0.017329026013612747 Top 1: 0.9038390293600074\n",
      "[Iter 3000] Loss: 0.01730337180197239 Top 1: 0.9035175460531799\n",
      "[Iter 3300] Loss: 0.017350398004055023 Top 1: 0.9033681897400925\n",
      "[Iter 3600] Loss: 0.017384467646479607 Top 1: 0.9028096131138431\n",
      "[Iter 3900] Loss: 0.017429169267416 Top 1: 0.9026896197986792\n",
      "Loss: 0.017372488975524902 Top 1: 0.9030431917570239\n",
      "Total time: 21.097473386923472 in minutes\n",
      "[Iter 300] Loss: 0.018085066229104996 Top 1: 0.9030910609857978\n",
      "[Iter 600] Loss: 0.018284978345036507 Top 1: 0.9012307050479766\n",
      "[Iter 900] Loss: 0.017683913931250572 Top 1: 0.9038782318598833\n",
      "[Iter 1200] Loss: 0.017287539318203926 Top 1: 0.9053575151136126\n",
      "[Iter 1500] Loss: 0.01734500378370285 Top 1: 0.9044939136234784\n",
      "[Iter 1800] Loss: 0.017399391159415245 Top 1: 0.9042656662498263\n",
      "[Iter 2100] Loss: 0.017326047644019127 Top 1: 0.9045790163153508\n",
      "[Iter 2400] Loss: 0.017348671332001686 Top 1: 0.9048140043763676\n",
      "[Iter 2700] Loss: 0.017231807112693787 Top 1: 0.9047883671390201\n",
      "[Iter 3000] Loss: 0.01723730005323887 Top 1: 0.9041218637992832\n",
      "[Iter 3300] Loss: 0.017202110961079597 Top 1: 0.9043722058043495\n",
      "[Iter 3600] Loss: 0.017216961830854416 Top 1: 0.9042335208724039\n",
      "[Iter 3900] Loss: 0.017207276076078415 Top 1: 0.9041802910816182\n",
      "Loss: 0.01725475862622261 Top 1: 0.9037770322889834\n",
      "Total time: 21.170339409510294 in minutes\n",
      "[Iter 300] Loss: 0.0172232985496521 Top 1: 0.9026733500417711\n",
      "[Iter 600] Loss: 0.017384108155965805 Top 1: 0.9005006257822278\n",
      "[Iter 900] Loss: 0.0175615381449461 Top 1: 0.9004726160689464\n",
      "[Iter 1200] Loss: 0.017620258033275604 Top 1: 0.8994163018553263\n",
      "[Iter 1500] Loss: 0.017684156075119972 Top 1: 0.9001584125396032\n",
      "[Iter 1800] Loss: 0.01750475913286209 Top 1: 0.9020772544115604\n",
      "[Iter 2100] Loss: 0.017453201115131378 Top 1: 0.9021376682148387\n",
      "[Iter 2400] Loss: 0.017408711835741997 Top 1: 0.9023653224966135\n",
      "[Iter 2700] Loss: 0.017433438450098038 Top 1: 0.9027507641011392\n",
      "[Iter 3000] Loss: 0.01739797741174698 Top 1: 0.9026423272484788\n",
      "[Iter 3300] Loss: 0.01737559214234352 Top 1: 0.9027619913616731\n",
      "[Iter 3600] Loss: 0.01738177053630352 Top 1: 0.9026706952837397\n",
      "[Iter 3900] Loss: 0.01739032380282879 Top 1: 0.9027377059690966\n",
      "Loss: 0.017397476360201836 Top 1: 0.9030881207691847\n",
      "Total time: 21.137868984540304 in minutes\n",
      "[Iter 300] Loss: 0.018020080402493477 Top 1: 0.9041353383458647\n",
      "[Iter 600] Loss: 0.017614582553505898 Top 1: 0.9036295369211514\n",
      "[Iter 900] Loss: 0.01745174452662468 Top 1: 0.9033222129552405\n",
      "[Iter 1200] Loss: 0.01768084056675434 Top 1: 0.9038461538461539\n",
      "[Iter 1500] Loss: 0.017492301762104034 Top 1: 0.9041187260296815\n",
      "[Iter 1800] Loss: 0.017459845170378685 Top 1: 0.9041614561622898\n",
      "[Iter 2100] Loss: 0.017522167414426804 Top 1: 0.9034178873407169\n",
      "[Iter 2400] Loss: 0.017617017030715942 Top 1: 0.9031989163280192\n",
      "[Iter 2700] Loss: 0.017632098868489265 Top 1: 0.9030054644808743\n",
      "[Iter 3000] Loss: 0.017520392313599586 Top 1: 0.9033091606234892\n",
      "[Iter 3300] Loss: 0.017509281635284424 Top 1: 0.9028945972569523\n",
      "[Iter 3600] Loss: 0.01751873455941677 Top 1: 0.9026012363686879\n",
      "[Iter 3900] Loss: 0.017490996047854424 Top 1: 0.9026094761813169\n",
      "Loss: 0.017496099695563316 Top 1: 0.9025339962858683\n",
      "Total time: 21.131151151657104 in minutes\n",
      "[Iter 300] Loss: 0.016752775758504868 Top 1: 0.9045530492898914\n",
      "[Iter 600] Loss: 0.017118405550718307 Top 1: 0.9037338339591156\n",
      "[Iter 900] Loss: 0.01727297343313694 Top 1: 0.9041562413122046\n",
      "[Iter 1200] Loss: 0.017294399440288544 Top 1: 0.9045757765269961\n",
      "[Iter 1500] Loss: 0.017223896458745003 Top 1: 0.9052859763214941\n",
      "[Iter 1800] Loss: 0.017283160239458084 Top 1: 0.9046477699041268\n",
      "[Iter 2100] Loss: 0.01729731447994709 Top 1: 0.9040133380969394\n",
      "[Iter 2400] Loss: 0.01728152483701706 Top 1: 0.9041888090028134\n",
      "[Iter 2700] Loss: 0.017225058749318123 Top 1: 0.9044410484393813\n",
      "[Iter 3000] Loss: 0.017209665849804878 Top 1: 0.9041218637992832\n",
      "[Iter 3300] Loss: 0.017179589718580246 Top 1: 0.904826854588164\n",
      "[Iter 3600] Loss: 0.01723562553524971 Top 1: 0.9043550739737445\n",
      "[Iter 3900] Loss: 0.017303459346294403 Top 1: 0.9041963198050907\n",
      "Loss: 0.017289113253355026 Top 1: 0.904286227760139\n",
      "Total time: 21.242238910992942 in minutes\n",
      "[Iter 300] Loss: 0.015988720580935478 Top 1: 0.9124895572263994\n",
      "[Iter 600] Loss: 0.017130373045802116 Top 1: 0.907488527325824\n",
      "[Iter 900] Loss: 0.017134230583906174 Top 1: 0.9063108145676954\n",
      "[Iter 1200] Loss: 0.016999561339616776 Top 1: 0.9060350218886805\n",
      "[Iter 1500] Loss: 0.017032355070114136 Top 1: 0.9055777888944472\n",
      "[Iter 1800] Loss: 0.017026212066411972 Top 1: 0.9055509239961095\n",
      "[Iter 2100] Loss: 0.017056653276085854 Top 1: 0.9051149219959509\n",
      "[Iter 2400] Loss: 0.017068123444914818 Top 1: 0.9045274564968219\n",
      "[Iter 2700] Loss: 0.017098069190979004 Top 1: 0.904232657219598\n",
      "[Iter 3000] Loss: 0.017164506018161774 Top 1: 0.9039551554555305\n",
      "[Iter 3300] Loss: 0.017149578779935837 Top 1: 0.9044100932030007\n",
      "[Iter 3600] Loss: 0.01720535196363926 Top 1: 0.9043724387025075\n",
      "[Iter 3900] Loss: 0.01716914027929306 Top 1: 0.9043726357632879\n",
      "Loss: 0.017156368121504784 Top 1: 0.9043461331096867\n",
      "Total time: 21.33055241902669 in minutes\n",
      "[Iter 300] Loss: 0.017387913540005684 Top 1: 0.9108187134502924\n",
      "[Iter 600] Loss: 0.017479047179222107 Top 1: 0.9055068836045056\n",
      "[Iter 900] Loss: 0.01732487417757511 Top 1: 0.9060328051153739\n",
      "[Iter 1200] Loss: 0.017088020220398903 Top 1: 0.9057744423598082\n",
      "[Iter 1500] Loss: 0.0171507578343153 Top 1: 0.9044522261130565\n",
      "[Iter 1800] Loss: 0.01740066520869732 Top 1: 0.903397248853689\n",
      "[Iter 2100] Loss: 0.017527010291814804 Top 1: 0.9028224365844945\n",
      "[Iter 2400] Loss: 0.01758144423365593 Top 1: 0.9024955715327707\n",
      "[Iter 2700] Loss: 0.017346510663628578 Top 1: 0.9032138557006576\n",
      "[Iter 3000] Loss: 0.017353665083646774 Top 1: 0.9028923897641077\n",
      "[Iter 3300] Loss: 0.01734137535095215 Top 1: 0.9029324846556035\n",
      "[Iter 3600] Loss: 0.017360735684633255 Top 1: 0.9029658956727096\n",
      "[Iter 3900] Loss: 0.017365023493766785 Top 1: 0.9028178495864589\n",
      "Loss: 0.017272677272558212 Top 1: 0.9035374108907925\n",
      "Total time: 21.312605500221252 in minutes\n",
      "[Iter 300] Loss: 0.016910403966903687 Top 1: 0.9028822055137845\n",
      "[Iter 600] Loss: 0.016989009454846382 Top 1: 0.9049853984146851\n",
      "[Iter 900] Loss: 0.01705721952021122 Top 1: 0.9050597720322491\n",
      "[Iter 1200] Loss: 0.016989443451166153 Top 1: 0.9057744423598082\n",
      "[Iter 1500] Loss: 0.0171909611672163 Top 1: 0.9059112889778222\n",
      "[Iter 1800] Loss: 0.017154186964035034 Top 1: 0.9056898707794915\n",
      "[Iter 2100] Loss: 0.01702381856739521 Top 1: 0.9058890079790402\n",
      "[Iter 2400] Loss: 0.017057375982403755 Top 1: 0.905569448786079\n",
      "[Iter 2700] Loss: 0.017159394919872284 Top 1: 0.9049041400388997\n",
      "[Iter 3000] Loss: 0.01716863550245762 Top 1: 0.9044761190297574\n",
      "[Iter 3300] Loss: 0.01717381551861763 Top 1: 0.9043722058043495\n",
      "[Iter 3600] Loss: 0.017232520505785942 Top 1: 0.9038688615683823\n",
      "[Iter 3900] Loss: 0.01730472221970558 Top 1: 0.9031704815028531\n",
      "Loss: 0.017207155004143715 Top 1: 0.9039118193254657\n",
      "Total time: 21.302339331309 in minutes\n",
      "[Iter 300] Loss: 0.01726767048239708 Top 1: 0.9074770258980785\n",
      "[Iter 600] Loss: 0.01674690842628479 Top 1: 0.908427200667501\n",
      "[Iter 900] Loss: 0.01689244620501995 Top 1: 0.9071448429246595\n",
      "[Iter 1200] Loss: 0.017009908333420753 Top 1: 0.9061392537002293\n",
      "[Iter 1500] Loss: 0.016967082396149635 Top 1: 0.9060363515090879\n",
      "[Iter 1800] Loss: 0.017211662605404854 Top 1: 0.9052382937335001\n",
      "[Iter 2100] Loss: 0.017293399199843407 Top 1: 0.9055615100631178\n",
      "[Iter 2400] Loss: 0.017312802374362946 Top 1: 0.9045274564968219\n",
      "[Iter 2700] Loss: 0.01731656864285469 Top 1: 0.9044642030193573\n",
      "[Iter 3000] Loss: 0.01736689731478691 Top 1: 0.9045386346586647\n",
      "[Iter 3300] Loss: 0.017324909567832947 Top 1: 0.9043153747063727\n",
      "[Iter 3600] Loss: 0.017257925122976303 Top 1: 0.9045287212613738\n",
      "[Iter 3900] Loss: 0.017295168712735176 Top 1: 0.9041322049112008\n",
      "Loss: 0.01729828305542469 Top 1: 0.9038818666506919\n",
      "Total time: 21.305761698881785 in minutes\n",
      "[Iter 300] Loss: 0.016524413600564003 Top 1: 0.908312447786132\n",
      "[Iter 600] Loss: 0.016512423753738403 Top 1: 0.908427200667501\n",
      "[Iter 900] Loss: 0.01671467535197735 Top 1: 0.9070058381984988\n",
      "[Iter 1200] Loss: 0.016850363463163376 Top 1: 0.9058265582655827\n",
      "[Iter 1500] Loss: 0.016956394538283348 Top 1: 0.9053693513423379\n",
      "[Iter 1800] Loss: 0.016996093094348907 Top 1: 0.9057593441711824\n",
      "[Iter 2100] Loss: 0.01692235842347145 Top 1: 0.9063058235083958\n",
      "[Iter 2400] Loss: 0.016923250630497932 Top 1: 0.9054652495571532\n",
      "[Iter 2700] Loss: 0.017002282664179802 Top 1: 0.9061313327776234\n",
      "[Iter 3000] Loss: 0.017187638208270073 Top 1: 0.9047678586313245\n",
      "[Iter 3300] Loss: 0.017158420756459236 Top 1: 0.9052057285746761\n",
      "[Iter 3600] Loss: 0.01717969961464405 Top 1: 0.9048586511078697\n",
      "[Iter 3900] Loss: 0.01721508987247944 Top 1: 0.9051099570430211\n",
      "Loss: 0.017265234142541885 Top 1: 0.9052147606781286\n",
      "Total time: 21.248382421334586 in minutes\n",
      "[Iter 300] Loss: 0.018432138487696648 Top 1: 0.9001670843776107\n",
      "[Iter 600] Loss: 0.017664961516857147 Top 1: 0.9061326658322904\n",
      "[Iter 900] Loss: 0.017676016315817833 Top 1: 0.9043647484014457\n",
      "[Iter 1200] Loss: 0.017589271068572998 Top 1: 0.9052011673962893\n",
      "[Iter 1500] Loss: 0.01737845130264759 Top 1: 0.9062447890611973\n",
      "[Iter 1800] Loss: 0.017241178080439568 Top 1: 0.9064888147839377\n",
      "[Iter 2100] Loss: 0.017282210290431976 Top 1: 0.9062462784327736\n",
      "[Iter 2400] Loss: 0.017334328964352608 Top 1: 0.9064030426174846\n",
      "[Iter 2700] Loss: 0.01735016144812107 Top 1: 0.9060618690376957\n",
      "[Iter 3000] Loss: 0.01729239523410797 Top 1: 0.9060806868383763\n",
      "[Iter 3300] Loss: 0.017249323427677155 Top 1: 0.9063423505342123\n",
      "[Iter 3600] Loss: 0.017379799857735634 Top 1: 0.9057095228172536\n",
      "[Iter 3900] Loss: 0.017428183928132057 Top 1: 0.9055427325767775\n",
      "Loss: 0.017284218221902847 Top 1: 0.9062481279578266\n",
      "Total time: 21.129004923502603 in minutes\n",
      "[Iter 300] Loss: 0.017688674852252007 Top 1: 0.902046783625731\n",
      "[Iter 600] Loss: 0.017513668164610863 Top 1: 0.9017521902377973\n",
      "[Iter 900] Loss: 0.01754653826355934 Top 1: 0.9015151515151515\n",
      "[Iter 1200] Loss: 0.0175775159150362 Top 1: 0.9009797790285595\n",
      "[Iter 1500] Loss: 0.01744985394179821 Top 1: 0.902909788227447\n",
      "[Iter 1800] Loss: 0.017517441883683205 Top 1: 0.9029109351118522\n",
      "[Iter 2100] Loss: 0.01749958097934723 Top 1: 0.9039537930213172\n",
      "[Iter 2400] Loss: 0.017510345205664635 Top 1: 0.903615713243722\n",
      "[Iter 2700] Loss: 0.017532549798488617 Top 1: 0.9034917106603686\n",
      "[Iter 3000] Loss: 0.01747582107782364 Top 1: 0.9045177961156956\n",
      "[Iter 3300] Loss: 0.017448190599679947 Top 1: 0.9045805864969311\n",
      "[Iter 3600] Loss: 0.01747467741370201 Top 1: 0.9045287212613738\n",
      "[Iter 3900] Loss: 0.017480118200182915 Top 1: 0.9042444059755081\n",
      "Loss: 0.017433810979127884 Top 1: 0.9040166536871742\n",
      "Total time: 21.14841939608256 in minutes\n",
      "[Iter 300] Loss: 0.017431996762752533 Top 1: 0.9045530492898914\n",
      "[Iter 600] Loss: 0.01747054047882557 Top 1: 0.9061326658322904\n",
      "[Iter 900] Loss: 0.017755016684532166 Top 1: 0.904851264943008\n",
      "[Iter 1200] Loss: 0.01769986003637314 Top 1: 0.9053575151136126\n",
      "[Iter 1500] Loss: 0.01770756207406521 Top 1: 0.9051609137902284\n",
      "[Iter 1800] Loss: 0.017711346969008446 Top 1: 0.905516187300264\n",
      "[Iter 2100] Loss: 0.017679940909147263 Top 1: 0.90562105513874\n",
      "[Iter 2400] Loss: 0.01773214153945446 Top 1: 0.9048140043763676\n",
      "[Iter 2700] Loss: 0.01758616417646408 Top 1: 0.9051588404186348\n",
      "[Iter 3000] Loss: 0.017578601837158203 Top 1: 0.9056639159789948\n",
      "[Iter 3300] Loss: 0.017651688307523727 Top 1: 0.9055277714632113\n",
      "[Iter 3600] Loss: 0.017732826992869377 Top 1: 0.9049975689379732\n",
      "[Iter 3900] Loss: 0.017759673297405243 Top 1: 0.9051740719369109\n",
      "Loss: 0.017815465107560158 Top 1: 0.9050200682920985\n",
      "Total time: 21.07706613143285 in minutes\n",
      "[Iter 300] Loss: 0.017750026658177376 Top 1: 0.9041353383458647\n",
      "[Iter 600] Loss: 0.018037034198641777 Top 1: 0.905819774718398\n",
      "[Iter 900] Loss: 0.017983317375183105 Top 1: 0.9049902696691687\n",
      "[Iter 1200] Loss: 0.017839018255472183 Top 1: 0.9054096310193871\n",
      "[Iter 1500] Loss: 0.0179250817745924 Top 1: 0.9044939136234784\n",
      "[Iter 1800] Loss: 0.017838647589087486 Top 1: 0.9047519799916632\n",
      "[Iter 2100] Loss: 0.01787831075489521 Top 1: 0.904162200785995\n",
      "[Iter 2400] Loss: 0.0178943183273077 Top 1: 0.9040585599666563\n",
      "[Iter 2700] Loss: 0.017824560403823853 Top 1: 0.9042558117995739\n",
      "[Iter 3000] Loss: 0.01771344058215618 Top 1: 0.9045177961156956\n",
      "[Iter 3300] Loss: 0.01773936301469803 Top 1: 0.9047510797908616\n",
      "[Iter 3600] Loss: 0.017716487869620323 Top 1: 0.9045113565326109\n",
      "[Iter 3900] Loss: 0.017589226365089417 Top 1: 0.9050137847021863\n",
      "Loss: 0.01760859787464142 Top 1: 0.9046157071826514\n",
      "Total time: 21.110591308275858 in minutes\n",
      "[Iter 300] Loss: 0.01812002807855606 Top 1: 0.8995405179615706\n",
      "[Iter 600] Loss: 0.018002450466156006 Top 1: 0.9009178139340843\n",
      "[Iter 900] Loss: 0.017772162333130836 Top 1: 0.9036002224075619\n",
      "[Iter 1200] Loss: 0.017749575898051262 Top 1: 0.9033771106941839\n",
      "[Iter 1500] Loss: 0.017721673473715782 Top 1: 0.9040770385192596\n",
      "[Iter 1800] Loss: 0.017683643847703934 Top 1: 0.9041614561622898\n",
      "[Iter 2100] Loss: 0.017554733902215958 Top 1: 0.9046683339287841\n",
      "[Iter 2400] Loss: 0.01748484931886196 Top 1: 0.9049703032197561\n",
      "[Iter 2700] Loss: 0.01751563511788845 Top 1: 0.9046725942391405\n",
      "[Iter 3000] Loss: 0.017518863081932068 Top 1: 0.9047470200883554\n",
      "[Iter 3300] Loss: 0.017602641135454178 Top 1: 0.9041259377131167\n",
      "[Iter 3600] Loss: 0.017502617090940475 Top 1: 0.9044766270750851\n",
      "[Iter 3900] Loss: 0.017486000433564186 Top 1: 0.9045810091684299\n",
      "Loss: 0.01756943203508854 Top 1: 0.9046306835200384\n",
      "Total time: 21.132576207319897 in minutes\n",
      "[Iter 300] Loss: 0.017883433029055595 Top 1: 0.9016290726817042\n",
      "[Iter 600] Loss: 0.017727447673678398 Top 1: 0.9001877346683355\n",
      "[Iter 900] Loss: 0.01739635318517685 Top 1: 0.9026271893244371\n",
      "[Iter 1200] Loss: 0.016770895570516586 Top 1: 0.9065040650406504\n",
      "[Iter 1500] Loss: 0.01693069562315941 Top 1: 0.9064949141237285\n",
      "[Iter 1800] Loss: 0.017107300460338593 Top 1: 0.9061067111296374\n",
      "[Iter 2100] Loss: 0.017115196213126183 Top 1: 0.9060080981302846\n",
      "[Iter 2400] Loss: 0.017060715705156326 Top 1: 0.9063248931957903\n",
      "[Iter 2700] Loss: 0.01707439124584198 Top 1: 0.9065018060572381\n",
      "[Iter 3000] Loss: 0.01721845380961895 Top 1: 0.9055180461782112\n",
      "[Iter 3300] Loss: 0.01734975539147854 Top 1: 0.9048079108888384\n",
      "[Iter 3600] Loss: 0.017432909458875656 Top 1: 0.9045808154476627\n",
      "[Iter 3900] Loss: 0.017460376024246216 Top 1: 0.9046932102327371\n",
      "Loss: 0.017452837899327278 Top 1: 0.9049751392799377\n",
      "Total time: 21.15834447542826 in minutes\n",
      "[Iter 300] Loss: 0.017410745844244957 Top 1: 0.9068504594820385\n",
      "[Iter 600] Loss: 0.017286306247115135 Top 1: 0.9067584480600751\n",
      "[Iter 900] Loss: 0.017646756023168564 Top 1: 0.9040867389491243\n",
      "[Iter 1200] Loss: 0.017403896898031235 Top 1: 0.9052532833020638\n",
      "[Iter 1500] Loss: 0.017465678974986076 Top 1: 0.9043688510922128\n",
      "[Iter 1800] Loss: 0.017489206045866013 Top 1: 0.9050298735584271\n",
      "[Iter 2100] Loss: 0.01762310415506363 Top 1: 0.9045194712397284\n",
      "[Iter 2400] Loss: 0.017619671300053596 Top 1: 0.9051526518703762\n",
      "[Iter 2700] Loss: 0.017763815820217133 Top 1: 0.9045568213392609\n",
      "[Iter 3000] Loss: 0.017694955691695213 Top 1: 0.9044136034008502\n",
      "[Iter 3300] Loss: 0.01759808510541916 Top 1: 0.9047700234901872\n",
      "[Iter 3600] Loss: 0.01752910017967224 Top 1: 0.904771827464055\n",
      "[Iter 3900] Loss: 0.01759849488735199 Top 1: 0.9049817272552414\n",
      "Loss: 0.01768350601196289 Top 1: 0.9047504942191338\n",
      "Total time: 21.47631344397863 in minutes\n",
      "[Iter 300] Loss: 0.01781926304101944 Top 1: 0.9030910609857978\n",
      "[Iter 600] Loss: 0.017975689843297005 Top 1: 0.9026908635794744\n",
      "[Iter 900] Loss: 0.017436617985367775 Top 1: 0.9062413122046149\n",
      "[Iter 1200] Loss: 0.017421217635273933 Top 1: 0.9067646445695227\n",
      "[Iter 1500] Loss: 0.01752609759569168 Top 1: 0.9063698515924629\n",
      "[Iter 1800] Loss: 0.017472246661782265 Top 1: 0.9065235514797833\n",
      "[Iter 2100] Loss: 0.017487283796072006 Top 1: 0.9060080981302846\n",
      "[Iter 2400] Loss: 0.0176787618547678 Top 1: 0.9047879545691362\n",
      "[Iter 2700] Loss: 0.017717281356453896 Top 1: 0.9040937297397426\n",
      "[Iter 3000] Loss: 0.017618101090192795 Top 1: 0.9046011502875719\n",
      "[Iter 3300] Loss: 0.017521480098366737 Top 1: 0.9049784041827688\n",
      "[Iter 3600] Loss: 0.01765298657119274 Top 1: 0.9046676390914774\n",
      "[Iter 3900] Loss: 0.017591718584299088 Top 1: 0.9049015836378791\n",
      "Loss: 0.01764959655702114 Top 1: 0.9046306835200384\n",
      "Total time: 21.306241687138876 in minutes\n",
      "[Iter 300] Loss: 0.016496870666742325 Top 1: 0.9118629908103593\n",
      "[Iter 600] Loss: 0.01726917363703251 Top 1: 0.9054025865665415\n",
      "[Iter 900] Loss: 0.017304712906479836 Top 1: 0.9055462885738115\n",
      "[Iter 1200] Loss: 0.017533039674162865 Top 1: 0.9048363560558682\n",
      "[Iter 1500] Loss: 0.017465876415371895 Top 1: 0.9047023511755878\n",
      "[Iter 1800] Loss: 0.01739455573260784 Top 1: 0.9046130332082812\n",
      "[Iter 2100] Loss: 0.017530927434563637 Top 1: 0.9041324282481839\n",
      "[Iter 2400] Loss: 0.0173343475908041 Top 1: 0.9048661039908305\n",
      "[Iter 2700] Loss: 0.017318585887551308 Top 1: 0.905112531258683\n",
      "[Iter 3000] Loss: 0.01739613711833954 Top 1: 0.905184629490706\n",
      "[Iter 3300] Loss: 0.017612399533391 Top 1: 0.9047510797908616\n",
      "[Iter 3600] Loss: 0.017737705260515213 Top 1: 0.9042682503299299\n",
      "[Iter 3900] Loss: 0.017705243080854416 Top 1: 0.90451689427454\n",
      "Loss: 0.017761826515197754 Top 1: 0.9046606361948122\n",
      "Total time: 21.0911793589592 in minutes\n",
      "[Iter 300] Loss: 0.017256159335374832 Top 1: 0.9062238930659984\n",
      "[Iter 600] Loss: 0.017919152975082397 Top 1: 0.9027951606174385\n",
      "[Iter 900] Loss: 0.017383845522999763 Top 1: 0.9041562413122046\n",
      "[Iter 1200] Loss: 0.017080970108509064 Top 1: 0.9062956014175526\n",
      "[Iter 1500] Loss: 0.017221560701727867 Top 1: 0.9062447890611973\n",
      "[Iter 1800] Loss: 0.017017316073179245 Top 1: 0.9069403918299291\n",
      "[Iter 2100] Loss: 0.017222484573721886 Top 1: 0.9058294629034179\n",
      "[Iter 2400] Loss: 0.01740645058453083 Top 1: 0.9052308012920705\n",
      "[Iter 2700] Loss: 0.017547694966197014 Top 1: 0.9040705751597666\n",
      "[Iter 3000] Loss: 0.017581848427653313 Top 1: 0.9037676085688089\n",
      "[Iter 3300] Loss: 0.01759936474263668 Top 1: 0.9040691066151398\n",
      "[Iter 3600] Loss: 0.017543310299515724 Top 1: 0.9046155449051886\n",
      "[Iter 3900] Loss: 0.017613671720027924 Top 1: 0.9047412964031545\n",
      "Loss: 0.017613139003515244 Top 1: 0.9047954232312946\n",
      "Total time: 21.29582322835922 in minutes\n",
      "[Iter 300] Loss: 0.017866602167487144 Top 1: 0.9005847953216374\n",
      "[Iter 600] Loss: 0.01747199520468712 Top 1: 0.9056111806424697\n",
      "[Iter 900] Loss: 0.01710488647222519 Top 1: 0.9070753405615791\n",
      "[Iter 1200] Loss: 0.017636679112911224 Top 1: 0.9062434855117782\n",
      "[Iter 1500] Loss: 0.017667412757873535 Top 1: 0.9052442888110722\n",
      "[Iter 1800] Loss: 0.01751447096467018 Top 1: 0.9061067111296374\n",
      "[Iter 2100] Loss: 0.017459139227867126 Top 1: 0.9069608193402405\n",
      "[Iter 2400] Loss: 0.01757127232849598 Top 1: 0.9068198395331875\n",
      "[Iter 2700] Loss: 0.01740439608693123 Top 1: 0.9074742984162267\n",
      "[Iter 3000] Loss: 0.017595389857888222 Top 1: 0.9064141035258815\n",
      "[Iter 3300] Loss: 0.01756412349641323 Top 1: 0.9061908009396075\n",
      "[Iter 3600] Loss: 0.017629556357860565 Top 1: 0.906404111967771\n",
      "[Iter 3900] Loss: 0.01777750439941883 Top 1: 0.9052542155542732\n",
      "Loss: 0.01785941980779171 Top 1: 0.9050649973042593\n",
      "Total time: 21.18874929745992 in minutes\n"
     ]
    }
   ],
   "source": [
    "run_model(model_r50, train_generator, epochs=30, training=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6646e851-0137-43fb-8557-a1d343388788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'block_group3': <tf.Tensor 'model/StatefulPartitionedCall:2' shape=(16, 16, 16, 2048) dtype=float32>, 'block_group1': <tf.Tensor 'model/StatefulPartitionedCall:0' shape=(16, 64, 64, 512) dtype=float32>, 'logits_sup': <tf.Tensor 'model/StatefulPartitionedCall:7' shape=(16, 1000) dtype=float32>, 'block_group4': <tf.Tensor 'model/StatefulPartitionedCall:3' shape=(16, 8, 8, 4096) dtype=float32>, 'final_avg_pool': <tf.Tensor 'model/StatefulPartitionedCall:4' shape=(16, 4096) dtype=float32>, 'initial_max_pool': <tf.Tensor 'model/StatefulPartitionedCall:6' shape=(16, 64, 64, 128) dtype=float32>, 'block_group2': <tf.Tensor 'model/StatefulPartitionedCall:1' shape=(16, 32, 32, 1024) dtype=float32>, 'initial_conv': <tf.Tensor 'model/StatefulPartitionedCall:5' shape=(16, 128, 128, 128) dtype=float32>}\n",
      "[Iter 300] Loss: 0.0059816581197083 Top 1: 0.96625\n",
      "[Iter 600] Loss: 0.014207178726792336 Top 1: 0.9291666666666667\n",
      "[Iter 900] Loss: 0.020354006439447403 Top 1: 0.8919444444444444\n",
      "Loss: 0.018975526094436646 Top 1: 0.8992689357622243\n",
      "Total time: 5.933304131031036 in minutes\n"
     ]
    }
   ],
   "source": [
    "run_model(model_r50, valid_generator, epochs=1, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a46409e-4d9a-4a68-a084-314d63a32d21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
