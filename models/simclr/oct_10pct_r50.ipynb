{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a261348-3abc-49da-aa9e-773b5b78c024",
   "metadata": {},
   "source": [
    "### General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2130f56d-dd15-4519-ae64-7905fa9ddf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import abs1\n",
    "import itertools\n",
    "import os\n",
    "import more_itertools\n",
    "import sys\n",
    "import time\n",
    "from absl import flags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7057deeb-ce61-4126-b15a-21a10684817b",
   "metadata": {},
   "source": [
    "## Tensorflow and keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de4a64c1-14b1-40d1-a40c-cd23c1ecde6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import keras\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "# import tensorflow.compat.v2 as tf\n",
    "# tf.compat.v1.enable_v2_behavior()\n",
    "# import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub \n",
    "from keras.applications.resnet_v2 import ResNet101V2\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.layers import Input, GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import (Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, UpSampling2D, \n",
    "                          concatenate, GlobalAveragePooling2D, Input)\n",
    "from keras.regularizers import l2, l1_l2\n",
    "from keras.constraints import MaxNorm\n",
    "from keras.models import load_model, Sequential, Model\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet_v2 import ResNet50V2, ResNet152V2\n",
    "from keras.applications.xception import Xception\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.callbacks import Callback, EarlyStopping, LearningRateScheduler, ModelCheckpoint, LearningRateScheduler, TensorBoard\n",
    "import sklearn as sk \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fae050-4b87-41a3-872c-5fa21bda5b21",
   "metadata": {},
   "source": [
    "## SimCLRv2 colab fine tuning code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26fece07-762f-493d-9947-b16426dd8f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS_color_jitter_strength = 0.3\n",
    "CROP_PROPORTION = 0.875  # Standard for ImageNet.\n",
    "\n",
    "\n",
    "def random_apply(func, p, x):\n",
    "  \"\"\"Randomly apply function func to x with probability p.\"\"\"\n",
    "  return tf.cond(\n",
    "      tf.less(tf.random_uniform([], minval=0, maxval=1, dtype=tf.float32),\n",
    "              tf.cast(p, tf.float32)),\n",
    "      lambda: func(x),\n",
    "      lambda: x)\n",
    "\n",
    "\n",
    "def random_brightness(image, max_delta, impl='simclrv2'):\n",
    "  \"\"\"A multiplicative vs additive change of brightness.\"\"\"\n",
    "  if impl == 'simclrv2':\n",
    "    factor = tf.random_uniform(\n",
    "        [], tf.maximum(1.0 - max_delta, 0), 1.0 + max_delta)\n",
    "    image = image * factor\n",
    "  elif impl == 'simclrv1':\n",
    "    image = random_brightness(image, max_delta=max_delta)\n",
    "  else:\n",
    "    raise ValueError('Unknown impl {} for random brightness.'.format(impl))\n",
    "  return image\n",
    "\n",
    "\n",
    "def to_grayscale(image, keep_channels=True):\n",
    "  image = tf.image.rgb_to_grayscale(image)\n",
    "  if keep_channels:\n",
    "    image = tf.tile(image, [1, 1, 3])\n",
    "  return image\n",
    "\n",
    "\n",
    "def color_jitter(image,\n",
    "                 strength,\n",
    "                 random_order=True):\n",
    "  \"\"\"Distorts the color of the image.\n",
    "  Args:\n",
    "    image: The input image tensor.\n",
    "    strength: the floating number for the strength of the color augmentation.\n",
    "    random_order: A bool, specifying whether to randomize the jittering order.\n",
    "  Returns:\n",
    "    The distorted image tensor.\n",
    "  \"\"\"\n",
    "  brightness = 0.8 * strength\n",
    "  contrast = 0.8 * strength\n",
    "  saturation = 0.8 * strength\n",
    "  hue = 0.2 * strength\n",
    "  if random_order:\n",
    "    return color_jitter_rand(image, brightness, contrast, saturation, hue)\n",
    "  else:\n",
    "    return color_jitter_nonrand(image, brightness, contrast, saturation, hue)\n",
    "\n",
    "\n",
    "def color_jitter_nonrand(image, brightness=0, contrast=0, saturation=0, hue=0):\n",
    "  \"\"\"Distorts the color of the image (jittering order is fixed).\n",
    "  Args:\n",
    "    image: The input image tensor.\n",
    "    brightness: A float, specifying the brightness for color jitter.\n",
    "    contrast: A float, specifying the contrast for color jitter.\n",
    "    saturation: A float, specifying the saturation for color jitter.\n",
    "    hue: A float, specifying the hue for color jitter.\n",
    "  Returns:\n",
    "    The distorted image tensor.\n",
    "  \"\"\"\n",
    "  with tf.name_scope('distort_color'):\n",
    "    def apply_transform(i, x, brightness, contrast, saturation, hue):\n",
    "      \"\"\"Apply the i-th transformation.\"\"\"\n",
    "      if brightness != 0 and i == 0:\n",
    "        x = random_brightness(x, max_delta=brightness)\n",
    "      elif contrast != 0 and i == 1:\n",
    "        x = tf.image.random_contrast(\n",
    "            x, lower=1-contrast, upper=1+contrast)\n",
    "      elif saturation != 0 and i == 2:\n",
    "        x = tf.image.random_saturation(\n",
    "            x, lower=1-saturation, upper=1+saturation)\n",
    "      elif hue != 0:\n",
    "        x = tf.image.random_hue(x, max_delta=hue)\n",
    "      return x\n",
    "\n",
    "    for i in range(4):\n",
    "      image = apply_transform(i, image, brightness, contrast, saturation, hue)\n",
    "      image = tf.clip_by_value(image, 0., 1.)\n",
    "    return image\n",
    "\n",
    "\n",
    "def color_jitter_rand(image, brightness=0, contrast=0, saturation=0, hue=0):\n",
    "  \"\"\"Distorts the color of the image (jittering order is random).\n",
    "  Args:\n",
    "    image: The input image tensor.\n",
    "    brightness: A float, specifying the brightness for color jitter.\n",
    "    contrast: A float, specifying the contrast for color jitter.\n",
    "    saturation: A float, specifying the saturation for color jitter.\n",
    "    hue: A float, specifying the hue for color jitter.\n",
    "  Returns:\n",
    "    The distorted image tensor.\n",
    "  \"\"\"\n",
    "  with tf.name_scope('distort_color'):\n",
    "    def apply_transform(i, x):\n",
    "      \"\"\"Apply the i-th transformation.\"\"\"\n",
    "      def brightness_foo():\n",
    "        if brightness == 0:\n",
    "          return x\n",
    "        else:\n",
    "          return random_brightness(x, max_delta=brightness)\n",
    "      def contrast_foo():\n",
    "        if contrast == 0:\n",
    "          return x\n",
    "        else:\n",
    "          return tf.image.random_contrast(x, lower=1-contrast, upper=1+contrast)\n",
    "      def saturation_foo():\n",
    "        if saturation == 0:\n",
    "          return x\n",
    "        else:\n",
    "          return tf.image.random_saturation(\n",
    "              x, lower=1-saturation, upper=1+saturation)\n",
    "      def hue_foo():\n",
    "        if hue == 0:\n",
    "          return x\n",
    "        else:\n",
    "          return tf.image.random_hue(x, max_delta=hue)\n",
    "      x = tf.cond(tf.less(i, 2),\n",
    "                  lambda: tf.cond(tf.less(i, 1), brightness_foo, contrast_foo),\n",
    "                  lambda: tf.cond(tf.less(i, 3), saturation_foo, hue_foo))\n",
    "      return x\n",
    "\n",
    "    perm = tf.random_shuffle(tf.range(4))\n",
    "    for i in range(4):\n",
    "      image = apply_transform(perm[i], image)\n",
    "      image = tf.clip_by_value(image, 0., 1.)\n",
    "    return image\n",
    "\n",
    "\n",
    "def _compute_crop_shape(\n",
    "    image_height, image_width, aspect_ratio, crop_proportion):\n",
    "  \"\"\"Compute aspect ratio-preserving shape for central crop.\n",
    "  The resulting shape retains `crop_proportion` along one side and a proportion\n",
    "  less than or equal to `crop_proportion` along the other side.\n",
    "  Args:\n",
    "    image_height: Height of image to be cropped.\n",
    "    image_width: Width of image to be cropped.\n",
    "    aspect_ratio: Desired aspect ratio (width / height) of output.\n",
    "    crop_proportion: Proportion of image to retain along the less-cropped side.\n",
    "  Returns:\n",
    "    crop_height: Height of image after cropping.\n",
    "    crop_width: Width of image after cropping.\n",
    "  \"\"\"\n",
    "  image_width_float = tf.cast(image_width, tf.float32)\n",
    "  image_height_float = tf.cast(image_height, tf.float32)\n",
    "\n",
    "  def _requested_aspect_ratio_wider_than_image():\n",
    "    crop_height = tf.cast(tf.math.rint(\n",
    "        crop_proportion / aspect_ratio * image_width_float), tf.int32)\n",
    "    crop_width = tf.cast(tf.math.rint(\n",
    "        crop_proportion * image_width_float), tf.int32)\n",
    "    return crop_height, crop_width\n",
    "\n",
    "  def _image_wider_than_requested_aspect_ratio():\n",
    "    crop_height = tf.cast(\n",
    "        tf.math.rint(crop_proportion * image_height_float), tf.int32)\n",
    "    crop_width = tf.cast(tf.math.rint(\n",
    "        crop_proportion * aspect_ratio *\n",
    "        image_height_float), tf.int32)\n",
    "    return crop_height, crop_width\n",
    "\n",
    "  return tf.cond(\n",
    "      aspect_ratio > image_width_float / image_height_float,\n",
    "      _requested_aspect_ratio_wider_than_image,\n",
    "      _image_wider_than_requested_aspect_ratio)\n",
    "\n",
    "\n",
    "def center_crop(image, height, width, crop_proportion):\n",
    "  \"\"\"Crops to center of image and rescales to desired size.\n",
    "  Args:\n",
    "    image: Image Tensor to crop.\n",
    "    height: Height of image to be cropped.\n",
    "    width: Width of image to be cropped.\n",
    "    crop_proportion: Proportion of image to retain along the less-cropped side.\n",
    "  Returns:\n",
    "    A `height` x `width` x channels Tensor holding a central crop of `image`.\n",
    "  \"\"\"\n",
    "  shape = tf.shape(image)\n",
    "  image_height = shape[0]\n",
    "  image_width = shape[1]\n",
    "  crop_height, crop_width = _compute_crop_shape(\n",
    "      image_height, image_width, height / width, crop_proportion)\n",
    "  offset_height = ((image_height - crop_height) + 1) // 2\n",
    "  offset_width = ((image_width - crop_width) + 1) // 2\n",
    "  image = tf.image.crop_to_bounding_box(\n",
    "      image, offset_height, offset_width, crop_height, crop_width)\n",
    "\n",
    "  image = tf.compat.v1.image.resize_bicubic([image], [height, width])[0]\n",
    "\n",
    "  return image\n",
    "\n",
    "\n",
    "def distorted_bounding_box_crop(image,\n",
    "                                bbox,\n",
    "                                min_object_covered=0.1,\n",
    "                                aspect_ratio_range=(0.75, 1.33),\n",
    "                                area_range=(0.05, 1.0),\n",
    "                                max_attempts=100,\n",
    "                                scope=None):\n",
    "  \"\"\"Generates cropped_image using one of the bboxes randomly distorted.\n",
    "  See `tf.image.sample_distorted_bounding_box` for more documentation.\n",
    "  Args:\n",
    "    image: `Tensor` of image data.\n",
    "    bbox: `Tensor` of bounding boxes arranged `[1, num_boxes, coords]`\n",
    "        where each coordinate is [0, 1) and the coordinates are arranged\n",
    "        as `[ymin, xmin, ymax, xmax]`. If num_boxes is 0 then use the whole\n",
    "        image.\n",
    "    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n",
    "        area of the image must contain at least this fraction of any bounding\n",
    "        box supplied.\n",
    "    aspect_ratio_range: An optional list of `float`s. The cropped area of the\n",
    "        image must have an aspect ratio = width / height within this range.\n",
    "    area_range: An optional list of `float`s. The cropped area of the image\n",
    "        must contain a fraction of the supplied image within in this range.\n",
    "    max_attempts: An optional `int`. Number of attempts at generating a cropped\n",
    "        region of the image of the specified constraints. After `max_attempts`\n",
    "        failures, return the entire image.\n",
    "    scope: Optional `str` for name scope.\n",
    "  Returns:\n",
    "    (cropped image `Tensor`, distorted bbox `Tensor`).\n",
    "  \"\"\"\n",
    "  with tf.name_scope(scope, 'distorted_bounding_box_crop', [image, bbox]):\n",
    "    shape = tf.shape(image)\n",
    "    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n",
    "        shape,\n",
    "        bounding_boxes=bbox,\n",
    "        min_object_covered=min_object_covered,\n",
    "        aspect_ratio_range=aspect_ratio_range,\n",
    "        area_range=area_range,\n",
    "        max_attempts=max_attempts,\n",
    "        use_image_if_no_bounding_boxes=True)\n",
    "    bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n",
    "\n",
    "    # Crop the image to the specified bounding box.\n",
    "    offset_y, offset_x, _ = tf.unstack(bbox_begin)\n",
    "    target_height, target_width, _ = tf.unstack(bbox_size)\n",
    "    image = tf.image.crop_to_bounding_box(\n",
    "        image, offset_y, offset_x, target_height, target_width)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def crop_and_resize(image, height, width):\n",
    "  \"\"\"Make a random crop and resize it to height `height` and width `width`.\n",
    "  Args:\n",
    "    image: Tensor representing the image.\n",
    "    height: Desired image height.\n",
    "    width: Desired image width.\n",
    "  Returns:\n",
    "    A `height` x `width` x channels Tensor holding a random crop of `image`.\n",
    "  \"\"\"\n",
    "  bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])\n",
    "  aspect_ratio = width / height\n",
    "  image = distorted_bounding_box_crop(\n",
    "      image,\n",
    "      bbox,\n",
    "      min_object_covered=0.1,\n",
    "      aspect_ratio_range=(3. / 4 * aspect_ratio, 4. / 3. * aspect_ratio),\n",
    "      area_range=(0.08, 1.0),\n",
    "      max_attempts=100,\n",
    "      scope=None)\n",
    "  return tf.compat.v1.image.resize_bicubic([image], [height, width])[0]\n",
    "\n",
    "\n",
    "def gaussian_blur(image, kernel_size, sigma, padding='SAME'):\n",
    "  \"\"\"Blurs the given image with separable convolution.\n",
    "  Args:\n",
    "    image: Tensor of shape [height, width, channels] and dtype float to blur.\n",
    "    kernel_size: Integer Tensor for the size of the blur kernel. This is should\n",
    "      be an odd number. If it is an even number, the actual kernel size will be\n",
    "      size + 1.\n",
    "    sigma: Sigma value for gaussian operator.\n",
    "    padding: Padding to use for the convolution. Typically 'SAME' or 'VALID'.\n",
    "  Returns:\n",
    "    A Tensor representing the blurred image.\n",
    "  \"\"\"\n",
    "  radius = tf.to_int32(kernel_size / 2)\n",
    "  kernel_size = radius * 2 + 1\n",
    "  x = tf.to_float(tf.range(-radius, radius + 1))\n",
    "  blur_filter = tf.exp(\n",
    "      -tf.pow(x, 2.0) / (2.0 * tf.pow(tf.to_float(sigma), 2.0)))\n",
    "  blur_filter /= tf.reduce_sum(blur_filter)\n",
    "  # One vertical and one horizontal filter.\n",
    "  blur_v = tf.reshape(blur_filter, [kernel_size, 1, 1, 1])\n",
    "  blur_h = tf.reshape(blur_filter, [1, kernel_size, 1, 1])\n",
    "  num_channels = tf.shape(image)[-1]\n",
    "  blur_h = tf.tile(blur_h, [1, 1, num_channels, 1])\n",
    "  blur_v = tf.tile(blur_v, [1, 1, num_channels, 1])\n",
    "  expand_batch_dim = image.shape.ndims == 3\n",
    "  if expand_batch_dim:\n",
    "    # Tensorflow requires batched input to convolutions, which we can fake with\n",
    "    # an extra dimension.\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "  blurred = tf.nn.depthwise_conv2d(\n",
    "      image, blur_h, strides=[1, 1, 1, 1], padding=padding)\n",
    "  blurred = tf.nn.depthwise_conv2d(\n",
    "      blurred, blur_v, strides=[1, 1, 1, 1], padding=padding)\n",
    "  if expand_batch_dim:\n",
    "    blurred = tf.squeeze(blurred, axis=0)\n",
    "  return blurred\n",
    "\n",
    "\n",
    "def random_crop_with_resize(image, height, width, p=1.0):\n",
    "  \"\"\"Randomly crop and resize an image.\n",
    "  Args:\n",
    "    image: `Tensor` representing an image of arbitrary size.\n",
    "    height: Height of output image.\n",
    "    width: Width of output image.\n",
    "    p: Probability of applying this transformation.\n",
    "  Returns:\n",
    "    A preprocessed image `Tensor`.\n",
    "  \"\"\"\n",
    "  def _transform(image):  # pylint: disable=missing-docstring\n",
    "    image = crop_and_resize(image, height, width)\n",
    "    return image\n",
    "  return random_apply(_transform, p=p, x=image)\n",
    "\n",
    "\n",
    "def random_color_jitter(image, p=1.0):\n",
    "  def _transform(image):\n",
    "    color_jitter_t = functools.partial(\n",
    "        color_jitter, strength=FLAGS_color_jitter_strength)\n",
    "    image = random_apply(color_jitter_t, p=0.8, x=image)\n",
    "    return random_apply(to_grayscale, p=0.2, x=image)\n",
    "  return random_apply(_transform, p=p, x=image)\n",
    "\n",
    "\n",
    "def random_blur(image, height, width, p=1.0):\n",
    "  \"\"\"Randomly blur an image.\n",
    "  Args:\n",
    "    image: `Tensor` representing an image of arbitrary size.\n",
    "    height: Height of output image.\n",
    "    width: Width of output image.\n",
    "    p: probability of applying this transformation.\n",
    "  Returns:\n",
    "    A preprocessed image `Tensor`.\n",
    "  \"\"\"\n",
    "  del width\n",
    "  def _transform(image):\n",
    "    sigma = tf.random.uniform([], 0.1, 2.0, dtype=tf.float32)\n",
    "    return gaussian_blur(\n",
    "        image, kernel_size=height//10, sigma=sigma, padding='SAME')\n",
    "  return random_apply(_transform, p=p, x=image)\n",
    "\n",
    "\n",
    "def batch_random_blur(images_list, height, width, blur_probability=0.5):\n",
    "  \"\"\"Apply efficient batch data transformations.\n",
    "  Args:\n",
    "    images_list: a list of image tensors.\n",
    "    height: the height of image.\n",
    "    width: the width of image.\n",
    "    blur_probability: the probaility to apply the blur operator.\n",
    "  Returns:\n",
    "    Preprocessed feature list.\n",
    "  \"\"\"\n",
    "  def generate_selector(p, bsz):\n",
    "    shape = [bsz, 1, 1, 1]\n",
    "    selector = tf.cast(\n",
    "        tf.less(tf.random_uniform(shape, 0, 1, dtype=tf.float32), p),\n",
    "        tf.float32)\n",
    "    return selector\n",
    "\n",
    "  new_images_list = []\n",
    "  for images in images_list:\n",
    "    images_new = random_blur(images, height, width, p=1.)\n",
    "    selector = generate_selector(blur_probability, tf.shape(images)[0])\n",
    "    images = images_new * selector + images * (1 - selector)\n",
    "    images = tf.clip_by_value(images, 0., 1.)\n",
    "    new_images_list.append(images)\n",
    "\n",
    "  return new_images_list\n",
    "\n",
    "\n",
    "def preprocess_for_train(image, height, width,\n",
    "                         color_distort=True, crop=True, flip=True):\n",
    "  \"\"\"Preprocesses the given image for training.\n",
    "  Args:\n",
    "    image: `Tensor` representing an image of arbitrary size.\n",
    "    height: Height of output image.\n",
    "    width: Width of output image.\n",
    "    color_distort: Whether to apply the color distortion.\n",
    "    crop: Whether to crop the image.\n",
    "    flip: Whether or not to flip left and right of an image.\n",
    "  Returns:\n",
    "    A preprocessed image `Tensor`.\n",
    "  \"\"\"\n",
    "  if crop:\n",
    "    image = random_crop_with_resize(image, height, width)\n",
    "  if flip:\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "  if color_distort:\n",
    "    image = random_color_jitter(image)\n",
    "  image = tf.reshape(image, [height, width, 3])\n",
    "  image = tf.clip_by_value(image, 0., 1.)\n",
    "  return image\n",
    "\n",
    "\n",
    "def preprocess_for_eval(image, height, width, crop=True):\n",
    "  \"\"\"Preprocesses the given image for evaluation.\n",
    "  Args:\n",
    "    image: `Tensor` representing an image of arbitrary size.\n",
    "    height: Height of output image.\n",
    "    width: Width of output image.\n",
    "    crop: Whether or not to (center) crop the test images.\n",
    "  Returns:\n",
    "    A preprocessed image `Tensor`.\n",
    "  \"\"\"\n",
    "  if crop:\n",
    "    image = center_crop(image, height, width, crop_proportion=CROP_PROPORTION)\n",
    "  image = tf.reshape(image, [height, width, 3])\n",
    "  image = tf.clip_by_value(image, 0., 1.)\n",
    "  return image\n",
    "\n",
    "\n",
    "def preprocess_image(image, height, width, is_training=False,\n",
    "                     color_distort=True, test_crop=True):\n",
    "  \"\"\"Preprocesses the given image.\n",
    "  Args:\n",
    "    image: `Tensor` representing an image of arbitrary size.\n",
    "    height: Height of output image.\n",
    "    width: Width of output image.\n",
    "    is_training: `bool` for whether the preprocessing is for training.\n",
    "    color_distort: whether to apply the color distortion.\n",
    "    test_crop: whether or not to extract a central crop of the images\n",
    "        (as for standard ImageNet evaluation) during the evaluation.\n",
    "  Returns:\n",
    "    A preprocessed image `Tensor` of range [0, 1].\n",
    "  \"\"\"\n",
    "  image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "  if is_training:\n",
    "    return preprocess_for_train(image, height, width, color_distort)\n",
    "  else:\n",
    "    return preprocess_for_eval(image, height, width, test_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e26e4cbc-3686-4f65-9dac-13625e8d8351",
   "metadata": {},
   "outputs": [],
   "source": [
    "EETA_DEFAULT = 0.001\n",
    "\n",
    "\n",
    "class LARSOptimizer(tf.keras.optimizers.Optimizer):\n",
    "  \"\"\"Layer-wise Adaptive Rate Scaling for large batch training.\n",
    "\n",
    "  Introduced by \"Large Batch Training of Convolutional Networks\" by Y. You,\n",
    "  I. Gitman, and B. Ginsburg. (https://arxiv.org/abs/1708.03888)\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               learning_rate,\n",
    "               momentum=0.9,\n",
    "               use_nesterov=False,\n",
    "               weight_decay=0.0,\n",
    "               exclude_from_weight_decay=None,\n",
    "               exclude_from_layer_adaptation=None,\n",
    "               classic_momentum=True,\n",
    "               eeta=EETA_DEFAULT,\n",
    "               name=\"LARSOptimizer\"):\n",
    "    \"\"\"Constructs a LARSOptimizer.\n",
    "\n",
    "    Args:\n",
    "      learning_rate: A `float` for learning rate.\n",
    "      momentum: A `float` for momentum.\n",
    "      use_nesterov: A 'Boolean' for whether to use nesterov momentum.\n",
    "      weight_decay: A `float` for weight decay.\n",
    "      exclude_from_weight_decay: A list of `string` for variable screening, if\n",
    "          any of the string appears in a variable's name, the variable will be\n",
    "          excluded for computing weight decay. For example, one could specify\n",
    "          the list like ['batch_normalization', 'bias'] to exclude BN and bias\n",
    "          from weight decay.\n",
    "      exclude_from_layer_adaptation: Similar to exclude_from_weight_decay, but\n",
    "          for layer adaptation. If it is None, it will be defaulted the same as\n",
    "          exclude_from_weight_decay.\n",
    "      classic_momentum: A `boolean` for whether to use classic (or popular)\n",
    "          momentum. The learning rate is applied during momeuntum update in\n",
    "          classic momentum, but after momentum for popular momentum.\n",
    "      eeta: A `float` for scaling of learning rate when computing trust ratio.\n",
    "      name: The name for the scope.\n",
    "    \"\"\"\n",
    "    super(LARSOptimizer, self).__init__(name)\n",
    "\n",
    "    self._set_hyper(\"learning_rate\", learning_rate)\n",
    "    self.momentum = momentum\n",
    "    self.weight_decay = weight_decay\n",
    "    self.use_nesterov = use_nesterov\n",
    "    self.classic_momentum = classic_momentum\n",
    "    self.eeta = eeta\n",
    "    self.exclude_from_weight_decay = exclude_from_weight_decay\n",
    "    # exclude_from_layer_adaptation is set to exclude_from_weight_decay if the\n",
    "    # arg is None.\n",
    "    if exclude_from_layer_adaptation:\n",
    "      self.exclude_from_layer_adaptation = exclude_from_layer_adaptation\n",
    "    else:\n",
    "      self.exclude_from_layer_adaptation = exclude_from_weight_decay\n",
    "\n",
    "  def _create_slots(self, var_list):\n",
    "    for v in var_list:\n",
    "      self.add_slot(v, \"Momentum\")\n",
    "\n",
    "  def _resource_apply_dense(self, grad, param, apply_state=None):\n",
    "    if grad is None or param is None:\n",
    "      return tf.no_op()\n",
    "\n",
    "    var_device, var_dtype = param.device, param.dtype.base_dtype\n",
    "    coefficients = ((apply_state or {}).get((var_device, var_dtype)) or\n",
    "                    self._fallback_apply_state(var_device, var_dtype))\n",
    "    learning_rate = coefficients[\"lr_t\"]\n",
    "\n",
    "    param_name = param.name\n",
    "\n",
    "    v = self.get_slot(param, \"Momentum\")\n",
    "\n",
    "    if self._use_weight_decay(param_name):\n",
    "      grad += self.weight_decay * param\n",
    "\n",
    "    if self.classic_momentum:\n",
    "      trust_ratio = 1.0\n",
    "      if self._do_layer_adaptation(param_name):\n",
    "        w_norm = tf.norm(param, ord=2)\n",
    "        g_norm = tf.norm(grad, ord=2)\n",
    "        trust_ratio = tf.where(\n",
    "            tf.greater(w_norm, 0),\n",
    "            tf.where(tf.greater(g_norm, 0), (self.eeta * w_norm / g_norm), 1.0),\n",
    "            1.0)\n",
    "      scaled_lr = learning_rate * trust_ratio\n",
    "\n",
    "      next_v = tf.multiply(self.momentum, v) + scaled_lr * grad\n",
    "      if self.use_nesterov:\n",
    "        update = tf.multiply(self.momentum, next_v) + scaled_lr * grad\n",
    "      else:\n",
    "        update = next_v\n",
    "      next_param = param - update\n",
    "    else:\n",
    "      next_v = tf.multiply(self.momentum, v) + grad\n",
    "      if self.use_nesterov:\n",
    "        update = tf.multiply(self.momentum, next_v) + grad\n",
    "      else:\n",
    "        update = next_v\n",
    "\n",
    "      trust_ratio = 1.0\n",
    "      if self._do_layer_adaptation(param_name):\n",
    "        w_norm = tf.norm(param, ord=2)\n",
    "        v_norm = tf.norm(update, ord=2)\n",
    "        trust_ratio = tf.where(\n",
    "            tf.greater(w_norm, 0),\n",
    "            tf.where(tf.greater(v_norm, 0), (self.eeta * w_norm / v_norm), 1.0),\n",
    "            1.0)\n",
    "      scaled_lr = trust_ratio * learning_rate\n",
    "      next_param = param - scaled_lr * update\n",
    "\n",
    "    return tf.group(*[\n",
    "        param.assign(next_param, use_locking=False),\n",
    "        v.assign(next_v, use_locking=False)\n",
    "    ])\n",
    "\n",
    "  def _use_weight_decay(self, param_name):\n",
    "    \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n",
    "    if not self.weight_decay:\n",
    "      return False\n",
    "    if self.exclude_from_weight_decay:\n",
    "      for r in self.exclude_from_weight_decay:\n",
    "        if re.search(r, param_name) is not None:\n",
    "          return False\n",
    "    return True\n",
    "\n",
    "  def _do_layer_adaptation(self, param_name):\n",
    "    \"\"\"Whether to do layer-wise learning rate adaptation for `param_name`.\"\"\"\n",
    "    if self.exclude_from_layer_adaptation:\n",
    "      for r in self.exclude_from_layer_adaptation:\n",
    "        if re.search(r, param_name) is not None:\n",
    "          return False\n",
    "    return True\n",
    "\n",
    "  def get_config(self):\n",
    "    config = super(LARSOptimizer, self).get_config()\n",
    "    config.update({\n",
    "        \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n",
    "        \"momentum\": self.momentum,\n",
    "        \"classic_momentum\": self.classic_momentum,\n",
    "        \"weight_decay\": self.weight_decay,\n",
    "        \"eeta\": self.eeta,\n",
    "        \"use_nesterov\": self.use_nesterov,\n",
    "    })\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f29e85b-6a63-4e05-b430-3390cdc00104",
   "metadata": {},
   "source": [
    "## Image libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af31c0f8-1776-48f8-b37f-514b210b49f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "# from matplotlib. import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420d7755-321f-4af3-bd84-3b284d59cdc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3a362bf-8bf0-4904-ba92-9fa610e1aa38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CNV', 'DME', 'DRUSEN', 'NORMAL']\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = './OCT2017/all_data'\n",
    "data_listing = os.listdir(DATA_PATH)\n",
    "print(data_listing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4649bdff-1d92-40c8-a7a7-0a0b264a8638",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir= DATA_PATH\n",
    "CNV_PATH = os.path.join(train_dir, 'CNV')\n",
    "DME_PATH = os.path.join(train_dir, 'DME')\n",
    "NORMAL_PATH = os.path.join(train_dir, 'NORMAL')\n",
    "DRUSEN_PATH = os.path.join(train_dir, 'DRUSEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2feed38-44b2-4f6f-aa1f-5ad11304f637",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnv = os.listdir(CNV_PATH)\n",
    "dme = os.listdir(DME_PATH)\n",
    "normal = os.listdir(NORMAL_PATH)\n",
    "drusen = os.listdir(DRUSEN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c1e5d92-be42-4531-b1a6-6dd4cc904f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37205\n",
      "11348\n",
      "26315\n",
      "8616\n"
     ]
    }
   ],
   "source": [
    "print(len(cnv))\n",
    "print(len(dme))\n",
    "print(len(normal))\n",
    "print(len(drusen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8623bff1-0817-4ea3-9457-668fa82676e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 66788 images belonging to 4 classes.\n",
      "Found 16696 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "epochs=6\n",
    "batch_size=16\n",
    "steps=60000//batch_size\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255., data_format=\"channels_last\", \n",
    "                                   validation_split=0.20,\n",
    "                                   horizontal_flip=True,\n",
    "                                   rotation_range=37,\n",
    "                                   vertical_flip=True)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    # target_size=(256, 256),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    subset='training',\n",
    "                                                    class_mode='binary',\n",
    "                                                    color_mode='rgb')\n",
    "\n",
    "\n",
    "valid_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    # target_size=(96, 96), \n",
    "                                                    batch_size=batch_size,\n",
    "                                                    shuffle=False,\n",
    "                                                    subset='validation',\n",
    "                                                    class_mode='binary',\n",
    "                                                    color_mode='rgb')\n",
    "\n",
    "\n",
    "\n",
    "best_path='./best_val_acc_'+str(int(time.time()))\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=best_path,\n",
    "    monitor='val_acc',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    initial_value_threshold=0.92)\n",
    "               \n",
    "       \n",
    "\n",
    "def _lrs(epoch, lr):\n",
    "    return lr\n",
    "    \n",
    "lrs = LearningRateScheduler(_lrs) \n",
    " \n",
    "stop = EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        min_delta=0.0001,\n",
    "        patience=10,\n",
    "        verbose=1,\n",
    "        mode='auto',\n",
    "        baseline=None,\n",
    "        restore_best_weights=True\n",
    ")\n",
    "\n",
    "stop_acc = EarlyStopping(\n",
    "            monitor='acc',\n",
    "            min_delta=0.001,\n",
    "            patience=10,\n",
    "            verbose=1,\n",
    "            mode='auto',\n",
    "            baseline=None,\n",
    "            restore_best_weights=True)\n",
    "\n",
    "logdir='./logs'\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12cdb3fd-ef9d-4ca3-9561-38ca24a66640",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "weight_decay = 0.\n",
    "num_classes = 4\n",
    "class Model(tf.keras.Model):\n",
    "  def __init__(self, path):\n",
    "    super(Model, self).__init__()\n",
    "    self.saved_model = tf.saved_model.load(path)\n",
    "    self.dense_layer = tf.keras.layers.Dense(units=num_classes, name=\"head_supervised_new\")\n",
    "    self.optimizer = LARSOptimizer(\n",
    "      learning_rate,\n",
    "      momentum=momentum,\n",
    "      weight_decay=weight_decay,\n",
    "      exclude_from_weight_decay=['batch_normalization', 'bias', 'head_supervised'])\n",
    "\n",
    "  def call(self, x, training=True):\n",
    "    with tf.GradientTape() as tape:\n",
    "      outputs = self.saved_model(x[0], trainable=False)\n",
    "      print(outputs)\n",
    "      logits_t = self.dense_layer(outputs['final_avg_pool'])\n",
    "      loss_t = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels = tf.one_hot(x[1], num_classes), logits=logits_t))\n",
    "      if training:\n",
    "          dense_layer_weights = self.dense_layer.trainable_weights\n",
    "          print('Variables to train:', dense_layer_weights)\n",
    "          grads = tape.gradient(loss_t, dense_layer_weights)\n",
    "          self.optimizer.apply_gradients(zip(grads, dense_layer_weights))\n",
    "    return loss_t, x[0], logits_t, x[1]\n",
    "\n",
    "# model = Model(\"gs://simclr-checkpoints-tf2/simclrv2/finetuned_100pct/r50_1x_sk0/saved_model/\")\n",
    "\n",
    "# Remove this for debugging.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78d85efa-6825-4d67-bbd0-c71147ef5259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(m, generator, epochs=10, training=True):\n",
    "    steps = len(generator.filenames)//generator.batch_size \n",
    "    for e in range(epochs):\n",
    "        print(f'Starting epoch: {e+1}')\n",
    "        total_correct = 0 \n",
    "        total_processed = 0 \n",
    "        total_loss = 0\n",
    "        epoch_start_time = time.time()\n",
    "        for it in range(steps):\n",
    "            x = next(generator)\n",
    "            # x[0] = preprocess_image(\n",
    "            #      x[0], 256, 256, is_training=False, color_distort=False)\n",
    "            xx = (x[0], np.int32(x[1]))\n",
    "            loss, image, logits, labels = train_step(m, xx, training=training)\n",
    "            logits = logits.numpy()\n",
    "            labels = labels.numpy()\n",
    "            pred = logits.argmax(-1)\n",
    "            correct = np.sum(pred == labels)\n",
    "            total = labels.size\n",
    "            total_processed += total\n",
    "            total_loss += loss\n",
    "            total_correct += correct\n",
    "            if (it+1) % 300 == 0:\n",
    "                print(\"[Iter {}] Loss: {} Top 1: {}\".format(it+1, total_loss/total_processed, total_correct/float(total_processed)))\n",
    "        run_time = (time.time() - epoch_start_time)/60 \n",
    "        print(f\"Loss: {total_loss/total_processed} Top 1: {total_correct/total_processed}\")\n",
    "        print(f\"Total time: {run_time} in minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e316fad-dff5-4e65-9848-090acf242d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "977e3f72-b302-4ecc-b771-21bae17b49b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(m, x, training=True):\n",
    "  return m(x, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f868baa-e328-4e6d-9850-6ae74e65b9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 08:08:48.680858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-27 08:08:48.698906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-27 08:08:48.699014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-27 08:08:48.699694: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-27 08:08:48.700239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-27 08:08:48.700340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-27 08:08:48.700433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-27 08:08:48.953631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-27 08:08:48.953766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-27 08:08:48.953865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-27 08:08:48.953954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10248 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:08:00.0, compute capability: 8.6\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_9_layer_call_and_return_conditional_losses_20526) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_54_layer_call_and_return_conditional_losses_30270) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_60_layer_call_and_return_conditional_losses_31578) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_15_layer_call_and_return_conditional_losses_44504) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_35_layer_call_and_return_conditional_losses_26166) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_31_layer_call_and_return_conditional_losses_49664) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_20_layer_call_and_return_conditional_losses_22914) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_70_layer_call_and_return_conditional_losses_47984) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_47_layer_call_and_return_conditional_losses_28758) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_24_layer_call_and_return_conditional_losses_45104) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_43_layer_call_and_return_conditional_losses_50264) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_19_layer_call_and_return_conditional_losses_22698) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_5_layer_call_and_return_conditional_losses_48104) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_36_layer_call_and_return_conditional_losses_26382) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_3_layer_call_and_return_conditional_losses_43784) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_23_layer_call_and_return_conditional_losses_49184) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_30_layer_call_and_return_conditional_losses_49544) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_29_layer_call_and_return_conditional_losses_45464) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_33_layer_call_and_return_conditional_losses_45704) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_4_layer_call_and_return_conditional_losses_19446) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_73_layer_call_and_return_conditional_losses_43670) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_17_layer_call_and_return_conditional_losses_22266) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_54_layer_call_and_return_conditional_losses_47024) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_26_layer_call_and_return_conditional_losses_24210) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_39_layer_call_and_return_conditional_losses_50024) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_11_layer_call_and_return_conditional_losses_20958) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_46_layer_call_and_return_conditional_losses_28542) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_13_layer_call_and_return_conditional_losses_21390) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_26_layer_call_and_return_conditional_losses_49304) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_63_layer_call_and_return_conditional_losses_32226) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_67_layer_call_and_return_conditional_losses_47864) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_61_layer_call_and_return_conditional_losses_51344) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_55_layer_call_and_return_conditional_losses_30486) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_3_layer_call_and_return_conditional_losses_19230) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_51_layer_call_and_return_conditional_losses_50744) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_31_layer_call_and_return_conditional_losses_25290) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_57_layer_call_and_return_conditional_losses_47144) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_42_layer_call_and_return_conditional_losses_27678) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_35_layer_call_and_return_conditional_losses_49784) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_39_layer_call_and_return_conditional_losses_27030) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_2_layer_call_and_return_conditional_losses_18990) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_53_layer_call_and_return_conditional_losses_46904) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_73_layer_call_and_return_conditional_losses_34314) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_1_layer_call_and_return_conditional_losses_43225) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_24_layer_call_and_return_conditional_losses_23778) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_layer_call_and_return_conditional_losses_43105) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_33_layer_call_and_return_conditional_losses_25734) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_49_layer_call_and_return_conditional_losses_46664) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_14_layer_call_and_return_conditional_losses_48704) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_34_layer_call_and_return_conditional_losses_25950) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_25_layer_call_and_return_conditional_losses_23994) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_55_layer_call_and_return_conditional_losses_50984) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_71_layer_call_and_return_conditional_losses_43454) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_53_layer_call_and_return_conditional_losses_30054) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_56_layer_call_and_return_conditional_losses_51104) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_32_layer_call_and_return_conditional_losses_45584) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_38_layer_call_and_return_conditional_losses_46064) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_46_layer_call_and_return_conditional_losses_46544) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_23_layer_call_and_return_conditional_losses_23562) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_45_layer_call_and_return_conditional_losses_28326) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_29_layer_call_and_return_conditional_losses_24858) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_28_layer_call_and_return_conditional_losses_24642) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_10_layer_call_and_return_conditional_losses_20742) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_40_layer_call_and_return_conditional_losses_50144) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_62_layer_call_and_return_conditional_losses_32010) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_19_layer_call_and_return_conditional_losses_48944) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_72_layer_call_and_return_conditional_losses_34126) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_21_layer_call_and_return_conditional_losses_23130) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_52_layer_call_and_return_conditional_losses_50864) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_61_layer_call_and_return_conditional_losses_31794) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_16_layer_call_and_return_conditional_losses_22050) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_40_layer_call_and_return_conditional_losses_27246) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_6_layer_call_and_return_conditional_losses_48224) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_41_layer_call_and_return_conditional_losses_46184) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_65_layer_call_and_return_conditional_losses_32658) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_36_layer_call_and_return_conditional_losses_49904) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_layer_call_and_return_conditional_losses_18558) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference___call___40673) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_5_layer_call_and_return_conditional_losses_19662) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_27_layer_call_and_return_conditional_losses_24426) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_13_layer_call_and_return_conditional_losses_48584) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_12_layer_call_and_return_conditional_losses_44384) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_60_layer_call_and_return_conditional_losses_51224) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_22_layer_call_and_return_conditional_losses_23346) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_12_layer_call_and_return_conditional_losses_21174) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_70_layer_call_and_return_conditional_losses_33738) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_66_layer_call_and_return_conditional_losses_32874) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_10_layer_call_and_return_conditional_losses_48464) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_38_layer_call_and_return_conditional_losses_26814) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_6_layer_call_and_return_conditional_losses_19878) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_30_layer_call_and_return_conditional_losses_25074) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_42_layer_call_and_return_conditional_losses_46304) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_67_layer_call_and_return_conditional_losses_33090) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_64_layer_call_and_return_conditional_losses_51464) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_25_layer_call_and_return_conditional_losses_45224) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_44_layer_call_and_return_conditional_losses_28110) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_68_layer_call_and_return_conditional_losses_51704) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_48_layer_call_and_return_conditional_losses_28974) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_17_layer_call_and_return_conditional_losses_44744) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_71_layer_call_and_return_conditional_losses_33932) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_16_layer_call_and_return_conditional_losses_44624) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_50_layer_call_and_return_conditional_losses_29406) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_28_layer_call_and_return_conditional_losses_45344) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_4_layer_call_and_return_conditional_losses_43904) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_15_layer_call_and_return_conditional_losses_21822) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_62_layer_call_and_return_conditional_losses_47504) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_8_layer_call_and_return_conditional_losses_44144) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_7_layer_call_and_return_conditional_losses_20094) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_63_layer_call_and_return_conditional_losses_47624) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_44_layer_call_and_return_conditional_losses_50384) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_57_layer_call_and_return_conditional_losses_30918) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_34_layer_call_and_return_conditional_losses_45824) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_37_layer_call_and_return_conditional_losses_26598) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_49_layer_call_and_return_conditional_losses_29190) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_14_layer_call_and_return_conditional_losses_21606) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_66_layer_call_and_return_conditional_losses_47744) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_47_layer_call_and_return_conditional_losses_50504) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_64_layer_call_and_return_conditional_losses_32442) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_11_layer_call_and_return_conditional_losses_44264) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_69_layer_call_and_return_conditional_losses_33522) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_58_layer_call_and_return_conditional_losses_31146) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_41_layer_call_and_return_conditional_losses_27462) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_2_layer_call_and_return_conditional_losses_43345) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_37_layer_call_and_return_conditional_losses_45944) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_43_layer_call_and_return_conditional_losses_27894) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_45_layer_call_and_return_conditional_losses_46424) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_18_layer_call_and_return_conditional_losses_48824) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_8_layer_call_and_return_conditional_losses_20310) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_21_layer_call_and_return_conditional_losses_44984) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_68_layer_call_and_return_conditional_losses_33306) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_27_layer_call_and_return_conditional_losses_49424) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_50_layer_call_and_return_conditional_losses_46784) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_69_layer_call_and_return_conditional_losses_51824) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_59_layer_call_and_return_conditional_losses_47384) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_52_layer_call_and_return_conditional_losses_29838) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_72_layer_call_and_return_conditional_losses_43563) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_48_layer_call_and_return_conditional_losses_50624) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_9_layer_call_and_return_conditional_losses_48344) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_65_layer_call_and_return_conditional_losses_51584) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_58_layer_call_and_return_conditional_losses_47264) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_59_layer_call_and_return_conditional_losses_31362) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_32_layer_call_and_return_conditional_losses_25506) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_1_layer_call_and_return_conditional_losses_18774) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_56_layer_call_and_return_conditional_losses_30702) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_22_layer_call_and_return_conditional_losses_49064) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_20_layer_call_and_return_conditional_losses_44864) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_51_layer_call_and_return_conditional_losses_29622) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_7_layer_call_and_return_conditional_losses_44024) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_18_layer_call_and_return_conditional_losses_22482) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    }
   ],
   "source": [
    "model_r50_10 = Model(\"./10pct/r50_2x_sk1/saved_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca4816b0-5960-4008-b340-8f6df2b32ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer=keras.optimizers.adam_v2.Adam(learning_rate=0.000001), loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "model_r50_10.compile(loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31526744-2c56-46a5-859d-cbe8c28b27d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CNV': 0, 'DME': 1, 'DRUSEN': 2, 'NORMAL': 3}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20b31431-d5c0-4373-a44b-2caafda2612d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3750\n",
      "6\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(steps)\n",
    "print(epochs)\n",
    "print(batch_size)\n",
    "# res101_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "425613e8-3d69-4330-9534-51bcef3c2cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 1\n",
      "{'final_avg_pool': <tf.Tensor 'model/StatefulPartitionedCall:4' shape=(16, 4096) dtype=float32>, 'initial_conv': <tf.Tensor 'model/StatefulPartitionedCall:5' shape=(16, 128, 128, 128) dtype=float32>, 'initial_max_pool': <tf.Tensor 'model/StatefulPartitionedCall:6' shape=(16, 64, 64, 128) dtype=float32>, 'block_group2': <tf.Tensor 'model/StatefulPartitionedCall:1' shape=(16, 32, 32, 1024) dtype=float32>, 'block_group1': <tf.Tensor 'model/StatefulPartitionedCall:0' shape=(16, 64, 64, 512) dtype=float32>, 'block_group3': <tf.Tensor 'model/StatefulPartitionedCall:2' shape=(16, 16, 16, 2048) dtype=float32>, 'logits_sup': <tf.Tensor 'model/StatefulPartitionedCall:7' shape=(16, 1000) dtype=float32>, 'block_group4': <tf.Tensor 'model/StatefulPartitionedCall:3' shape=(16, 8, 8, 4096) dtype=float32>}\n",
      "Variables to train: [<tf.Variable 'model/head_supervised_new/kernel:0' shape=(4096, 4) dtype=float32>, <tf.Variable 'model/head_supervised_new/bias:0' shape=(4,) dtype=float32>]\n",
      "{'final_avg_pool': <tf.Tensor 'model/StatefulPartitionedCall:4' shape=(16, 4096) dtype=float32>, 'initial_conv': <tf.Tensor 'model/StatefulPartitionedCall:5' shape=(16, 128, 128, 128) dtype=float32>, 'initial_max_pool': <tf.Tensor 'model/StatefulPartitionedCall:6' shape=(16, 64, 64, 128) dtype=float32>, 'block_group2': <tf.Tensor 'model/StatefulPartitionedCall:1' shape=(16, 32, 32, 1024) dtype=float32>, 'block_group1': <tf.Tensor 'model/StatefulPartitionedCall:0' shape=(16, 64, 64, 512) dtype=float32>, 'block_group3': <tf.Tensor 'model/StatefulPartitionedCall:2' shape=(16, 16, 16, 2048) dtype=float32>, 'logits_sup': <tf.Tensor 'model/StatefulPartitionedCall:7' shape=(16, 1000) dtype=float32>, 'block_group4': <tf.Tensor 'model/StatefulPartitionedCall:3' shape=(16, 8, 8, 4096) dtype=float32>}\n",
      "Variables to train: [<tf.Variable 'model/head_supervised_new/kernel:0' shape=(4096, 4) dtype=float32>, <tf.Variable 'model/head_supervised_new/bias:0' shape=(4,) dtype=float32>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 08:09:05.339283: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8302\n",
      "2022-04-27 08:09:07.587199: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 300] Loss: 0.05195876583456993 Top 1: 0.7077083333333334\n",
      "[Iter 600] Loss: 0.045702312141656876 Top 1: 0.7419791666666666\n",
      "[Iter 900] Loss: 0.04190608859062195 Top 1: 0.7645833333333333\n",
      "[Iter 1200] Loss: 0.03924291208386421 Top 1: 0.7795833333333333\n",
      "[Iter 1500] Loss: 0.037307873368263245 Top 1: 0.7899583333333333\n",
      "[Iter 1800] Loss: 0.03588515520095825 Top 1: 0.7973263888888888\n",
      "[Iter 2100] Loss: 0.03473987802863121 Top 1: 0.8035119047619048\n",
      "[Iter 2400] Loss: 0.03373759612441063 Top 1: 0.8086979166666667\n",
      "[Iter 2700] Loss: 0.0329405777156353 Top 1: 0.8130324074074075\n",
      "[Iter 3000] Loss: 0.03221418336033821 Top 1: 0.8172708333333333\n",
      "[Iter 3300] Loss: 0.03154420480132103 Top 1: 0.8210037878787879\n",
      "[Iter 3600] Loss: 0.03101951815187931 Top 1: 0.8237152777777778\n",
      "[Iter 3900] Loss: 0.03049336187541485 Top 1: 0.8266666666666667\n",
      "Loss: 0.029957395046949387 Top 1: 0.8294351940584571\n",
      "Total time: 21.302076816558838 in minutes\n",
      "Starting epoch: 2\n",
      "{'final_avg_pool': <tf.Tensor 'model/StatefulPartitionedCall:4' shape=(4, 4096) dtype=float32>, 'initial_conv': <tf.Tensor 'model/StatefulPartitionedCall:5' shape=(4, 128, 128, 128) dtype=float32>, 'initial_max_pool': <tf.Tensor 'model/StatefulPartitionedCall:6' shape=(4, 64, 64, 128) dtype=float32>, 'block_group2': <tf.Tensor 'model/StatefulPartitionedCall:1' shape=(4, 32, 32, 1024) dtype=float32>, 'block_group1': <tf.Tensor 'model/StatefulPartitionedCall:0' shape=(4, 64, 64, 512) dtype=float32>, 'block_group3': <tf.Tensor 'model/StatefulPartitionedCall:2' shape=(4, 16, 16, 2048) dtype=float32>, 'logits_sup': <tf.Tensor 'model/StatefulPartitionedCall:7' shape=(4, 1000) dtype=float32>, 'block_group4': <tf.Tensor 'model/StatefulPartitionedCall:3' shape=(4, 8, 8, 4096) dtype=float32>}\n",
      "Variables to train: [<tf.Variable 'model/head_supervised_new/kernel:0' shape=(4096, 4) dtype=float32>, <tf.Variable 'model/head_supervised_new/bias:0' shape=(4,) dtype=float32>]\n",
      "[Iter 300] Loss: 0.023490965366363525 Top 1: 0.8711361737677528\n",
      "[Iter 600] Loss: 0.022416697815060616 Top 1: 0.8753650396328744\n",
      "[Iter 900] Loss: 0.022663921117782593 Top 1: 0.8712121212121212\n",
      "[Iter 1200] Loss: 0.022462064400315285 Top 1: 0.872993537627684\n",
      "[Iter 1500] Loss: 0.02248270995914936 Top 1: 0.8728114057028514\n",
      "[Iter 1800] Loss: 0.022310767322778702 Top 1: 0.8733847436431846\n",
      "[Iter 2100] Loss: 0.022229718044400215 Top 1: 0.873436941764916\n",
      "[Iter 2400] Loss: 0.02215738408267498 Top 1: 0.8733979368552672\n",
      "[Iter 2700] Loss: 0.02210431732237339 Top 1: 0.8734602204316013\n",
      "[Iter 3000] Loss: 0.02193133533000946 Top 1: 0.8743644244394432\n",
      "[Iter 3300] Loss: 0.02181464619934559 Top 1: 0.8751989088429188\n",
      "[Iter 3600] Loss: 0.021744713187217712 Top 1: 0.8755469889560326\n",
      "[Iter 3900] Loss: 0.021752482280135155 Top 1: 0.8753927037250754\n",
      "Loss: 0.021676432341337204 Top 1: 0.8758162103875876\n",
      "Total time: 21.359083930651348 in minutes\n",
      "Starting epoch: 3\n",
      "[Iter 300] Loss: 0.020038850605487823 Top 1: 0.883876357560568\n",
      "[Iter 600] Loss: 0.020540477707982063 Top 1: 0.8806841885690446\n",
      "[Iter 900] Loss: 0.020674239844083786 Top 1: 0.8810814567695302\n",
      "[Iter 1200] Loss: 0.02052348107099533 Top 1: 0.8821138211382114\n",
      "[Iter 1500] Loss: 0.02047816477715969 Top 1: 0.8822327830581957\n",
      "[Iter 1800] Loss: 0.020671745762228966 Top 1: 0.881547867166875\n",
      "[Iter 2100] Loss: 0.020590361207723618 Top 1: 0.8818625699654639\n",
      "[Iter 2400] Loss: 0.020330501720309258 Top 1: 0.883401062832135\n",
      "[Iter 2700] Loss: 0.02026647888123989 Top 1: 0.8840650180605724\n",
      "[Iter 3000] Loss: 0.020234566181898117 Top 1: 0.8844086021505376\n",
      "[Iter 3300] Loss: 0.020278871059417725 Top 1: 0.8839319542320224\n",
      "[Iter 3600] Loss: 0.02018078602850437 Top 1: 0.8848371188442037\n",
      "[Iter 3900] Loss: 0.019994355738162994 Top 1: 0.886019747387318\n",
      "Loss: 0.019965533167123795 Top 1: 0.8864494099323069\n",
      "Total time: 21.32531813780467 in minutes\n",
      "Starting epoch: 4\n",
      "[Iter 300] Loss: 0.019298087805509567 Top 1: 0.8959899749373433\n",
      "[Iter 600] Loss: 0.019270675256848335 Top 1: 0.8917396745932415\n",
      "[Iter 900] Loss: 0.01916770450770855 Top 1: 0.8913678065054211\n",
      "[Iter 1200] Loss: 0.019053859636187553 Top 1: 0.8909214092140921\n",
      "[Iter 1500] Loss: 0.01919688656926155 Top 1: 0.8901117225279306\n",
      "[Iter 1800] Loss: 0.019120873883366585 Top 1: 0.8904751979991663\n",
      "[Iter 2100] Loss: 0.01918318122625351 Top 1: 0.8899607002500893\n",
      "[Iter 2400] Loss: 0.019209278747439384 Top 1: 0.8894706679170574\n",
      "[Iter 2700] Loss: 0.01909765787422657 Top 1: 0.890779846253589\n",
      "[Iter 3000] Loss: 0.018962029367685318 Top 1: 0.891931316162374\n",
      "[Iter 3300] Loss: 0.01900823973119259 Top 1: 0.891566265060241\n",
      "[Iter 3600] Loss: 0.019000286236405373 Top 1: 0.8915572688754602\n",
      "[Iter 3900] Loss: 0.01897820457816124 Top 1: 0.8915817144322626\n",
      "Loss: 0.018949685618281364 Top 1: 0.8918558677289882\n",
      "Total time: 21.259986809889476 in minutes\n",
      "Starting epoch: 5\n",
      "[Iter 300] Loss: 0.01854378916323185 Top 1: 0.8924394319131161\n",
      "[Iter 600] Loss: 0.018243562430143356 Top 1: 0.893825615352524\n",
      "[Iter 900] Loss: 0.018069153651595116 Top 1: 0.8951209341117597\n",
      "[Iter 1200] Loss: 0.018673084676265717 Top 1: 0.892901813633521\n",
      "[Iter 1500] Loss: 0.018649760633707047 Top 1: 0.8929047857261965\n",
      "[Iter 1800] Loss: 0.018529340624809265 Top 1: 0.893636237321106\n",
      "[Iter 2100] Loss: 0.018560266122221947 Top 1: 0.8927593188043349\n",
      "[Iter 2400] Loss: 0.01858527772128582 Top 1: 0.8924663957486715\n",
      "[Iter 2700] Loss: 0.018566463142633438 Top 1: 0.8929332221913494\n",
      "[Iter 3000] Loss: 0.01855178363621235 Top 1: 0.8935358839709927\n",
      "[Iter 3300] Loss: 0.018517138436436653 Top 1: 0.8938016215806622\n",
      "[Iter 3600] Loss: 0.01858164742588997 Top 1: 0.8933805653955685\n",
      "[Iter 3900] Loss: 0.018555479124188423 Top 1: 0.8940180804000769\n",
      "Loss: 0.018530860543251038 Top 1: 0.894207152698736\n",
      "Total time: 21.011881053447723 in minutes\n",
      "Starting epoch: 6\n",
      "[Iter 300] Loss: 0.017992965877056122 Top 1: 0.8972431077694235\n",
      "[Iter 600] Loss: 0.018066147342324257 Top 1: 0.8963287442636629\n",
      "[Iter 900] Loss: 0.018558766692876816 Top 1: 0.8938698915763136\n",
      "[Iter 1200] Loss: 0.01857195422053337 Top 1: 0.8939962476547842\n",
      "[Iter 1500] Loss: 0.018576085567474365 Top 1: 0.8939052859763215\n",
      "[Iter 1800] Loss: 0.01855408027768135 Top 1: 0.8938099208003335\n",
      "[Iter 2100] Loss: 0.018580857664346695 Top 1: 0.8938013576277242\n",
      "[Iter 2400] Loss: 0.018506277352571487 Top 1: 0.8946545795561113\n",
      "[Iter 2700] Loss: 0.018422627821564674 Top 1: 0.895063443549134\n",
      "[Iter 3000] Loss: 0.01838095113635063 Top 1: 0.8951404517796115\n",
      "[Iter 3300] Loss: 0.018240030854940414 Top 1: 0.8960748654997348\n",
      "[Iter 3600] Loss: 0.018272195011377335 Top 1: 0.8960894630825866\n",
      "[Iter 3900] Loss: 0.018185989931225777 Top 1: 0.8966307623260884\n",
      "Loss: 0.018151994794607162 Top 1: 0.8970526568022524\n",
      "Total time: 21.06063979069392 in minutes\n",
      "Starting epoch: 7\n",
      "[Iter 300] Loss: 0.0171133354306221 Top 1: 0.9051796157059315\n",
      "[Iter 600] Loss: 0.018334511667490005 Top 1: 0.8982060909470171\n",
      "[Iter 900] Loss: 0.018307434394955635 Top 1: 0.8960939671948847\n",
      "[Iter 1200] Loss: 0.01806742511689663 Top 1: 0.8979049405878674\n",
      "[Iter 1500] Loss: 0.018183890730142593 Top 1: 0.8969067867266967\n",
      "[Iter 1800] Loss: 0.018143747001886368 Top 1: 0.8969709601222732\n",
      "[Iter 2100] Loss: 0.0180381927639246 Top 1: 0.8973442896272479\n",
      "[Iter 2400] Loss: 0.01807822659611702 Top 1: 0.897389809315411\n",
      "[Iter 2700] Loss: 0.017891298979520798 Top 1: 0.8984440122256182\n",
      "[Iter 3000] Loss: 0.01797812059521675 Top 1: 0.8981828790530966\n",
      "[Iter 3300] Loss: 0.017919490113854408 Top 1: 0.8986890960066681\n",
      "[Iter 3600] Loss: 0.017910577356815338 Top 1: 0.8989372785997083\n",
      "[Iter 3900] Loss: 0.017866255715489388 Top 1: 0.899147271911265\n",
      "Loss: 0.01782948523759842 Top 1: 0.8994189181093871\n",
      "Total time: 21.026940337816875 in minutes\n",
      "Starting epoch: 8\n",
      "[Iter 300] Loss: 0.01773686707019806 Top 1: 0.8947368421052632\n",
      "[Iter 600] Loss: 0.018122538924217224 Top 1: 0.893825615352524\n",
      "[Iter 900] Loss: 0.01801074855029583 Top 1: 0.8960244648318043\n",
      "[Iter 1200] Loss: 0.018162311986088753 Top 1: 0.8960287679799875\n",
      "[Iter 1500] Loss: 0.01817532815039158 Top 1: 0.8955727863931966\n",
      "[Iter 1800] Loss: 0.018096288666129112 Top 1: 0.8961025427261359\n",
      "[Iter 2100] Loss: 0.017936618998646736 Top 1: 0.8967786114088365\n",
      "[Iter 2400] Loss: 0.017875883728265762 Top 1: 0.8970511618214025\n",
      "[Iter 2700] Loss: 0.017897747457027435 Top 1: 0.897077892007039\n",
      "[Iter 3000] Loss: 0.017852287739515305 Top 1: 0.897724431107777\n",
      "[Iter 3300] Loss: 0.017807699739933014 Top 1: 0.8980260665302721\n",
      "[Iter 3600] Loss: 0.017842035740613937 Top 1: 0.8981732305341391\n",
      "[Iter 3900] Loss: 0.0178693700581789 Top 1: 0.8979611463743028\n",
      "Loss: 0.017849575728178024 Top 1: 0.8984304798418499\n",
      "Total time: 21.00902140935262 in minutes\n",
      "Starting epoch: 9\n",
      "[Iter 300] Loss: 0.017703209072351456 Top 1: 0.898078529657477\n",
      "[Iter 600] Loss: 0.018294882029294968 Top 1: 0.8965373383395911\n",
      "[Iter 900] Loss: 0.017931947484612465 Top 1: 0.8988740617180984\n",
      "[Iter 1200] Loss: 0.01770007237792015 Top 1: 0.9007191994996873\n",
      "[Iter 1500] Loss: 0.017800984904170036 Top 1: 0.9001167250291813\n",
      "[Iter 1800] Loss: 0.017888657748699188 Top 1: 0.9000972627483673\n",
      "[Iter 2100] Loss: 0.017906133085489273 Top 1: 0.8991008693581041\n",
      "[Iter 2400] Loss: 0.017850445583462715 Top 1: 0.8987183494842138\n",
      "[Iter 2700] Loss: 0.017800044268369675 Top 1: 0.899370195424655\n",
      "[Iter 3000] Loss: 0.01781865395605564 Top 1: 0.8999958322914062\n",
      "[Iter 3300] Loss: 0.017807479947805405 Top 1: 0.8998636053648557\n",
      "[Iter 3600] Loss: 0.017818452790379524 Top 1: 0.899545044106411\n",
      "[Iter 3900] Loss: 0.0178215354681015 Top 1: 0.8995800474450215\n",
      "Loss: 0.01777086779475212 Top 1: 0.8997034685197388\n",
      "Total time: 21.14742937485377 in minutes\n",
      "Starting epoch: 10\n",
      "[Iter 300] Loss: 0.01737145334482193 Top 1: 0.9024644945697577\n",
      "[Iter 600] Loss: 0.017607348039746284 Top 1: 0.9006049228201919\n",
      "[Iter 900] Loss: 0.01764717698097229 Top 1: 0.8998470948012233\n",
      "[Iter 1200] Loss: 0.017639948055148125 Top 1: 0.8995205336668751\n",
      "[Iter 1500] Loss: 0.01770734041929245 Top 1: 0.9003251625812907\n",
      "[Iter 1800] Loss: 0.017599595710635185 Top 1: 0.901625677365569\n",
      "[Iter 2100] Loss: 0.01745961233973503 Top 1: 0.9016315350720495\n",
      "[Iter 2400] Loss: 0.017679551616311073 Top 1: 0.900619985412108\n",
      "[Iter 2700] Loss: 0.017795458436012268 Top 1: 0.8999490599240529\n",
      "[Iter 3000] Loss: 0.017709629610180855 Top 1: 0.8998916395765608\n",
      "[Iter 3300] Loss: 0.017627056688070297 Top 1: 0.900280366750019\n",
      "[Iter 3600] Loss: 0.01760600507259369 Top 1: 0.9004306452733208\n",
      "[Iter 3900] Loss: 0.01756490394473076 Top 1: 0.9005898570237866\n",
      "Loss: 0.017607849091291428 Top 1: 0.9003324746899898\n",
      "Total time: 21.18564743200938 in minutes\n",
      "Starting epoch: 11\n",
      "[Iter 300] Loss: 0.01895798183977604 Top 1: 0.8987050960735171\n",
      "[Iter 600] Loss: 0.018864884972572327 Top 1: 0.8960158531497705\n",
      "[Iter 900] Loss: 0.017966149374842644 Top 1: 0.8992910758965805\n",
      "[Iter 1200] Loss: 0.017863979563117027 Top 1: 0.9007191994996873\n",
      "[Iter 1500] Loss: 0.01788141205906868 Top 1: 0.9006169751542438\n",
      "[Iter 1800] Loss: 0.017540201544761658 Top 1: 0.9026330415450883\n",
      "[Iter 2100] Loss: 0.017482131719589233 Top 1: 0.9029712992735501\n",
      "[Iter 2400] Loss: 0.017408467829227448 Top 1: 0.902886318641242\n",
      "[Iter 2700] Loss: 0.01742386445403099 Top 1: 0.902218208761693\n",
      "[Iter 3000] Loss: 0.01754779927432537 Top 1: 0.901871301158623\n",
      "[Iter 3300] Loss: 0.01754644513130188 Top 1: 0.9021179055846026\n",
      "[Iter 3600] Loss: 0.017440857365727425 Top 1: 0.902583871639925\n",
      "[Iter 3900] Loss: 0.017407841980457306 Top 1: 0.902545361287427\n",
      "Loss: 0.017388788983225822 Top 1: 0.903028215419637\n",
      "Total time: 21.142147441705067 in minutes\n",
      "Starting epoch: 12\n",
      "[Iter 300] Loss: 0.01716935634613037 Top 1: 0.9062238930659984\n",
      "[Iter 600] Loss: 0.01759922318160534 Top 1: 0.9025865665415103\n",
      "[Iter 900] Loss: 0.01766500063240528 Top 1: 0.9011676396997498\n",
      "[Iter 1200] Loss: 0.01770171709358692 Top 1: 0.9006149676881384\n",
      "[Iter 1500] Loss: 0.017578672617673874 Top 1: 0.9009921627480407\n",
      "[Iter 1800] Loss: 0.017698826268315315 Top 1: 0.9001319994442128\n",
      "[Iter 2100] Loss: 0.017575712874531746 Top 1: 0.9009467667023937\n",
      "[Iter 2400] Loss: 0.017658468335866928 Top 1: 0.900984682713348\n",
      "[Iter 2700] Loss: 0.017697477713227272 Top 1: 0.9002037603037881\n",
      "[Iter 3000] Loss: 0.01756712608039379 Top 1: 0.9011836292406435\n",
      "[Iter 3300] Loss: 0.01751178689301014 Top 1: 0.9012464954156247\n",
      "[Iter 3600] Loss: 0.01745864935219288 Top 1: 0.9017156352017781\n",
      "[Iter 3900] Loss: 0.017505193129181862 Top 1: 0.901471436814772\n",
      "Loss: 0.017526403069496155 Top 1: 0.9013808183070748\n",
      "Total time: 21.063737424214683 in minutes\n",
      "Starting epoch: 13\n",
      "[Iter 300] Loss: 0.0180426724255085 Top 1: 0.8966165413533834\n",
      "[Iter 600] Loss: 0.017734913155436516 Top 1: 0.8999791405924071\n",
      "[Iter 900] Loss: 0.017709819599986076 Top 1: 0.9017931609674729\n",
      "[Iter 1200] Loss: 0.017436590045690536 Top 1: 0.9006670835939129\n",
      "[Iter 1500] Loss: 0.01738291233778 Top 1: 0.9013256628314157\n",
      "[Iter 1800] Loss: 0.017455104738473892 Top 1: 0.9011741003195776\n",
      "[Iter 2100] Loss: 0.017465826123952866 Top 1: 0.9013338096939383\n",
      "[Iter 2400] Loss: 0.017502671107649803 Top 1: 0.9011930811711993\n",
      "[Iter 2700] Loss: 0.01744805835187435 Top 1: 0.9021255904417894\n",
      "[Iter 3000] Loss: 0.017358968034386635 Top 1: 0.9026214887055097\n",
      "[Iter 3300] Loss: 0.017495885491371155 Top 1: 0.9021368492839281\n",
      "[Iter 3600] Loss: 0.01745615154504776 Top 1: 0.9022713065221921\n",
      "[Iter 3900] Loss: 0.01750159077346325 Top 1: 0.902160671924088\n",
      "Loss: 0.017489749938249588 Top 1: 0.9024291619241598\n",
      "Total time: 21.03524250984192 in minutes\n",
      "Starting epoch: 14\n",
      "[Iter 300] Loss: 0.017636947333812714 Top 1: 0.9001670843776107\n",
      "[Iter 600] Loss: 0.017335543408989906 Top 1: 0.9033166458072591\n",
      "[Iter 900] Loss: 0.01739911176264286 Top 1: 0.9027661940505978\n",
      "[Iter 1200] Loss: 0.017461374402046204 Top 1: 0.9026474880133417\n",
      "[Iter 1500] Loss: 0.017662925645709038 Top 1: 0.9017425379356345\n",
      "[Iter 1800] Loss: 0.017429636791348457 Top 1: 0.9022509378907878\n",
      "[Iter 2100] Loss: 0.017329728230834007 Top 1: 0.9031201619626057\n",
      "[Iter 2400] Loss: 0.017517903819680214 Top 1: 0.9026779201833907\n",
      "[Iter 2700] Loss: 0.017611196264624596 Top 1: 0.9019866629619339\n",
      "[Iter 3000] Loss: 0.017489630728960037 Top 1: 0.9026631657914479\n",
      "[Iter 3300] Loss: 0.01748393103480339 Top 1: 0.9031219216488596\n",
      "[Iter 3600] Loss: 0.017511367797851562 Top 1: 0.9027575189275544\n",
      "[Iter 3900] Loss: 0.01749199442565441 Top 1: 0.903026222991601\n",
      "Loss: 0.017548440024256706 Top 1: 0.9025489726232553\n",
      "Total time: 21.12393933137258 in minutes\n",
      "Starting epoch: 15\n",
      "[Iter 300] Loss: 0.016787579283118248 Top 1: 0.9091478696741855\n",
      "[Iter 600] Loss: 0.017369037494063377 Top 1: 0.9032123487692949\n",
      "[Iter 900] Loss: 0.017471637576818466 Top 1: 0.9041562413122046\n",
      "[Iter 1200] Loss: 0.017410479485988617 Top 1: 0.9025953721075672\n",
      "[Iter 1500] Loss: 0.017582068219780922 Top 1: 0.9022844755711189\n",
      "[Iter 1800] Loss: 0.01741807535290718 Top 1: 0.903327775461998\n",
      "[Iter 2100] Loss: 0.017401758581399918 Top 1: 0.9029117541979278\n",
      "[Iter 2400] Loss: 0.017522115260362625 Top 1: 0.9026258205689278\n",
      "[Iter 2700] Loss: 0.017490796744823456 Top 1: 0.9027276095211633\n",
      "[Iter 3000] Loss: 0.01750633306801319 Top 1: 0.9022255563890973\n",
      "[Iter 3300] Loss: 0.017469918355345726 Top 1: 0.9018337500947184\n",
      "[Iter 3600] Loss: 0.01744898408651352 Top 1: 0.9016809057442523\n",
      "[Iter 3900] Loss: 0.017429832369089127 Top 1: 0.9020003846893634\n",
      "Loss: 0.017412936314940453 Top 1: 0.9020547534894866\n",
      "Total time: 21.02926428715388 in minutes\n",
      "Starting epoch: 16\n",
      "[Iter 300] Loss: 0.016722366213798523 Top 1: 0.9049707602339181\n",
      "[Iter 600] Loss: 0.017339399084448814 Top 1: 0.9013350020859408\n",
      "[Iter 900] Loss: 0.017512518912553787 Top 1: 0.9008201278843481\n",
      "[Iter 1200] Loss: 0.017511889338493347 Top 1: 0.9012403585574317\n",
      "[Iter 1500] Loss: 0.017609210684895515 Top 1: 0.9016591629147908\n",
      "[Iter 1800] Loss: 0.017538992688059807 Top 1: 0.9020077810198693\n",
      "[Iter 2100] Loss: 0.017526637762784958 Top 1: 0.9022865309038942\n",
      "[Iter 2400] Loss: 0.0174152459949255 Top 1: 0.9022611232676878\n",
      "[Iter 2700] Loss: 0.017440639436244965 Top 1: 0.9023339816615726\n",
      "[Iter 3000] Loss: 0.01736324466764927 Top 1: 0.9025381345336334\n",
      "[Iter 3300] Loss: 0.017345497384667397 Top 1: 0.9029514283549291\n",
      "[Iter 3600] Loss: 0.01737789809703827 Top 1: 0.9028269778426061\n",
      "[Iter 3900] Loss: 0.017412811517715454 Top 1: 0.9028178495864589\n",
      "Loss: 0.01754123531281948 Top 1: 0.9023243275624513\n",
      "Total time: 21.15528505643209 in minutes\n",
      "Starting epoch: 17\n",
      "[Iter 300] Loss: 0.017226384952664375 Top 1: 0.9035087719298246\n",
      "[Iter 600] Loss: 0.017007190734148026 Top 1: 0.9049853984146851\n",
      "[Iter 900] Loss: 0.01744050718843937 Top 1: 0.9026966916875174\n",
      "[Iter 1200] Loss: 0.017172357067465782 Top 1: 0.9040546174692516\n",
      "[Iter 1500] Loss: 0.01722426526248455 Top 1: 0.904285476071369\n",
      "[Iter 1800] Loss: 0.017201192677021027 Top 1: 0.9045088231207448\n",
      "[Iter 2100] Loss: 0.01714891940355301 Top 1: 0.9049958318447064\n",
      "[Iter 2400] Loss: 0.017105044797062874 Top 1: 0.9054131499426904\n",
      "[Iter 2700] Loss: 0.01721031777560711 Top 1: 0.9044178938594054\n",
      "[Iter 3000] Loss: 0.017214059829711914 Top 1: 0.9042468950570977\n",
      "[Iter 3300] Loss: 0.017224857583642006 Top 1: 0.9037849511252557\n",
      "[Iter 3600] Loss: 0.01724165491759777 Top 1: 0.9035215669931236\n",
      "[Iter 3900] Loss: 0.017198139801621437 Top 1: 0.904068090017311\n",
      "Loss: 0.017215479165315628 Top 1: 0.9040915353741089\n",
      "Total time: 21.21030060052872 in minutes\n",
      "Starting epoch: 18\n",
      "[Iter 300] Loss: 0.01812121830880642 Top 1: 0.900375939849624\n",
      "[Iter 600] Loss: 0.017347028478980064 Top 1: 0.904881101376721\n",
      "[Iter 900] Loss: 0.017149819061160088 Top 1: 0.9058242980261328\n",
      "[Iter 1200] Loss: 0.01735844835639 Top 1: 0.9033249947884094\n",
      "[Iter 1500] Loss: 0.01729452796280384 Top 1: 0.9038269134567284\n",
      "[Iter 1800] Loss: 0.01745777390897274 Top 1: 0.9029804085035431\n",
      "[Iter 2100] Loss: 0.017303241416811943 Top 1: 0.9032690246516613\n",
      "[Iter 2400] Loss: 0.017247725278139114 Top 1: 0.9028602688340106\n",
      "[Iter 2700] Loss: 0.017553886398673058 Top 1: 0.9020098175419098\n",
      "[Iter 3000] Loss: 0.01760459691286087 Top 1: 0.9019963324164374\n",
      "[Iter 3300] Loss: 0.01759812980890274 Top 1: 0.9022315677805561\n",
      "[Iter 3600] Loss: 0.017546016722917557 Top 1: 0.9026186010974508\n",
      "[Iter 3900] Loss: 0.017515847459435463 Top 1: 0.9030743091620184\n",
      "Loss: 0.017520058900117874 Top 1: 0.9031929551308931\n",
      "Total time: 21.192779898643494 in minutes\n",
      "Starting epoch: 19\n",
      "[Iter 300] Loss: 0.018083268776535988 Top 1: 0.8989139515455304\n",
      "[Iter 600] Loss: 0.017503540962934494 Top 1: 0.9021693783896537\n",
      "[Iter 900] Loss: 0.017198480665683746 Top 1: 0.9047122602168474\n",
      "[Iter 1200] Loss: 0.017478812485933304 Top 1: 0.9035334584115072\n",
      "[Iter 1500] Loss: 0.017438823357224464 Top 1: 0.9043688510922128\n",
      "[Iter 1800] Loss: 0.017220212146639824 Top 1: 0.9052382937335001\n",
      "[Iter 2100] Loss: 0.01729356124997139 Top 1: 0.9046683339287841\n",
      "[Iter 2400] Loss: 0.017364922910928726 Top 1: 0.9041888090028134\n",
      "[Iter 2700] Loss: 0.017561187967658043 Top 1: 0.9034222469204408\n",
      "[Iter 3000] Loss: 0.01762678660452366 Top 1: 0.9029549053930149\n",
      "[Iter 3300] Loss: 0.01759999431669712 Top 1: 0.9030272031522316\n",
      "[Iter 3600] Loss: 0.01755826734006405 Top 1: 0.9033131902479683\n",
      "[Iter 3900] Loss: 0.017506515607237816 Top 1: 0.9032506251202155\n",
      "Loss: 0.01758681610226631 Top 1: 0.9029084047205416\n",
      "Total time: 21.202433415253957 in minutes\n",
      "Starting epoch: 20\n",
      "[Iter 300] Loss: 0.017648670822381973 Top 1: 0.9045530492898914\n",
      "[Iter 600] Loss: 0.017248837277293205 Top 1: 0.9057154776804339\n",
      "[Iter 900] Loss: 0.01723313331604004 Top 1: 0.9034612176814012\n",
      "[Iter 1200] Loss: 0.016804302111268044 Top 1: 0.905513862830936\n",
      "[Iter 1500] Loss: 0.01717894896864891 Top 1: 0.9040353510088378\n",
      "[Iter 1800] Loss: 0.01705304905772209 Top 1: 0.9046477699041268\n",
      "[Iter 2100] Loss: 0.017170580103993416 Top 1: 0.9040728831725616\n",
      "[Iter 2400] Loss: 0.017123190686106682 Top 1: 0.9044493070751276\n",
      "[Iter 2700] Loss: 0.017096545547246933 Top 1: 0.9047420579790683\n",
      "[Iter 3000] Loss: 0.01726705953478813 Top 1: 0.9038718012836543\n",
      "[Iter 3300] Loss: 0.017327072098851204 Top 1: 0.9040880503144654\n",
      "[Iter 3600] Loss: 0.0172598697245121 Top 1: 0.9046676390914774\n",
      "[Iter 3900] Loss: 0.017447486519813538 Top 1: 0.9037154581009168\n",
      "Loss: 0.017432188615202904 Top 1: 0.9037770322889834\n",
      "Total time: 21.258918134371438 in minutes\n",
      "Starting epoch: 21\n",
      "[Iter 300] Loss: 0.017318328842520714 Top 1: 0.9072681704260651\n",
      "[Iter 600] Loss: 0.017055556178092957 Top 1: 0.910095953274927\n",
      "[Iter 900] Loss: 0.01760992407798767 Top 1: 0.9083263830970253\n",
      "[Iter 1200] Loss: 0.01768667623400688 Top 1: 0.9060871377944548\n",
      "[Iter 1500] Loss: 0.017464477568864822 Top 1: 0.9061197265299317\n",
      "[Iter 1800] Loss: 0.017496760934591293 Top 1: 0.9056203973878004\n",
      "[Iter 2100] Loss: 0.017483772709965706 Top 1: 0.9051149219959509\n",
      "[Iter 2400] Loss: 0.01745077781379223 Top 1: 0.9056996978222361\n",
      "[Iter 2700] Loss: 0.01720941998064518 Top 1: 0.9070343613966843\n",
      "[Iter 3000] Loss: 0.017154846340417862 Top 1: 0.9072684837876136\n",
      "[Iter 3300] Loss: 0.017297105863690376 Top 1: 0.906361294233538\n",
      "[Iter 3600] Loss: 0.0174116063863039 Top 1: 0.9060047232062235\n",
      "[Iter 3900] Loss: 0.017437836155295372 Top 1: 0.9054786176828877\n",
      "Loss: 0.017441267147660255 Top 1: 0.9053944767267716\n",
      "Total time: 21.23394855658213 in minutes\n",
      "Starting epoch: 22\n",
      "[Iter 300] Loss: 0.01811997778713703 Top 1: 0.9032999164578112\n",
      "[Iter 600] Loss: 0.017896248027682304 Top 1: 0.9052982895285774\n",
      "[Iter 900] Loss: 0.018139613792300224 Top 1: 0.9029747011398388\n",
      "[Iter 1200] Loss: 0.01799364574253559 Top 1: 0.9041588492808005\n",
      "[Iter 1500] Loss: 0.017845699563622475 Top 1: 0.9042437885609471\n",
      "[Iter 1800] Loss: 0.017796268686652184 Top 1: 0.9041267194664444\n",
      "[Iter 2100] Loss: 0.017771154642105103 Top 1: 0.9043706085506729\n",
      "[Iter 2400] Loss: 0.017743632197380066 Top 1: 0.9039804105449619\n",
      "[Iter 2700] Loss: 0.017578724771738052 Top 1: 0.9044642030193573\n",
      "[Iter 3000] Loss: 0.017640918493270874 Top 1: 0.9041218637992832\n",
      "[Iter 3300] Loss: 0.017717938870191574 Top 1: 0.9037849511252557\n",
      "[Iter 3600] Loss: 0.017711754888296127 Top 1: 0.9034694728068348\n",
      "[Iter 3900] Loss: 0.01776687614619732 Top 1: 0.9034750272488299\n",
      "Loss: 0.017741363495588303 Top 1: 0.9034924818786317\n",
      "Total time: 21.265747074286143 in minutes\n",
      "Starting epoch: 23\n",
      "[Iter 300] Loss: 0.0165697168558836 Top 1: 0.9026733500417711\n",
      "[Iter 600] Loss: 0.017066245898604393 Top 1: 0.9037338339591156\n",
      "[Iter 900] Loss: 0.017385756596922874 Top 1: 0.9031137058659995\n",
      "[Iter 1200] Loss: 0.017656411975622177 Top 1: 0.9020742130498228\n",
      "[Iter 1500] Loss: 0.017530178651213646 Top 1: 0.9027847256961814\n",
      "[Iter 1800] Loss: 0.01746535673737526 Top 1: 0.903327775461998\n",
      "[Iter 2100] Loss: 0.01735101267695427 Top 1: 0.9038347028700726\n",
      "[Iter 2400] Loss: 0.017416462302207947 Top 1: 0.9033552151714077\n",
      "[Iter 2700] Loss: 0.01731015555560589 Top 1: 0.9040937297397426\n",
      "[Iter 3000] Loss: 0.017343508079648018 Top 1: 0.9040176710844378\n",
      "[Iter 3300] Loss: 0.017421897500753403 Top 1: 0.9035955141319997\n",
      "[Iter 3600] Loss: 0.017515279352664948 Top 1: 0.903122178231576\n",
      "[Iter 3900] Loss: 0.01748952642083168 Top 1: 0.9035711995896647\n",
      "Loss: 0.017548231407999992 Top 1: 0.9034775055412448\n",
      "Total time: 21.28390052715937 in minutes\n",
      "Starting epoch: 24\n",
      "[Iter 300] Loss: 0.017538828775286674 Top 1: 0.9012113617376776\n",
      "[Iter 600] Loss: 0.016888326033949852 Top 1: 0.9044639132248644\n",
      "[Iter 900] Loss: 0.017161455005407333 Top 1: 0.9051987767584098\n",
      "[Iter 1200] Loss: 0.017368949949741364 Top 1: 0.9036898061288305\n",
      "[Iter 1500] Loss: 0.01723172329366207 Top 1: 0.9053693513423379\n",
      "[Iter 1800] Loss: 0.01733343116939068 Top 1: 0.9052035570376545\n",
      "[Iter 2100] Loss: 0.017364583909511566 Top 1: 0.9052637846850066\n",
      "[Iter 2400] Loss: 0.01743371970951557 Top 1: 0.9050745024486819\n",
      "[Iter 2700] Loss: 0.017412101849913597 Top 1: 0.9050662220987311\n",
      "[Iter 3000] Loss: 0.017399000003933907 Top 1: 0.9051221138617987\n",
      "[Iter 3300] Loss: 0.01747727394104004 Top 1: 0.9050731226793969\n",
      "[Iter 3600] Loss: 0.017435597255825996 Top 1: 0.9054837813433354\n",
      "[Iter 3900] Loss: 0.017480462789535522 Top 1: 0.9054786176828877\n",
      "Loss: 0.0174056738615036 Top 1: 0.9056640507997364\n",
      "Total time: 21.587461451689403 in minutes\n",
      "Starting epoch: 25\n",
      "[Iter 300] Loss: 0.018458057194948196 Top 1: 0.8978696741854637\n",
      "[Iter 600] Loss: 0.018271416425704956 Top 1: 0.8970588235294118\n",
      "[Iter 900] Loss: 0.018011609092354774 Top 1: 0.8992910758965805\n",
      "[Iter 1200] Loss: 0.01811482198536396 Top 1: 0.8993120700437773\n",
      "[Iter 1500] Loss: 0.01804596744477749 Top 1: 0.900908787727197\n",
      "[Iter 1800] Loss: 0.017700163647532463 Top 1: 0.9022509378907878\n",
      "[Iter 2100] Loss: 0.01783626340329647 Top 1: 0.9022865309038942\n",
      "[Iter 2400] Loss: 0.017934774979948997 Top 1: 0.9021048244242993\n",
      "[Iter 2700] Loss: 0.017803510650992393 Top 1: 0.9021950541817172\n",
      "[Iter 3000] Loss: 0.01773071102797985 Top 1: 0.9028298741352004\n",
      "[Iter 3300] Loss: 0.017669497057795525 Top 1: 0.9031219216488596\n",
      "[Iter 3600] Loss: 0.017721369862556458 Top 1: 0.9028617073001319\n",
      "[Iter 3900] Loss: 0.017791245132684708 Top 1: 0.9027697634160415\n",
      "Loss: 0.017702654004096985 Top 1: 0.9031779787935063\n",
      "Total time: 21.283907723426818 in minutes\n",
      "Starting epoch: 26\n",
      "[Iter 300] Loss: 0.015729786828160286 Top 1: 0.9149958228905597\n",
      "[Iter 600] Loss: 0.016737524420022964 Top 1: 0.9108260325406758\n",
      "[Iter 900] Loss: 0.016612835228443146 Top 1: 0.9103419516263553\n",
      "[Iter 1200] Loss: 0.016880659386515617 Top 1: 0.9075984990619137\n",
      "[Iter 1500] Loss: 0.017138062044978142 Top 1: 0.9054110388527598\n",
      "[Iter 1800] Loss: 0.017206290736794472 Top 1: 0.9053425038210365\n",
      "[Iter 2100] Loss: 0.017205286771059036 Top 1: 0.9054126473740621\n",
      "[Iter 2400] Loss: 0.017318613827228546 Top 1: 0.9056215484005419\n",
      "[Iter 2700] Loss: 0.017445936799049377 Top 1: 0.9048578308789479\n",
      "[Iter 3000] Loss: 0.017573049291968346 Top 1: 0.9045386346586647\n",
      "[Iter 3300] Loss: 0.017666786909103394 Top 1: 0.9040501629158142\n",
      "[Iter 3600] Loss: 0.017614100128412247 Top 1: 0.9044071681600333\n",
      "[Iter 3900] Loss: 0.017531240358948708 Top 1: 0.9049496698082965\n",
      "Loss: 0.01753220520913601 Top 1: 0.905139878991194\n",
      "Total time: 21.271589465936025 in minutes\n",
      "Starting epoch: 27\n",
      "[Iter 300] Loss: 0.018103323876857758 Top 1: 0.9001670843776107\n",
      "[Iter 600] Loss: 0.01838669367134571 Top 1: 0.9000834376303714\n",
      "[Iter 900] Loss: 0.0185713954269886 Top 1: 0.9008896302474284\n",
      "[Iter 1200] Loss: 0.01843896508216858 Top 1: 0.9012924744632062\n",
      "[Iter 1500] Loss: 0.018080947920680046 Top 1: 0.9035351008837752\n",
      "[Iter 1800] Loss: 0.01789780892431736 Top 1: 0.9033625121578436\n",
      "[Iter 2100] Loss: 0.017820319160819054 Top 1: 0.9040431106347505\n",
      "[Iter 2400] Loss: 0.01778789609670639 Top 1: 0.9039543607377305\n",
      "[Iter 2700] Loss: 0.017702272161841393 Top 1: 0.9047652125590442\n",
      "[Iter 3000] Loss: 0.017647426575422287 Top 1: 0.9046428273735101\n",
      "[Iter 3300] Loss: 0.017701277509331703 Top 1: 0.9041827688110934\n",
      "[Iter 3600] Loss: 0.017657915130257607 Top 1: 0.9042335208724039\n",
      "[Iter 3900] Loss: 0.01752881146967411 Top 1: 0.9047893825735719\n",
      "Loss: 0.017605260014533997 Top 1: 0.9047355178817469\n",
      "Total time: 21.185919654369354 in minutes\n",
      "Starting epoch: 28\n",
      "[Iter 300] Loss: 0.018162285909056664 Top 1: 0.9026733500417711\n",
      "[Iter 600] Loss: 0.01802695542573929 Top 1: 0.9043596161869003\n",
      "[Iter 900] Loss: 0.01789655163884163 Top 1: 0.9039477342229636\n",
      "[Iter 1200] Loss: 0.017985619604587555 Top 1: 0.9035334584115072\n",
      "[Iter 1500] Loss: 0.017824074253439903 Top 1: 0.9034100383525095\n",
      "[Iter 1800] Loss: 0.017992807552218437 Top 1: 0.9024246213700153\n",
      "[Iter 2100] Loss: 0.017822857946157455 Top 1: 0.9028819816601167\n",
      "[Iter 2400] Loss: 0.017837153747677803 Top 1: 0.9027300197978535\n",
      "[Iter 2700] Loss: 0.01790560968220234 Top 1: 0.9026118366212836\n",
      "[Iter 3000] Loss: 0.017912527546286583 Top 1: 0.9025172959906643\n",
      "[Iter 3300] Loss: 0.0179266557097435 Top 1: 0.9018905811926953\n",
      "[Iter 3600] Loss: 0.0178252924233675 Top 1: 0.9024623185385844\n",
      "[Iter 3900] Loss: 0.01787092722952366 Top 1: 0.9028018208629864\n",
      "Loss: 0.017854198813438416 Top 1: 0.9032977894926016\n",
      "Total time: 21.221434446175895 in minutes\n",
      "Starting epoch: 29\n",
      "[Iter 300] Loss: 0.01835019141435623 Top 1: 0.9028822055137845\n",
      "[Iter 600] Loss: 0.017789946869015694 Top 1: 0.9061326658322904\n",
      "[Iter 900] Loss: 0.017638208344578743 Top 1: 0.9055462885738115\n",
      "[Iter 1200] Loss: 0.01774749532341957 Top 1: 0.9052532833020638\n",
      "[Iter 1500] Loss: 0.01773761585354805 Top 1: 0.9044522261130565\n",
      "[Iter 1800] Loss: 0.017672166228294373 Top 1: 0.9049604001667362\n",
      "[Iter 2100] Loss: 0.017794501036405563 Top 1: 0.9046683339287841\n",
      "[Iter 2400] Loss: 0.01785050891339779 Top 1: 0.9037720120871106\n",
      "[Iter 2700] Loss: 0.017805814743041992 Top 1: 0.9040242659998148\n",
      "[Iter 3000] Loss: 0.017825331538915634 Top 1: 0.9036009002250562\n",
      "[Iter 3300] Loss: 0.0177830271422863 Top 1: 0.904296431007047\n",
      "[Iter 3600] Loss: 0.017768554389476776 Top 1: 0.9045981801764257\n",
      "[Iter 3900] Loss: 0.017767541110515594 Top 1: 0.9043405783163428\n",
      "Loss: 0.017771385610103607 Top 1: 0.9043161804349128\n",
      "Total time: 21.28878031174342 in minutes\n",
      "Starting epoch: 30\n",
      "[Iter 300] Loss: 0.019038120284676552 Top 1: 0.8959899749373433\n",
      "[Iter 600] Loss: 0.017952246591448784 Top 1: 0.9002920317062996\n",
      "[Iter 900] Loss: 0.017353827133774757 Top 1: 0.904642757853767\n",
      "[Iter 1200] Loss: 0.01720944605767727 Top 1: 0.9051490514905149\n",
      "[Iter 1500] Loss: 0.017365407198667526 Top 1: 0.9048274137068534\n",
      "[Iter 1800] Loss: 0.017418261617422104 Top 1: 0.9050646102542727\n",
      "[Iter 2100] Loss: 0.017478274181485176 Top 1: 0.9049660593068953\n",
      "[Iter 2400] Loss: 0.017754122614860535 Top 1: 0.9045014066895904\n",
      "[Iter 2700] Loss: 0.017789924517273903 Top 1: 0.9042789663795499\n",
      "[Iter 3000] Loss: 0.01776348240673542 Top 1: 0.9043302492289739\n",
      "[Iter 3300] Loss: 0.017858296632766724 Top 1: 0.9040691066151398\n",
      "[Iter 3600] Loss: 0.01776709221303463 Top 1: 0.9041640619573522\n",
      "[Iter 3900] Loss: 0.01768418960273266 Top 1: 0.90451689427454\n",
      "Loss: 0.01764793135225773 Top 1: 0.9045707781704906\n",
      "Total time: 21.33459579149882 in minutes\n"
     ]
    }
   ],
   "source": [
    "run_model(model_r50_10, train_generator, epochs=30, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a46409e-4d9a-4a68-a084-314d63a32d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 1\n",
      "{'final_avg_pool': <tf.Tensor 'model/StatefulPartitionedCall:4' shape=(16, 4096) dtype=float32>, 'initial_conv': <tf.Tensor 'model/StatefulPartitionedCall:5' shape=(16, 128, 128, 128) dtype=float32>, 'initial_max_pool': <tf.Tensor 'model/StatefulPartitionedCall:6' shape=(16, 64, 64, 128) dtype=float32>, 'block_group2': <tf.Tensor 'model/StatefulPartitionedCall:1' shape=(16, 32, 32, 1024) dtype=float32>, 'block_group1': <tf.Tensor 'model/StatefulPartitionedCall:0' shape=(16, 64, 64, 512) dtype=float32>, 'block_group3': <tf.Tensor 'model/StatefulPartitionedCall:2' shape=(16, 16, 16, 2048) dtype=float32>, 'logits_sup': <tf.Tensor 'model/StatefulPartitionedCall:7' shape=(16, 1000) dtype=float32>, 'block_group4': <tf.Tensor 'model/StatefulPartitionedCall:3' shape=(16, 8, 8, 4096) dtype=float32>}\n",
      "[Iter 300] Loss: 0.005281632300466299 Top 1: 0.9716666666666667\n",
      "[Iter 600] Loss: 0.012448831461369991 Top 1: 0.93625\n",
      "[Iter 900] Loss: 0.019188622012734413 Top 1: 0.8970138888888889\n",
      "Loss: 0.017723407596349716 Top 1: 0.9047219558964525\n",
      "Total time: 5.357358467578888 in minutes\n"
     ]
    }
   ],
   "source": [
    "run_model(model_r50_10, valid_generator, epochs=1, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbefec42-8f0c-439e-8191-fe1144040cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
