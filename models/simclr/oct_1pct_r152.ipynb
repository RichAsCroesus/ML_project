{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a261348-3abc-49da-aa9e-773b5b78c024",
   "metadata": {},
   "source": [
    "### General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2130f56d-dd15-4519-ae64-7905fa9ddf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import abs1\n",
    "import itertools\n",
    "import os\n",
    "import more_itertools\n",
    "import sys\n",
    "import time\n",
    "from absl import flags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7057deeb-ce61-4126-b15a-21a10684817b",
   "metadata": {},
   "source": [
    "## Tensorflow and keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de4a64c1-14b1-40d1-a40c-cd23c1ecde6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import keras\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "tf.compat.v1.enable_v2_behavior()\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub \n",
    "from keras.applications.resnet_v2 import ResNet101V2\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.layers import Input, GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import (Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, UpSampling2D, \n",
    "                          concatenate, GlobalAveragePooling2D, Input)\n",
    "from keras.regularizers import l2, l1_l2\n",
    "from keras.constraints import MaxNorm\n",
    "from keras.models import load_model, Sequential, Model\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet_v2 import ResNet50V2, ResNet152V2\n",
    "from keras.applications.xception import Xception\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.callbacks import Callback, EarlyStopping, LearningRateScheduler, ModelCheckpoint, LearningRateScheduler, TensorBoard\n",
    "import sklearn as sk \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fae050-4b87-41a3-872c-5fa21bda5b21",
   "metadata": {},
   "source": [
    "## SimCLRv2 colab fine tuning code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26fece07-762f-493d-9947-b16426dd8f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS_color_jitter_strength = 0.3\n",
    "CROP_PROPORTION = 0.875  # Standard for ImageNet.\n",
    "\n",
    "\n",
    "def random_apply(func, p, x):\n",
    "  \"\"\"Randomly apply function func to x with probability p.\"\"\"\n",
    "  return tf.cond(\n",
    "      tf.less(tf.random_uniform([], minval=0, maxval=1, dtype=tf.float32),\n",
    "              tf.cast(p, tf.float32)),\n",
    "      lambda: func(x),\n",
    "      lambda: x)\n",
    "\n",
    "\n",
    "def random_brightness(image, max_delta, impl='simclrv2'):\n",
    "  \"\"\"A multiplicative vs additive change of brightness.\"\"\"\n",
    "  if impl == 'simclrv2':\n",
    "    factor = tf.random_uniform(\n",
    "        [], tf.maximum(1.0 - max_delta, 0), 1.0 + max_delta)\n",
    "    image = image * factor\n",
    "  elif impl == 'simclrv1':\n",
    "    image = random_brightness(image, max_delta=max_delta)\n",
    "  else:\n",
    "    raise ValueError('Unknown impl {} for random brightness.'.format(impl))\n",
    "  return image\n",
    "\n",
    "\n",
    "def to_grayscale(image, keep_channels=True):\n",
    "  image = tf.image.rgb_to_grayscale(image)\n",
    "  if keep_channels:\n",
    "    image = tf.tile(image, [1, 1, 3])\n",
    "  return image\n",
    "\n",
    "\n",
    "def color_jitter(image,\n",
    "                 strength,\n",
    "                 random_order=True):\n",
    "  \"\"\"Distorts the color of the image.\n",
    "  Args:\n",
    "    image: The input image tensor.\n",
    "    strength: the floating number for the strength of the color augmentation.\n",
    "    random_order: A bool, specifying whether to randomize the jittering order.\n",
    "  Returns:\n",
    "    The distorted image tensor.\n",
    "  \"\"\"\n",
    "  brightness = 0.8 * strength\n",
    "  contrast = 0.8 * strength\n",
    "  saturation = 0.8 * strength\n",
    "  hue = 0.2 * strength\n",
    "  if random_order:\n",
    "    return color_jitter_rand(image, brightness, contrast, saturation, hue)\n",
    "  else:\n",
    "    return color_jitter_nonrand(image, brightness, contrast, saturation, hue)\n",
    "\n",
    "\n",
    "def color_jitter_nonrand(image, brightness=0, contrast=0, saturation=0, hue=0):\n",
    "  \"\"\"Distorts the color of the image (jittering order is fixed).\n",
    "  Args:\n",
    "    image: The input image tensor.\n",
    "    brightness: A float, specifying the brightness for color jitter.\n",
    "    contrast: A float, specifying the contrast for color jitter.\n",
    "    saturation: A float, specifying the saturation for color jitter.\n",
    "    hue: A float, specifying the hue for color jitter.\n",
    "  Returns:\n",
    "    The distorted image tensor.\n",
    "  \"\"\"\n",
    "  with tf.name_scope('distort_color'):\n",
    "    def apply_transform(i, x, brightness, contrast, saturation, hue):\n",
    "      \"\"\"Apply the i-th transformation.\"\"\"\n",
    "      if brightness != 0 and i == 0:\n",
    "        x = random_brightness(x, max_delta=brightness)\n",
    "      elif contrast != 0 and i == 1:\n",
    "        x = tf.image.random_contrast(\n",
    "            x, lower=1-contrast, upper=1+contrast)\n",
    "      elif saturation != 0 and i == 2:\n",
    "        x = tf.image.random_saturation(\n",
    "            x, lower=1-saturation, upper=1+saturation)\n",
    "      elif hue != 0:\n",
    "        x = tf.image.random_hue(x, max_delta=hue)\n",
    "      return x\n",
    "\n",
    "    for i in range(4):\n",
    "      image = apply_transform(i, image, brightness, contrast, saturation, hue)\n",
    "      image = tf.clip_by_value(image, 0., 1.)\n",
    "    return image\n",
    "\n",
    "\n",
    "def color_jitter_rand(image, brightness=0, contrast=0, saturation=0, hue=0):\n",
    "  \"\"\"Distorts the color of the image (jittering order is random).\n",
    "  Args:\n",
    "    image: The input image tensor.\n",
    "    brightness: A float, specifying the brightness for color jitter.\n",
    "    contrast: A float, specifying the contrast for color jitter.\n",
    "    saturation: A float, specifying the saturation for color jitter.\n",
    "    hue: A float, specifying the hue for color jitter.\n",
    "  Returns:\n",
    "    The distorted image tensor.\n",
    "  \"\"\"\n",
    "  with tf.name_scope('distort_color'):\n",
    "    def apply_transform(i, x):\n",
    "      \"\"\"Apply the i-th transformation.\"\"\"\n",
    "      def brightness_foo():\n",
    "        if brightness == 0:\n",
    "          return x\n",
    "        else:\n",
    "          return random_brightness(x, max_delta=brightness)\n",
    "      def contrast_foo():\n",
    "        if contrast == 0:\n",
    "          return x\n",
    "        else:\n",
    "          return tf.image.random_contrast(x, lower=1-contrast, upper=1+contrast)\n",
    "      def saturation_foo():\n",
    "        if saturation == 0:\n",
    "          return x\n",
    "        else:\n",
    "          return tf.image.random_saturation(\n",
    "              x, lower=1-saturation, upper=1+saturation)\n",
    "      def hue_foo():\n",
    "        if hue == 0:\n",
    "          return x\n",
    "        else:\n",
    "          return tf.image.random_hue(x, max_delta=hue)\n",
    "      x = tf.cond(tf.less(i, 2),\n",
    "                  lambda: tf.cond(tf.less(i, 1), brightness_foo, contrast_foo),\n",
    "                  lambda: tf.cond(tf.less(i, 3), saturation_foo, hue_foo))\n",
    "      return x\n",
    "\n",
    "    perm = tf.random_shuffle(tf.range(4))\n",
    "    for i in range(4):\n",
    "      image = apply_transform(perm[i], image)\n",
    "      image = tf.clip_by_value(image, 0., 1.)\n",
    "    return image\n",
    "\n",
    "\n",
    "def _compute_crop_shape(\n",
    "    image_height, image_width, aspect_ratio, crop_proportion):\n",
    "  \"\"\"Compute aspect ratio-preserving shape for central crop.\n",
    "  The resulting shape retains `crop_proportion` along one side and a proportion\n",
    "  less than or equal to `crop_proportion` along the other side.\n",
    "  Args:\n",
    "    image_height: Height of image to be cropped.\n",
    "    image_width: Width of image to be cropped.\n",
    "    aspect_ratio: Desired aspect ratio (width / height) of output.\n",
    "    crop_proportion: Proportion of image to retain along the less-cropped side.\n",
    "  Returns:\n",
    "    crop_height: Height of image after cropping.\n",
    "    crop_width: Width of image after cropping.\n",
    "  \"\"\"\n",
    "  image_width_float = tf.cast(image_width, tf.float32)\n",
    "  image_height_float = tf.cast(image_height, tf.float32)\n",
    "\n",
    "  def _requested_aspect_ratio_wider_than_image():\n",
    "    crop_height = tf.cast(tf.math.rint(\n",
    "        crop_proportion / aspect_ratio * image_width_float), tf.int32)\n",
    "    crop_width = tf.cast(tf.math.rint(\n",
    "        crop_proportion * image_width_float), tf.int32)\n",
    "    return crop_height, crop_width\n",
    "\n",
    "  def _image_wider_than_requested_aspect_ratio():\n",
    "    crop_height = tf.cast(\n",
    "        tf.math.rint(crop_proportion * image_height_float), tf.int32)\n",
    "    crop_width = tf.cast(tf.math.rint(\n",
    "        crop_proportion * aspect_ratio *\n",
    "        image_height_float), tf.int32)\n",
    "    return crop_height, crop_width\n",
    "\n",
    "  return tf.cond(\n",
    "      aspect_ratio > image_width_float / image_height_float,\n",
    "      _requested_aspect_ratio_wider_than_image,\n",
    "      _image_wider_than_requested_aspect_ratio)\n",
    "\n",
    "\n",
    "def center_crop(image, height, width, crop_proportion):\n",
    "  \"\"\"Crops to center of image and rescales to desired size.\n",
    "  Args:\n",
    "    image: Image Tensor to crop.\n",
    "    height: Height of image to be cropped.\n",
    "    width: Width of image to be cropped.\n",
    "    crop_proportion: Proportion of image to retain along the less-cropped side.\n",
    "  Returns:\n",
    "    A `height` x `width` x channels Tensor holding a central crop of `image`.\n",
    "  \"\"\"\n",
    "  shape = tf.shape(image)\n",
    "  image_height = shape[0]\n",
    "  image_width = shape[1]\n",
    "  crop_height, crop_width = _compute_crop_shape(\n",
    "      image_height, image_width, height / width, crop_proportion)\n",
    "  offset_height = ((image_height - crop_height) + 1) // 2\n",
    "  offset_width = ((image_width - crop_width) + 1) // 2\n",
    "  image = tf.image.crop_to_bounding_box(\n",
    "      image, offset_height, offset_width, crop_height, crop_width)\n",
    "\n",
    "  image = tf.compat.v1.image.resize_bicubic([image], [height, width])[0]\n",
    "\n",
    "  return image\n",
    "\n",
    "\n",
    "def distorted_bounding_box_crop(image,\n",
    "                                bbox,\n",
    "                                min_object_covered=0.1,\n",
    "                                aspect_ratio_range=(0.75, 1.33),\n",
    "                                area_range=(0.05, 1.0),\n",
    "                                max_attempts=100,\n",
    "                                scope=None):\n",
    "  \"\"\"Generates cropped_image using one of the bboxes randomly distorted.\n",
    "  See `tf.image.sample_distorted_bounding_box` for more documentation.\n",
    "  Args:\n",
    "    image: `Tensor` of image data.\n",
    "    bbox: `Tensor` of bounding boxes arranged `[1, num_boxes, coords]`\n",
    "        where each coordinate is [0, 1) and the coordinates are arranged\n",
    "        as `[ymin, xmin, ymax, xmax]`. If num_boxes is 0 then use the whole\n",
    "        image.\n",
    "    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n",
    "        area of the image must contain at least this fraction of any bounding\n",
    "        box supplied.\n",
    "    aspect_ratio_range: An optional list of `float`s. The cropped area of the\n",
    "        image must have an aspect ratio = width / height within this range.\n",
    "    area_range: An optional list of `float`s. The cropped area of the image\n",
    "        must contain a fraction of the supplied image within in this range.\n",
    "    max_attempts: An optional `int`. Number of attempts at generating a cropped\n",
    "        region of the image of the specified constraints. After `max_attempts`\n",
    "        failures, return the entire image.\n",
    "    scope: Optional `str` for name scope.\n",
    "  Returns:\n",
    "    (cropped image `Tensor`, distorted bbox `Tensor`).\n",
    "  \"\"\"\n",
    "  with tf.name_scope(scope, 'distorted_bounding_box_crop', [image, bbox]):\n",
    "    shape = tf.shape(image)\n",
    "    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n",
    "        shape,\n",
    "        bounding_boxes=bbox,\n",
    "        min_object_covered=min_object_covered,\n",
    "        aspect_ratio_range=aspect_ratio_range,\n",
    "        area_range=area_range,\n",
    "        max_attempts=max_attempts,\n",
    "        use_image_if_no_bounding_boxes=True)\n",
    "    bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n",
    "\n",
    "    # Crop the image to the specified bounding box.\n",
    "    offset_y, offset_x, _ = tf.unstack(bbox_begin)\n",
    "    target_height, target_width, _ = tf.unstack(bbox_size)\n",
    "    image = tf.image.crop_to_bounding_box(\n",
    "        image, offset_y, offset_x, target_height, target_width)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def crop_and_resize(image, height, width):\n",
    "  \"\"\"Make a random crop and resize it to height `height` and width `width`.\n",
    "  Args:\n",
    "    image: Tensor representing the image.\n",
    "    height: Desired image height.\n",
    "    width: Desired image width.\n",
    "  Returns:\n",
    "    A `height` x `width` x channels Tensor holding a random crop of `image`.\n",
    "  \"\"\"\n",
    "  bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])\n",
    "  aspect_ratio = width / height\n",
    "  image = distorted_bounding_box_crop(\n",
    "      image,\n",
    "      bbox,\n",
    "      min_object_covered=0.1,\n",
    "      aspect_ratio_range=(3. / 4 * aspect_ratio, 4. / 3. * aspect_ratio),\n",
    "      area_range=(0.08, 1.0),\n",
    "      max_attempts=100,\n",
    "      scope=None)\n",
    "  return tf.compat.v1.image.resize_bicubic([image], [height, width])[0]\n",
    "\n",
    "\n",
    "def gaussian_blur(image, kernel_size, sigma, padding='SAME'):\n",
    "  \"\"\"Blurs the given image with separable convolution.\n",
    "  Args:\n",
    "    image: Tensor of shape [height, width, channels] and dtype float to blur.\n",
    "    kernel_size: Integer Tensor for the size of the blur kernel. This is should\n",
    "      be an odd number. If it is an even number, the actual kernel size will be\n",
    "      size + 1.\n",
    "    sigma: Sigma value for gaussian operator.\n",
    "    padding: Padding to use for the convolution. Typically 'SAME' or 'VALID'.\n",
    "  Returns:\n",
    "    A Tensor representing the blurred image.\n",
    "  \"\"\"\n",
    "  radius = tf.to_int32(kernel_size / 2)\n",
    "  kernel_size = radius * 2 + 1\n",
    "  x = tf.to_float(tf.range(-radius, radius + 1))\n",
    "  blur_filter = tf.exp(\n",
    "      -tf.pow(x, 2.0) / (2.0 * tf.pow(tf.to_float(sigma), 2.0)))\n",
    "  blur_filter /= tf.reduce_sum(blur_filter)\n",
    "  # One vertical and one horizontal filter.\n",
    "  blur_v = tf.reshape(blur_filter, [kernel_size, 1, 1, 1])\n",
    "  blur_h = tf.reshape(blur_filter, [1, kernel_size, 1, 1])\n",
    "  num_channels = tf.shape(image)[-1]\n",
    "  blur_h = tf.tile(blur_h, [1, 1, num_channels, 1])\n",
    "  blur_v = tf.tile(blur_v, [1, 1, num_channels, 1])\n",
    "  expand_batch_dim = image.shape.ndims == 3\n",
    "  if expand_batch_dim:\n",
    "    # Tensorflow requires batched input to convolutions, which we can fake with\n",
    "    # an extra dimension.\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "  blurred = tf.nn.depthwise_conv2d(\n",
    "      image, blur_h, strides=[1, 1, 1, 1], padding=padding)\n",
    "  blurred = tf.nn.depthwise_conv2d(\n",
    "      blurred, blur_v, strides=[1, 1, 1, 1], padding=padding)\n",
    "  if expand_batch_dim:\n",
    "    blurred = tf.squeeze(blurred, axis=0)\n",
    "  return blurred\n",
    "\n",
    "\n",
    "def random_crop_with_resize(image, height, width, p=1.0):\n",
    "  \"\"\"Randomly crop and resize an image.\n",
    "  Args:\n",
    "    image: `Tensor` representing an image of arbitrary size.\n",
    "    height: Height of output image.\n",
    "    width: Width of output image.\n",
    "    p: Probability of applying this transformation.\n",
    "  Returns:\n",
    "    A preprocessed image `Tensor`.\n",
    "  \"\"\"\n",
    "  def _transform(image):  # pylint: disable=missing-docstring\n",
    "    image = crop_and_resize(image, height, width)\n",
    "    return image\n",
    "  return random_apply(_transform, p=p, x=image)\n",
    "\n",
    "\n",
    "def random_color_jitter(image, p=1.0):\n",
    "  def _transform(image):\n",
    "    color_jitter_t = functools.partial(\n",
    "        color_jitter, strength=FLAGS_color_jitter_strength)\n",
    "    image = random_apply(color_jitter_t, p=0.8, x=image)\n",
    "    return random_apply(to_grayscale, p=0.2, x=image)\n",
    "  return random_apply(_transform, p=p, x=image)\n",
    "\n",
    "\n",
    "def random_blur(image, height, width, p=1.0):\n",
    "  \"\"\"Randomly blur an image.\n",
    "  Args:\n",
    "    image: `Tensor` representing an image of arbitrary size.\n",
    "    height: Height of output image.\n",
    "    width: Width of output image.\n",
    "    p: probability of applying this transformation.\n",
    "  Returns:\n",
    "    A preprocessed image `Tensor`.\n",
    "  \"\"\"\n",
    "  del width\n",
    "  def _transform(image):\n",
    "    sigma = tf.random.uniform([], 0.1, 2.0, dtype=tf.float32)\n",
    "    return gaussian_blur(\n",
    "        image, kernel_size=height//10, sigma=sigma, padding='SAME')\n",
    "  return random_apply(_transform, p=p, x=image)\n",
    "\n",
    "\n",
    "def batch_random_blur(images_list, height, width, blur_probability=0.5):\n",
    "  \"\"\"Apply efficient batch data transformations.\n",
    "  Args:\n",
    "    images_list: a list of image tensors.\n",
    "    height: the height of image.\n",
    "    width: the width of image.\n",
    "    blur_probability: the probaility to apply the blur operator.\n",
    "  Returns:\n",
    "    Preprocessed feature list.\n",
    "  \"\"\"\n",
    "  def generate_selector(p, bsz):\n",
    "    shape = [bsz, 1, 1, 1]\n",
    "    selector = tf.cast(\n",
    "        tf.less(tf.random_uniform(shape, 0, 1, dtype=tf.float32), p),\n",
    "        tf.float32)\n",
    "    return selector\n",
    "\n",
    "  new_images_list = []\n",
    "  for images in images_list:\n",
    "    images_new = random_blur(images, height, width, p=1.)\n",
    "    selector = generate_selector(blur_probability, tf.shape(images)[0])\n",
    "    images = images_new * selector + images * (1 - selector)\n",
    "    images = tf.clip_by_value(images, 0., 1.)\n",
    "    new_images_list.append(images)\n",
    "\n",
    "  return new_images_list\n",
    "\n",
    "\n",
    "def preprocess_for_train(image, height, width,\n",
    "                         color_distort=True, crop=True, flip=True):\n",
    "  \"\"\"Preprocesses the given image for training.\n",
    "  Args:\n",
    "    image: `Tensor` representing an image of arbitrary size.\n",
    "    height: Height of output image.\n",
    "    width: Width of output image.\n",
    "    color_distort: Whether to apply the color distortion.\n",
    "    crop: Whether to crop the image.\n",
    "    flip: Whether or not to flip left and right of an image.\n",
    "  Returns:\n",
    "    A preprocessed image `Tensor`.\n",
    "  \"\"\"\n",
    "  if crop:\n",
    "    image = random_crop_with_resize(image, height, width)\n",
    "  if flip:\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "  if color_distort:\n",
    "    image = random_color_jitter(image)\n",
    "  image = tf.reshape(image, [height, width, 3])\n",
    "  image = tf.clip_by_value(image, 0., 1.)\n",
    "  return image\n",
    "\n",
    "\n",
    "def preprocess_for_eval(image, height, width, crop=True):\n",
    "  \"\"\"Preprocesses the given image for evaluation.\n",
    "  Args:\n",
    "    image: `Tensor` representing an image of arbitrary size.\n",
    "    height: Height of output image.\n",
    "    width: Width of output image.\n",
    "    crop: Whether or not to (center) crop the test images.\n",
    "  Returns:\n",
    "    A preprocessed image `Tensor`.\n",
    "  \"\"\"\n",
    "  if crop:\n",
    "    image = center_crop(image, height, width, crop_proportion=CROP_PROPORTION)\n",
    "  image = tf.reshape(image, [height, width, 3])\n",
    "  image = tf.clip_by_value(image, 0., 1.)\n",
    "  return image\n",
    "\n",
    "\n",
    "def preprocess_image(image, height, width, is_training=False,\n",
    "                     color_distort=True, test_crop=True):\n",
    "  \"\"\"Preprocesses the given image.\n",
    "  Args:\n",
    "    image: `Tensor` representing an image of arbitrary size.\n",
    "    height: Height of output image.\n",
    "    width: Width of output image.\n",
    "    is_training: `bool` for whether the preprocessing is for training.\n",
    "    color_distort: whether to apply the color distortion.\n",
    "    test_crop: whether or not to extract a central crop of the images\n",
    "        (as for standard ImageNet evaluation) during the evaluation.\n",
    "  Returns:\n",
    "    A preprocessed image `Tensor` of range [0, 1].\n",
    "  \"\"\"\n",
    "  image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "  if is_training:\n",
    "    return preprocess_for_train(image, height, width, color_distort)\n",
    "  else:\n",
    "    return preprocess_for_eval(image, height, width, test_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e26e4cbc-3686-4f65-9dac-13625e8d8351",
   "metadata": {},
   "outputs": [],
   "source": [
    "EETA_DEFAULT = 0.001\n",
    "\n",
    "\n",
    "class LARSOptimizer(tf.keras.optimizers.Optimizer):\n",
    "  \"\"\"Layer-wise Adaptive Rate Scaling for large batch training.\n",
    "\n",
    "  Introduced by \"Large Batch Training of Convolutional Networks\" by Y. You,\n",
    "  I. Gitman, and B. Ginsburg. (https://arxiv.org/abs/1708.03888)\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               learning_rate,\n",
    "               momentum=0.9,\n",
    "               use_nesterov=False,\n",
    "               weight_decay=0.0,\n",
    "               exclude_from_weight_decay=None,\n",
    "               exclude_from_layer_adaptation=None,\n",
    "               classic_momentum=True,\n",
    "               eeta=EETA_DEFAULT,\n",
    "               name=\"LARSOptimizer\"):\n",
    "    \"\"\"Constructs a LARSOptimizer.\n",
    "\n",
    "    Args:\n",
    "      learning_rate: A `float` for learning rate.\n",
    "      momentum: A `float` for momentum.\n",
    "      use_nesterov: A 'Boolean' for whether to use nesterov momentum.\n",
    "      weight_decay: A `float` for weight decay.\n",
    "      exclude_from_weight_decay: A list of `string` for variable screening, if\n",
    "          any of the string appears in a variable's name, the variable will be\n",
    "          excluded for computing weight decay. For example, one could specify\n",
    "          the list like ['batch_normalization', 'bias'] to exclude BN and bias\n",
    "          from weight decay.\n",
    "      exclude_from_layer_adaptation: Similar to exclude_from_weight_decay, but\n",
    "          for layer adaptation. If it is None, it will be defaulted the same as\n",
    "          exclude_from_weight_decay.\n",
    "      classic_momentum: A `boolean` for whether to use classic (or popular)\n",
    "          momentum. The learning rate is applied during momeuntum update in\n",
    "          classic momentum, but after momentum for popular momentum.\n",
    "      eeta: A `float` for scaling of learning rate when computing trust ratio.\n",
    "      name: The name for the scope.\n",
    "    \"\"\"\n",
    "    super(LARSOptimizer, self).__init__(name)\n",
    "\n",
    "    self._set_hyper(\"learning_rate\", learning_rate)\n",
    "    self.momentum = momentum\n",
    "    self.weight_decay = weight_decay\n",
    "    self.use_nesterov = use_nesterov\n",
    "    self.classic_momentum = classic_momentum\n",
    "    self.eeta = eeta\n",
    "    self.exclude_from_weight_decay = exclude_from_weight_decay\n",
    "    # exclude_from_layer_adaptation is set to exclude_from_weight_decay if the\n",
    "    # arg is None.\n",
    "    if exclude_from_layer_adaptation:\n",
    "      self.exclude_from_layer_adaptation = exclude_from_layer_adaptation\n",
    "    else:\n",
    "      self.exclude_from_layer_adaptation = exclude_from_weight_decay\n",
    "\n",
    "  def _create_slots(self, var_list):\n",
    "    for v in var_list:\n",
    "      self.add_slot(v, \"Momentum\")\n",
    "\n",
    "  def _resource_apply_dense(self, grad, param, apply_state=None):\n",
    "    if grad is None or param is None:\n",
    "      return tf.no_op()\n",
    "\n",
    "    var_device, var_dtype = param.device, param.dtype.base_dtype\n",
    "    coefficients = ((apply_state or {}).get((var_device, var_dtype)) or\n",
    "                    self._fallback_apply_state(var_device, var_dtype))\n",
    "    learning_rate = coefficients[\"lr_t\"]\n",
    "\n",
    "    param_name = param.name\n",
    "\n",
    "    v = self.get_slot(param, \"Momentum\")\n",
    "\n",
    "    if self._use_weight_decay(param_name):\n",
    "      grad += self.weight_decay * param\n",
    "\n",
    "    if self.classic_momentum:\n",
    "      trust_ratio = 1.0\n",
    "      if self._do_layer_adaptation(param_name):\n",
    "        w_norm = tf.norm(param, ord=2)\n",
    "        g_norm = tf.norm(grad, ord=2)\n",
    "        trust_ratio = tf.where(\n",
    "            tf.greater(w_norm, 0),\n",
    "            tf.where(tf.greater(g_norm, 0), (self.eeta * w_norm / g_norm), 1.0),\n",
    "            1.0)\n",
    "      scaled_lr = learning_rate * trust_ratio\n",
    "\n",
    "      next_v = tf.multiply(self.momentum, v) + scaled_lr * grad\n",
    "      if self.use_nesterov:\n",
    "        update = tf.multiply(self.momentum, next_v) + scaled_lr * grad\n",
    "      else:\n",
    "        update = next_v\n",
    "      next_param = param - update\n",
    "    else:\n",
    "      next_v = tf.multiply(self.momentum, v) + grad\n",
    "      if self.use_nesterov:\n",
    "        update = tf.multiply(self.momentum, next_v) + grad\n",
    "      else:\n",
    "        update = next_v\n",
    "\n",
    "      trust_ratio = 1.0\n",
    "      if self._do_layer_adaptation(param_name):\n",
    "        w_norm = tf.norm(param, ord=2)\n",
    "        v_norm = tf.norm(update, ord=2)\n",
    "        trust_ratio = tf.where(\n",
    "            tf.greater(w_norm, 0),\n",
    "            tf.where(tf.greater(v_norm, 0), (self.eeta * w_norm / v_norm), 1.0),\n",
    "            1.0)\n",
    "      scaled_lr = trust_ratio * learning_rate\n",
    "      next_param = param - scaled_lr * update\n",
    "\n",
    "    return tf.group(*[\n",
    "        param.assign(next_param, use_locking=False),\n",
    "        v.assign(next_v, use_locking=False)\n",
    "    ])\n",
    "\n",
    "  def _use_weight_decay(self, param_name):\n",
    "    \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n",
    "    if not self.weight_decay:\n",
    "      return False\n",
    "    if self.exclude_from_weight_decay:\n",
    "      for r in self.exclude_from_weight_decay:\n",
    "        if re.search(r, param_name) is not None:\n",
    "          return False\n",
    "    return True\n",
    "\n",
    "  def _do_layer_adaptation(self, param_name):\n",
    "    \"\"\"Whether to do layer-wise learning rate adaptation for `param_name`.\"\"\"\n",
    "    if self.exclude_from_layer_adaptation:\n",
    "      for r in self.exclude_from_layer_adaptation:\n",
    "        if re.search(r, param_name) is not None:\n",
    "          return False\n",
    "    return True\n",
    "\n",
    "  def get_config(self):\n",
    "    config = super(LARSOptimizer, self).get_config()\n",
    "    config.update({\n",
    "        \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n",
    "        \"momentum\": self.momentum,\n",
    "        \"classic_momentum\": self.classic_momentum,\n",
    "        \"weight_decay\": self.weight_decay,\n",
    "        \"eeta\": self.eeta,\n",
    "        \"use_nesterov\": self.use_nesterov,\n",
    "    })\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f29e85b-6a63-4e05-b430-3390cdc00104",
   "metadata": {},
   "source": [
    "## Image libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af31c0f8-1776-48f8-b37f-514b210b49f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "# from matplotlib. import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eedb96e7-0291-4f74-a101-f18f63d974b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r50_2x_sk1 = tf.saved_model.load(\"/bigdata2/OCT/1pct/r50_2x_sk1/saved_model/\");\n",
    "# r50_2x_sk1 = hub.load(\"/bigdata2/OCT/1pct/r50_2x_sk1/saved_model/\");\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91eeb22a-491c-4652-b469-8d377c9ac59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/OCT'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a89fa3e6-a72e-4ebb-8e9b-cf953c45251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(r50_2x_sk1.model._resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64abd189-a02e-4546-9e13-acb7678a5048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r50_2x_sk1_layer = hub.KerasLayer(r50_2x_sk1, input_shape=(256, 256, 3), trainable=True, weights='imagenet');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85564a81-25dd-4897-843e-79f106bbf5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(r50_2x_sk1_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc42c4ee-1855-4615-9c63-0c0350c9b3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r50_2x_sk1_layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1126b9ee-3f73-4019-b729-61799de7c308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.Sequential([r50_2x_sk1_layer, \n",
    "#                          GlobalAveragePooling2D(),\n",
    "#                          Dense(1024, activation='leaky_relu'),\n",
    "#                          Dense(128, activation='leaky_relu'),\n",
    "#                         Dense(4, activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420d7755-321f-4af3-bd84-3b284d59cdc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3a362bf-8bf0-4904-ba92-9fa610e1aa38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CNV', 'DME', 'DRUSEN', 'NORMAL']\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = './OCT2017/all_data'\n",
    "data_listing = os.listdir(DATA_PATH)\n",
    "print(data_listing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4649bdff-1d92-40c8-a7a7-0a0b264a8638",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir= DATA_PATH\n",
    "CNV_PATH = os.path.join(train_dir, 'CNV')\n",
    "DME_PATH = os.path.join(train_dir, 'DME')\n",
    "NORMAL_PATH = os.path.join(train_dir, 'NORMAL')\n",
    "DRUSEN_PATH = os.path.join(train_dir, 'DRUSEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2feed38-44b2-4f6f-aa1f-5ad11304f637",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnv = os.listdir(CNV_PATH)\n",
    "dme = os.listdir(DME_PATH)\n",
    "normal = os.listdir(NORMAL_PATH)\n",
    "drusen = os.listdir(DRUSEN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c1e5d92-be42-4531-b1a6-6dd4cc904f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37205\n",
      "11348\n",
      "26315\n",
      "8616\n"
     ]
    }
   ],
   "source": [
    "print(len(cnv))\n",
    "print(len(dme))\n",
    "print(len(normal))\n",
    "print(len(drusen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8623bff1-0817-4ea3-9457-668fa82676e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 66788 images belonging to 4 classes.\n",
      "Found 16696 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "epochs=6\n",
    "batch_size=16\n",
    "steps=60000//batch_size\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255., data_format=\"channels_last\", \n",
    "                                   validation_split=0.20,\n",
    "                                   horizontal_flip=True,\n",
    "                                   rotation_range=37,\n",
    "                                   vertical_flip=True)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    # target_size=(256, 256),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    subset='training',\n",
    "                                                    class_mode='binary',\n",
    "                                                    color_mode='rgb')\n",
    "\n",
    "\n",
    "valid_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    # target_size=(96, 96), \n",
    "                                                    batch_size=batch_size,\n",
    "                                                    shuffle=False,\n",
    "                                                    subset='validation',\n",
    "                                                    class_mode='binary',\n",
    "                                                    color_mode='rgb')\n",
    "\n",
    "\n",
    "\n",
    "best_path='./best_val_acc_'+str(int(time.time()))\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=best_path,\n",
    "    monitor='val_acc',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    initial_value_threshold=0.92)\n",
    "               \n",
    "       \n",
    "\n",
    "def _lrs(epoch, lr):\n",
    "    return lr\n",
    "    \n",
    "lrs = LearningRateScheduler(_lrs) \n",
    " \n",
    "stop = EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        min_delta=0.0001,\n",
    "        patience=10,\n",
    "        verbose=1,\n",
    "        mode='auto',\n",
    "        baseline=None,\n",
    "        restore_best_weights=True\n",
    ")\n",
    "\n",
    "stop_acc = EarlyStopping(\n",
    "            monitor='acc',\n",
    "            min_delta=0.001,\n",
    "            patience=10,\n",
    "            verbose=1,\n",
    "            mode='auto',\n",
    "            baseline=None,\n",
    "            restore_best_weights=True)\n",
    "\n",
    "logdir='./logs'\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12cdb3fd-ef9d-4ca3-9561-38ca24a66640",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "weight_decay = 0.\n",
    "num_classes = 4\n",
    "class Model(tf.keras.Model):\n",
    "  def __init__(self, path):\n",
    "    super(Model, self).__init__()\n",
    "    self.saved_model = tf.saved_model.load(path)\n",
    "    self.dense_layer = tf.keras.layers.Dense(units=num_classes, name=\"head_supervised_new\")\n",
    "    self.optimizer = LARSOptimizer(\n",
    "      learning_rate,\n",
    "      momentum=momentum,\n",
    "      weight_decay=weight_decay,\n",
    "      exclude_from_weight_decay=['batch_normalization', 'bias', 'head_supervised'])\n",
    "\n",
    "  def call(self, x, training=True):\n",
    "    with tf.GradientTape() as tape:\n",
    "      outputs = self.saved_model(x[0], trainable=False)\n",
    "      print(outputs)\n",
    "      logits_t = self.dense_layer(outputs['final_avg_pool'])\n",
    "      loss_t = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels = tf.one_hot(x[1], num_classes), logits=logits_t))\n",
    "      if training:\n",
    "          dense_layer_weights = self.dense_layer.trainable_weights\n",
    "          print('Variables to train:', dense_layer_weights)\n",
    "          grads = tape.gradient(loss_t, dense_layer_weights)\n",
    "          self.optimizer.apply_gradients(zip(grads, dense_layer_weights))\n",
    "    return loss_t, x[0], logits_t, x[1]\n",
    "\n",
    "# model = Model(\"gs://simclr-checkpoints-tf2/simclrv2/finetuned_100pct/r50_1x_sk0/saved_model/\")\n",
    "\n",
    "# Remove this for debugging.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78d85efa-6825-4d67-bbd0-c71147ef5259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(m, generator, epochs=10, training=True):\n",
    "    steps = len(generator.filenames)//generator.batch_size \n",
    "    for e in range(epochs):\n",
    "        print(f'Starting epoch: {e+1}')\n",
    "        total_correct = 0 \n",
    "        total_processed = 0 \n",
    "        total_loss = 0\n",
    "        epoch_start_time = time.time()\n",
    "        for it in range(steps):\n",
    "            x = next(generator)\n",
    "            # x[0] = preprocess_image(\n",
    "            #      x[0], 256, 256, is_training=False, color_distort=False)\n",
    "            xx = (x[0], np.int32(x[1]))\n",
    "            loss, image, logits, labels = train_step(m, xx, training=training)\n",
    "            logits = logits.numpy()\n",
    "            labels = labels.numpy()\n",
    "            pred = logits.argmax(-1)\n",
    "            correct = np.sum(pred == labels)\n",
    "            total = labels.size\n",
    "            total_processed += total\n",
    "            total_loss += loss\n",
    "            total_correct += correct\n",
    "            if (it+1) % 300 == 0:\n",
    "                print(\"[Iter {}] Loss: {} Top 1: {}\".format(it+1, total_loss/total_processed, total_correct/float(total_processed)))\n",
    "        run_time = (time.time() - epoch_start_time)/60 \n",
    "        print(f\"Loss: {total_loss/total_processed} Top 1: {total_correct/total_processed}\")\n",
    "        print(f\"Total time: {run_time} in minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e316fad-dff5-4e65-9848-090acf242d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "977e3f72-b302-4ecc-b771-21bae17b49b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(m, x, training=True):\n",
    "  return m(x, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f868baa-e328-4e6d-9850-6ae74e65b9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 17:15:01.492203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-26 17:15:01.497171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-26 17:15:01.497342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-26 17:15:01.497848: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-26 17:15:01.498765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-26 17:15:01.498897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-26 17:15:01.499014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-26 17:15:01.811446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-26 17:15:01.811617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-26 17:15:01.811745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-26 17:15:01.811860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10241 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:05:00.0, compute capability: 8.6\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_28_layer_call_and_return_conditional_losses_59016) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_177_layer_call_and_return_conditional_losses_134414) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_45_layer_call_and_return_conditional_losses_62688) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_19_layer_call_and_return_conditional_losses_57072) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_133_layer_call_and_return_conditional_losses_81708) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_181_layer_call_and_return_conditional_losses_92076) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_169_layer_call_and_return_conditional_losses_133934) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_63_layer_call_and_return_conditional_losses_139694) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_106_layer_call_and_return_conditional_losses_130214) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_94_layer_call_and_return_conditional_losses_129494) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_200_layer_call_and_return_conditional_losses_96192) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_32_layer_call_and_return_conditional_losses_59880) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_90_layer_call_and_return_conditional_losses_72420) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_26_layer_call_and_return_conditional_losses_137534) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_194_layer_call_and_return_conditional_losses_135494) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_38_layer_call_and_return_conditional_losses_61176) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_37_layer_call_and_return_conditional_losses_126014) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference___call___116255) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_117_layer_call_and_return_conditional_losses_78252) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_72_layer_call_and_return_conditional_losses_68532) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_115_layer_call_and_return_conditional_losses_142814) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_60_layer_call_and_return_conditional_losses_139574) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_33_layer_call_and_return_conditional_losses_125774) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_189_layer_call_and_return_conditional_losses_93804) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_135_layer_call_and_return_conditional_losses_144014) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_5_layer_call_and_return_conditional_losses_136334) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_80_layer_call_and_return_conditional_losses_70260) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_56_layer_call_and_return_conditional_losses_65076) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_79_layer_call_and_return_conditional_losses_70044) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_142_layer_call_and_return_conditional_losses_83652) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_163_layer_call_and_return_conditional_losses_88188) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_105_layer_call_and_return_conditional_losses_75660) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_130_layer_call_and_return_conditional_losses_131654) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_39_layer_call_and_return_conditional_losses_138374) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_203_layer_call_and_return_conditional_losses_96840) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_208_layer_call_and_return_conditional_losses_123633) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_168_layer_call_and_return_conditional_losses_89268) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_209_layer_call_and_return_conditional_losses_98064) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_162_layer_call_and_return_conditional_losses_133574) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_65_layer_call_and_return_conditional_losses_127694) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_126_layer_call_and_return_conditional_losses_80196) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_202_layer_call_and_return_conditional_losses_135974) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_41_layer_call_and_return_conditional_losses_126254) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_199_layer_call_and_return_conditional_losses_135854) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_193_layer_call_and_return_conditional_losses_135374) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_3_layer_call_and_return_conditional_losses_53604) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_164_layer_call_and_return_conditional_losses_145814) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_112_layer_call_and_return_conditional_losses_77172) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_53_layer_call_and_return_conditional_losses_64428) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_86_layer_call_and_return_conditional_losses_71556) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_16_layer_call_and_return_conditional_losses_56424) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_8_layer_call_and_return_conditional_losses_124214) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_43_layer_call_and_return_conditional_losses_62256) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_125_layer_call_and_return_conditional_losses_131294) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_105_layer_call_and_return_conditional_losses_130094) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_159_layer_call_and_return_conditional_losses_145454) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_61_layer_call_and_return_conditional_losses_127454) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_186_layer_call_and_return_conditional_losses_135014) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_100_layer_call_and_return_conditional_losses_141974) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_59_layer_call_and_return_conditional_losses_139454) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_150_layer_call_and_return_conditional_losses_85380) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_157_layer_call_and_return_conditional_losses_86892) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_14_layer_call_and_return_conditional_losses_55980) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_157_layer_call_and_return_conditional_losses_133214) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_34_layer_call_and_return_conditional_losses_138014) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_98_layer_call_and_return_conditional_losses_129734) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_58_layer_call_and_return_conditional_losses_65508) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_9_layer_call_and_return_conditional_losses_136574) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_99_layer_call_and_return_conditional_losses_74364) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_57_layer_call_and_return_conditional_losses_127214) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_layer_call_and_return_conditional_losses_123175) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_92_layer_call_and_return_conditional_losses_72852) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_112_layer_call_and_return_conditional_losses_142694) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_179_layer_call_and_return_conditional_losses_91644) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_107_layer_call_and_return_conditional_losses_142334) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_170_layer_call_and_return_conditional_losses_134054) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_44_layer_call_and_return_conditional_losses_126374) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_21_layer_call_and_return_conditional_losses_57504) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_51_layer_call_and_return_conditional_losses_63996) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_144_layer_call_and_return_conditional_losses_144614) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_206_layer_call_and_return_conditional_losses_136214) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_16_layer_call_and_return_conditional_losses_124694) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_5_layer_call_and_return_conditional_losses_54036) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_159_layer_call_and_return_conditional_losses_87324) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_167_layer_call_and_return_conditional_losses_89052) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_75_layer_call_and_return_conditional_losses_69180) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_84_layer_call_and_return_conditional_losses_71124) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_136_layer_call_and_return_conditional_losses_144134) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_133_layer_call_and_return_conditional_losses_131774) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_192_layer_call_and_return_conditional_losses_94452) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_137_layer_call_and_return_conditional_losses_82572) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_174_layer_call_and_return_conditional_losses_90564) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_76_layer_call_and_return_conditional_losses_140534) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_96_layer_call_and_return_conditional_losses_141734) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_172_layer_call_and_return_conditional_losses_146294) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_142_layer_call_and_return_conditional_losses_132374) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_71_layer_call_and_return_conditional_losses_68316) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_136_layer_call_and_return_conditional_losses_82356) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_50_layer_call_and_return_conditional_losses_126854) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_201_layer_call_and_return_conditional_losses_96408) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_22_layer_call_and_return_conditional_losses_57720) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_36_layer_call_and_return_conditional_losses_60744) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_49_layer_call_and_return_conditional_losses_126734) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_84_layer_call_and_return_conditional_losses_141014) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_158_layer_call_and_return_conditional_losses_87108) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_153_layer_call_and_return_conditional_losses_86028) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_31_layer_call_and_return_conditional_losses_59664) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_67_layer_call_and_return_conditional_losses_139934) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_143_layer_call_and_return_conditional_losses_83868) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_172_layer_call_and_return_conditional_losses_90132) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_131_layer_call_and_return_conditional_losses_81276) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_39_layer_call_and_return_conditional_losses_61392) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_10_layer_call_and_return_conditional_losses_136694) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_208_layer_call_and_return_conditional_losses_97876) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_7_layer_call_and_return_conditional_losses_124094) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_178_layer_call_and_return_conditional_losses_134534) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_195_layer_call_and_return_conditional_losses_135614) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_42_layer_call_and_return_conditional_losses_138494) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_163_layer_call_and_return_conditional_losses_145694) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_146_layer_call_and_return_conditional_losses_84516) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_137_layer_call_and_return_conditional_losses_132014) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_37_layer_call_and_return_conditional_losses_60960) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_156_layer_call_and_return_conditional_losses_86676) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_32_layer_call_and_return_conditional_losses_125654) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_78_layer_call_and_return_conditional_losses_69828) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_74_layer_call_and_return_conditional_losses_128294) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_88_layer_call_and_return_conditional_losses_141254) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_50_layer_call_and_return_conditional_losses_63780) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_73_layer_call_and_return_conditional_losses_68748) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_91_layer_call_and_return_conditional_losses_141374) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_149_layer_call_and_return_conditional_losses_132734) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_111_layer_call_and_return_conditional_losses_142574) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_90_layer_call_and_return_conditional_losses_129254) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_4_layer_call_and_return_conditional_losses_53820) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_110_layer_call_and_return_conditional_losses_76740) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_35_layer_call_and_return_conditional_losses_60528) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_183_layer_call_and_return_conditional_losses_92508) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_75_layer_call_and_return_conditional_losses_140414) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_113_layer_call_and_return_conditional_losses_130574) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_29_layer_call_and_return_conditional_losses_59232) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_48_layer_call_and_return_conditional_losses_63336) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_17_layer_call_and_return_conditional_losses_124814) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_30_layer_call_and_return_conditional_losses_59448) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_64_layer_call_and_return_conditional_losses_139814) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_94_layer_call_and_return_conditional_losses_73284) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_25_layer_call_and_return_conditional_losses_58368) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_87_layer_call_and_return_conditional_losses_71772) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_185_layer_call_and_return_conditional_losses_92940) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_140_layer_call_and_return_conditional_losses_144374) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_162_layer_call_and_return_conditional_losses_87972) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_19_layer_call_and_return_conditional_losses_137174) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_111_layer_call_and_return_conditional_losses_76956) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_4_layer_call_and_return_conditional_losses_123974) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_88_layer_call_and_return_conditional_losses_71988) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_128_layer_call_and_return_conditional_losses_80628) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_114_layer_call_and_return_conditional_losses_77604) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_99_layer_call_and_return_conditional_losses_141854) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_179_layer_call_and_return_conditional_losses_146654) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_79_layer_call_and_return_conditional_losses_140654) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_70_layer_call_and_return_conditional_losses_68100) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_69_layer_call_and_return_conditional_losses_67884) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_29_layer_call_and_return_conditional_losses_125534) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_169_layer_call_and_return_conditional_losses_89484) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_8_layer_call_and_return_conditional_losses_54684) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_132_layer_call_and_return_conditional_losses_143894) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_108_layer_call_and_return_conditional_losses_142454) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_68_layer_call_and_return_conditional_losses_67668) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_206_layer_call_and_return_conditional_losses_97488) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_85_layer_call_and_return_conditional_losses_71340) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_83_layer_call_and_return_conditional_losses_70908) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_11_layer_call_and_return_conditional_losses_55332) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_116_layer_call_and_return_conditional_losses_142934) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_66_layer_call_and_return_conditional_losses_67236) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_87_layer_call_and_return_conditional_losses_141134) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_199_layer_call_and_return_conditional_losses_95976) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_28_layer_call_and_return_conditional_losses_125414) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_81_layer_call_and_return_conditional_losses_128654) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_201_layer_call_and_return_conditional_losses_147974) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_127_layer_call_and_return_conditional_losses_80412) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_1_layer_call_and_return_conditional_losses_53148) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_209_layer_call_and_return_conditional_losses_123740) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_119_layer_call_and_return_conditional_losses_143054) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_55_layer_call_and_return_conditional_losses_64860) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_115_layer_call_and_return_conditional_losses_77820) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_18_layer_call_and_return_conditional_losses_137054) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_101_layer_call_and_return_conditional_losses_129854) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_10_layer_call_and_return_conditional_losses_55116) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_45_layer_call_and_return_conditional_losses_126494) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_156_layer_call_and_return_conditional_losses_145334) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_198_layer_call_and_return_conditional_losses_135734) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_173_layer_call_and_return_conditional_losses_90348) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_117_layer_call_and_return_conditional_losses_130814) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_151_layer_call_and_return_conditional_losses_85596) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_61_layer_call_and_return_conditional_losses_66156) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_170_layer_call_and_return_conditional_losses_89700) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_191_layer_call_and_return_conditional_losses_147374) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_46_layer_call_and_return_conditional_losses_62904) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_23_layer_call_and_return_conditional_losses_137414) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_49_layer_call_and_return_conditional_losses_63564) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_155_layer_call_and_return_conditional_losses_86460) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_123_layer_call_and_return_conditional_losses_79548) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_144_layer_call_and_return_conditional_losses_84084) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_56_layer_call_and_return_conditional_losses_139334) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_93_layer_call_and_return_conditional_losses_73068) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_120_layer_call_and_return_conditional_losses_143174) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_152_layer_call_and_return_conditional_losses_85812) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_12_layer_call_and_return_conditional_losses_124454) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_80_layer_call_and_return_conditional_losses_140774) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_41_layer_call_and_return_conditional_losses_61824) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_92_layer_call_and_return_conditional_losses_141494) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_47_layer_call_and_return_conditional_losses_138854) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_181_layer_call_and_return_conditional_losses_134654) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_177_layer_call_and_return_conditional_losses_91212) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_35_layer_call_and_return_conditional_losses_138134) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_161_layer_call_and_return_conditional_losses_133454) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_24_layer_call_and_return_conditional_losses_125174) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_138_layer_call_and_return_conditional_losses_82788) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_189_layer_call_and_return_conditional_losses_135134) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_130_layer_call_and_return_conditional_losses_81060) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_174_layer_call_and_return_conditional_losses_134294) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_107_layer_call_and_return_conditional_losses_76092) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_171_layer_call_and_return_conditional_losses_146174) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_96_layer_call_and_return_conditional_losses_73716) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_65_layer_call_and_return_conditional_losses_67020) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_131_layer_call_and_return_conditional_losses_143774) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_102_layer_call_and_return_conditional_losses_129974) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_85_layer_call_and_return_conditional_losses_128894) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_62_layer_call_and_return_conditional_losses_66372) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_196_layer_call_and_return_conditional_losses_95328) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_122_layer_call_and_return_conditional_losses_79332) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_38_layer_call_and_return_conditional_losses_138254) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_24_layer_call_and_return_conditional_losses_58152) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_6_layer_call_and_return_conditional_losses_136454) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_114_layer_call_and_return_conditional_losses_130694) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_109_layer_call_and_return_conditional_losses_130334) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_166_layer_call_and_return_conditional_losses_133814) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_20_layer_call_and_return_conditional_losses_57288) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_191_layer_call_and_return_conditional_losses_94236) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_12_layer_call_and_return_conditional_losses_55548) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_30_layer_call_and_return_conditional_losses_137774) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_158_layer_call_and_return_conditional_losses_133334) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_154_layer_call_and_return_conditional_losses_86244) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_116_layer_call_and_return_conditional_losses_78036) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_125_layer_call_and_return_conditional_losses_79980) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_140_layer_call_and_return_conditional_losses_83220) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_layer_call_and_return_conditional_losses_52932) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_187_layer_call_and_return_conditional_losses_147134) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_197_layer_call_and_return_conditional_losses_95544) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_74_layer_call_and_return_conditional_losses_68964) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_57_layer_call_and_return_conditional_losses_65292) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_160_layer_call_and_return_conditional_losses_145574) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_173_layer_call_and_return_conditional_losses_134174) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_54_layer_call_and_return_conditional_losses_64644) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_207_layer_call_and_return_conditional_losses_123524) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_33_layer_call_and_return_conditional_losses_60096) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_21_layer_call_and_return_conditional_losses_125054) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_171_layer_call_and_return_conditional_losses_89916) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_55_layer_call_and_return_conditional_losses_139214) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_138_layer_call_and_return_conditional_losses_132134) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_187_layer_call_and_return_conditional_losses_93372) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_109_layer_call_and_return_conditional_losses_76524) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_141_layer_call_and_return_conditional_losses_83436) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_103_layer_call_and_return_conditional_losses_75228) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_188_layer_call_and_return_conditional_losses_147254) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_118_layer_call_and_return_conditional_losses_78468) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_121_layer_call_and_return_conditional_losses_131054) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_184_layer_call_and_return_conditional_losses_147014) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_152_layer_call_and_return_conditional_losses_145094) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_149_layer_call_and_return_conditional_losses_85164) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_126_layer_call_and_return_conditional_losses_131414) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_91_layer_call_and_return_conditional_losses_72636) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_44_layer_call_and_return_conditional_losses_62472) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_166_layer_call_and_return_conditional_losses_88836) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_150_layer_call_and_return_conditional_losses_132854) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_185_layer_call_and_return_conditional_losses_134894) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_155_layer_call_and_return_conditional_losses_145214) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_89_layer_call_and_return_conditional_losses_129134) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_161_layer_call_and_return_conditional_losses_87756) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_123_layer_call_and_return_conditional_losses_143294) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_9_layer_call_and_return_conditional_losses_54900) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_27_layer_call_and_return_conditional_losses_58800) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_164_layer_call_and_return_conditional_losses_88404) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_129_layer_call_and_return_conditional_losses_80844) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_3_layer_call_and_return_conditional_losses_123854) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_82_layer_call_and_return_conditional_losses_128774) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_124_layer_call_and_return_conditional_losses_143414) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_13_layer_call_and_return_conditional_losses_136814) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_145_layer_call_and_return_conditional_losses_132494) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_113_layer_call_and_return_conditional_losses_77388) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_103_layer_call_and_return_conditional_losses_142094) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_52_layer_call_and_return_conditional_losses_64212) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_97_layer_call_and_return_conditional_losses_73932) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_200_layer_call_and_return_conditional_losses_147854) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_66_layer_call_and_return_conditional_losses_127814) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_71_layer_call_and_return_conditional_losses_140174) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_83_layer_call_and_return_conditional_losses_140894) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_124_layer_call_and_return_conditional_losses_79764) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_76_layer_call_and_return_conditional_losses_69396) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_73_layer_call_and_return_conditional_losses_128174) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_134_layer_call_and_return_conditional_losses_131894) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_18_layer_call_and_return_conditional_losses_56856) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_146_layer_call_and_return_conditional_losses_132614) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_48_layer_call_and_return_conditional_losses_126614) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_52_layer_call_and_return_conditional_losses_139094) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_197_layer_call_and_return_conditional_losses_147734) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_93_layer_call_and_return_conditional_losses_129374) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_180_layer_call_and_return_conditional_losses_146774) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_196_layer_call_and_return_conditional_losses_147614) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_59_layer_call_and_return_conditional_losses_65724) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_34_layer_call_and_return_conditional_losses_60312) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_86_layer_call_and_return_conditional_losses_129014) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_78_layer_call_and_return_conditional_losses_128534) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_67_layer_call_and_return_conditional_losses_67452) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_147_layer_call_and_return_conditional_losses_144734) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_108_layer_call_and_return_conditional_losses_76308) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_62_layer_call_and_return_conditional_losses_127574) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_101_layer_call_and_return_conditional_losses_74796) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_1_layer_call_and_return_conditional_losses_123295) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_81_layer_call_and_return_conditional_losses_70476) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_192_layer_call_and_return_conditional_losses_147494) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_143_layer_call_and_return_conditional_losses_144494) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_20_layer_call_and_return_conditional_losses_124934) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_118_layer_call_and_return_conditional_losses_130934) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_148_layer_call_and_return_conditional_losses_84948) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_203_layer_call_and_return_conditional_losses_136094) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_186_layer_call_and_return_conditional_losses_93156) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_42_layer_call_and_return_conditional_losses_62040) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_154_layer_call_and_return_conditional_losses_133094) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_15_layer_call_and_return_conditional_losses_56196) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_207_layer_call_and_return_conditional_losses_97682) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_2_layer_call_and_return_conditional_losses_53364) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_27_layer_call_and_return_conditional_losses_137654) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_145_layer_call_and_return_conditional_losses_84300) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_168_layer_call_and_return_conditional_losses_146054) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_132_layer_call_and_return_conditional_losses_81492) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_176_layer_call_and_return_conditional_losses_90996) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_182_layer_call_and_return_conditional_losses_134774) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_204_layer_call_and_return_conditional_losses_148094) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_72_layer_call_and_return_conditional_losses_140294) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_95_layer_call_and_return_conditional_losses_141614) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_26_layer_call_and_return_conditional_losses_58584) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_135_layer_call_and_return_conditional_losses_82140) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_102_layer_call_and_return_conditional_losses_75012) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_160_layer_call_and_return_conditional_losses_87540) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_127_layer_call_and_return_conditional_losses_143534) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_184_layer_call_and_return_conditional_losses_92724) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_98_layer_call_and_return_conditional_losses_74148) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_15_layer_call_and_return_conditional_losses_124574) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_25_layer_call_and_return_conditional_losses_125294) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_139_layer_call_and_return_conditional_losses_144254) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_51_layer_call_and_return_conditional_losses_138974) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_120_layer_call_and_return_conditional_losses_78900) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_6_layer_call_and_return_conditional_losses_54252) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_141_layer_call_and_return_conditional_losses_132254) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_47_layer_call_and_return_conditional_losses_63120) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_22_layer_call_and_return_conditional_losses_137294) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_183_layer_call_and_return_conditional_losses_146894) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_82_layer_call_and_return_conditional_losses_70692) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_58_layer_call_and_return_conditional_losses_127334) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_106_layer_call_and_return_conditional_losses_75876) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_167_layer_call_and_return_conditional_losses_145934) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_11_layer_call_and_return_conditional_losses_124334) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_202_layer_call_and_return_conditional_losses_96624) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_129_layer_call_and_return_conditional_losses_131534) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_54_layer_call_and_return_conditional_losses_127094) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_188_layer_call_and_return_conditional_losses_93588) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_151_layer_call_and_return_conditional_losses_144974) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_193_layer_call_and_return_conditional_losses_94668) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_175_layer_call_and_return_conditional_losses_146414) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_204_layer_call_and_return_conditional_losses_97056) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_89_layer_call_and_return_conditional_losses_72204) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_43_layer_call_and_return_conditional_losses_138614) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_148_layer_call_and_return_conditional_losses_144854) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_153_layer_call_and_return_conditional_losses_132974) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_205_layer_call_and_return_conditional_losses_97272) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_122_layer_call_and_return_conditional_losses_131174) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_2_layer_call_and_return_conditional_losses_123415) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_175_layer_call_and_return_conditional_losses_90780) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_190_layer_call_and_return_conditional_losses_94020) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_104_layer_call_and_return_conditional_losses_75444) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_7_layer_call_and_return_conditional_losses_54468) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_121_layer_call_and_return_conditional_losses_79116) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_182_layer_call_and_return_conditional_losses_92292) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_64_layer_call_and_return_conditional_losses_66804) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_36_layer_call_and_return_conditional_losses_125894) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_178_layer_call_and_return_conditional_losses_91428) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_40_layer_call_and_return_conditional_losses_61608) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_31_layer_call_and_return_conditional_losses_137894) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_70_layer_call_and_return_conditional_losses_128054) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_180_layer_call_and_return_conditional_losses_91860) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_60_layer_call_and_return_conditional_losses_65940) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_95_layer_call_and_return_conditional_losses_73500) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_40_layer_call_and_return_conditional_losses_126134) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_139_layer_call_and_return_conditional_losses_83004) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_14_layer_call_and_return_conditional_losses_136934) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_165_layer_call_and_return_conditional_losses_133694) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_195_layer_call_and_return_conditional_losses_95112) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_23_layer_call_and_return_conditional_losses_57936) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_165_layer_call_and_return_conditional_losses_88620) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_69_layer_call_and_return_conditional_losses_127934) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_77_layer_call_and_return_conditional_losses_69612) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_77_layer_call_and_return_conditional_losses_128414) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_190_layer_call_and_return_conditional_losses_135254) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_128_layer_call_and_return_conditional_losses_143654) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_176_layer_call_and_return_conditional_losses_146534) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_198_layer_call_and_return_conditional_losses_95760) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_205_layer_call_and_return_conditional_losses_148214) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_104_layer_call_and_return_conditional_losses_142214) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_134_layer_call_and_return_conditional_losses_81924) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_147_layer_call_and_return_conditional_losses_84732) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_63_layer_call_and_return_conditional_losses_66588) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_119_layer_call_and_return_conditional_losses_78684) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_68_layer_call_and_return_conditional_losses_140054) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_100_layer_call_and_return_conditional_losses_74580) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_194_layer_call_and_return_conditional_losses_94896) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_13_layer_call_and_return_conditional_losses_55764) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_97_layer_call_and_return_conditional_losses_129614) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_46_layer_call_and_return_conditional_losses_138734) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_53_layer_call_and_return_conditional_losses_126974) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_17_layer_call_and_return_conditional_losses_56640) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sync_batch_normalization_110_layer_call_and_return_conditional_losses_130454) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    }
   ],
   "source": [
    "model_r152 = Model(\"./1pct/r152_2x_sk1/saved_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca4816b0-5960-4008-b340-8f6df2b32ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer=keras.optimizers.adam_v2.Adam(learning_rate=0.000001), loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "model_r152.compile(loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31526744-2c56-46a5-859d-cbe8c28b27d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CNV': 0, 'DME': 1, 'DRUSEN': 2, 'NORMAL': 3}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20b31431-d5c0-4373-a44b-2caafda2612d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3750\n",
      "6\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(steps)\n",
    "print(epochs)\n",
    "print(batch_size)\n",
    "# res101_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425613e8-3d69-4330-9534-51bcef3c2cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d1734d0-2c69-4f8e-bb2d-7c1e29833ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'initial_max_pool': <tf.Tensor 'model/StatefulPartitionedCall:6' shape=(16, 64, 64, 128) dtype=float32>, 'block_group1': <tf.Tensor 'model/StatefulPartitionedCall:0' shape=(16, 64, 64, 512) dtype=float32>, 'initial_conv': <tf.Tensor 'model/StatefulPartitionedCall:5' shape=(16, 128, 128, 128) dtype=float32>, 'block_group2': <tf.Tensor 'model/StatefulPartitionedCall:1' shape=(16, 32, 32, 1024) dtype=float32>, 'block_group4': <tf.Tensor 'model/StatefulPartitionedCall:3' shape=(16, 8, 8, 4096) dtype=float32>, 'final_avg_pool': <tf.Tensor 'model/StatefulPartitionedCall:4' shape=(16, 4096) dtype=float32>, 'logits_sup': <tf.Tensor 'model/StatefulPartitionedCall:7' shape=(16, 1000) dtype=float32>, 'block_group3': <tf.Tensor 'model/StatefulPartitionedCall:2' shape=(16, 16, 16, 2048) dtype=float32>}\n",
      "Variables to train: [<tf.Variable 'model/head_supervised_new/kernel:0' shape=(4096, 4) dtype=float32>, <tf.Variable 'model/head_supervised_new/bias:0' shape=(4,) dtype=float32>]\n",
      "{'initial_max_pool': <tf.Tensor 'model/StatefulPartitionedCall:6' shape=(16, 64, 64, 128) dtype=float32>, 'block_group1': <tf.Tensor 'model/StatefulPartitionedCall:0' shape=(16, 64, 64, 512) dtype=float32>, 'initial_conv': <tf.Tensor 'model/StatefulPartitionedCall:5' shape=(16, 128, 128, 128) dtype=float32>, 'block_group2': <tf.Tensor 'model/StatefulPartitionedCall:1' shape=(16, 32, 32, 1024) dtype=float32>, 'block_group4': <tf.Tensor 'model/StatefulPartitionedCall:3' shape=(16, 8, 8, 4096) dtype=float32>, 'final_avg_pool': <tf.Tensor 'model/StatefulPartitionedCall:4' shape=(16, 4096) dtype=float32>, 'logits_sup': <tf.Tensor 'model/StatefulPartitionedCall:7' shape=(16, 1000) dtype=float32>, 'block_group3': <tf.Tensor 'model/StatefulPartitionedCall:2' shape=(16, 16, 16, 2048) dtype=float32>}\n",
      "Variables to train: [<tf.Variable 'model/head_supervised_new/kernel:0' shape=(4096, 4) dtype=float32>, <tf.Variable 'model/head_supervised_new/bias:0' shape=(4,) dtype=float32>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 17:15:30.560038: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8202\n",
      "2022-04-26 17:15:33.444804: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 300] Loss: 0.053756363689899445 Top 1: 0.693125\n",
      "[Iter 600] Loss: 0.046800535172224045 Top 1: 0.738125\n",
      "[Iter 900] Loss: 0.04304898902773857 Top 1: 0.7595138888888889\n",
      "[Iter 1200] Loss: 0.04028021916747093 Top 1: 0.77359375\n",
      "[Iter 1500] Loss: 0.03841250017285347 Top 1: 0.7827083333333333\n",
      "[Iter 1800] Loss: 0.03680357709527016 Top 1: 0.7919791666666667\n",
      "[Iter 2100] Loss: 0.03567297384142876 Top 1: 0.798422619047619\n",
      "[Iter 2400] Loss: 0.03481744974851608 Top 1: 0.8033072916666667\n",
      "[Iter 2700] Loss: 0.03395311161875725 Top 1: 0.8086574074074074\n",
      "[Iter 3000] Loss: 0.03320631384849548 Top 1: 0.8129375\n",
      "[Iter 3300] Loss: 0.03249930590391159 Top 1: 0.8172348484848485\n",
      "[Iter 3600] Loss: 0.03206910192966461 Top 1: 0.8191493055555555\n",
      "[Iter 3900] Loss: 0.03145100176334381 Top 1: 0.8226442307692308\n",
      "Loss: 0.03102106973528862 Top 1: 0.8252874940105415\n",
      "Total time: 41.66109090646108 in minutes\n",
      "{'initial_max_pool': <tf.Tensor 'model/StatefulPartitionedCall:6' shape=(4, 64, 64, 128) dtype=float32>, 'block_group1': <tf.Tensor 'model/StatefulPartitionedCall:0' shape=(4, 64, 64, 512) dtype=float32>, 'initial_conv': <tf.Tensor 'model/StatefulPartitionedCall:5' shape=(4, 128, 128, 128) dtype=float32>, 'block_group2': <tf.Tensor 'model/StatefulPartitionedCall:1' shape=(4, 32, 32, 1024) dtype=float32>, 'block_group4': <tf.Tensor 'model/StatefulPartitionedCall:3' shape=(4, 8, 8, 4096) dtype=float32>, 'final_avg_pool': <tf.Tensor 'model/StatefulPartitionedCall:4' shape=(4, 4096) dtype=float32>, 'logits_sup': <tf.Tensor 'model/StatefulPartitionedCall:7' shape=(4, 1000) dtype=float32>, 'block_group3': <tf.Tensor 'model/StatefulPartitionedCall:2' shape=(4, 16, 16, 2048) dtype=float32>}\n",
      "Variables to train: [<tf.Variable 'model/head_supervised_new/kernel:0' shape=(4096, 4) dtype=float32>, <tf.Variable 'model/head_supervised_new/bias:0' shape=(4,) dtype=float32>]\n",
      "[Iter 300] Loss: 0.02485422044992447 Top 1: 0.85421888053467\n",
      "[Iter 600] Loss: 0.02469629794359207 Top 1: 0.8563829787234043\n",
      "[Iter 900] Loss: 0.024270132184028625 Top 1: 0.859257714762302\n",
      "[Iter 1200] Loss: 0.02415855973958969 Top 1: 0.8611111111111112\n",
      "[Iter 1500] Loss: 0.02393900230526924 Top 1: 0.8618475904618976\n",
      "[Iter 1800] Loss: 0.023936089128255844 Top 1: 0.8627900514103098\n",
      "[Iter 2100] Loss: 0.023852158337831497 Top 1: 0.8633738239847565\n",
      "[Iter 2400] Loss: 0.023636173456907272 Top 1: 0.8649577993122851\n",
      "[Iter 2700] Loss: 0.02355581894516945 Top 1: 0.8653098082800778\n",
      "[Iter 3000] Loss: 0.023508505895733833 Top 1: 0.8651746269900809\n",
      "[Iter 3300] Loss: 0.023372622206807137 Top 1: 0.8664090323558384\n",
      "[Iter 3600] Loss: 0.02327568456530571 Top 1: 0.8669688129471418\n",
      "[Iter 3900] Loss: 0.023156266659498215 Top 1: 0.8674424568827339\n",
      "Loss: 0.02307944931089878 Top 1: 0.8677289881986462\n",
      "Total time: 41.53263524770737 in minutes\n",
      "[Iter 300] Loss: 0.02196469157934189 Top 1: 0.8751044277360067\n",
      "[Iter 600] Loss: 0.021960120648145676 Top 1: 0.8729662077596996\n",
      "[Iter 900] Loss: 0.021892402321100235 Top 1: 0.8747567417292188\n",
      "[Iter 1200] Loss: 0.021557265892624855 Top 1: 0.8763810715030227\n",
      "[Iter 1500] Loss: 0.021476199850440025 Top 1: 0.8768134067033517\n",
      "[Iter 1800] Loss: 0.021476436406373978 Top 1: 0.8769973600111157\n",
      "[Iter 2100] Loss: 0.02134217694401741 Top 1: 0.878528045730618\n",
      "[Iter 2400] Loss: 0.021229974925518036 Top 1: 0.8787641971449411\n",
      "[Iter 2700] Loss: 0.021259907633066177 Top 1: 0.8789478558858942\n",
      "[Iter 3000] Loss: 0.021209370344877243 Top 1: 0.879282320580145\n",
      "[Iter 3300] Loss: 0.02126099355518818 Top 1: 0.8790255361066909\n",
      "[Iter 3600] Loss: 0.021230295300483704 Top 1: 0.8787941932347016\n",
      "[Iter 3900] Loss: 0.021186506375670433 Top 1: 0.8790793101237417\n",
      "Loss: 0.021161561831831932 Top 1: 0.8795303420595459\n",
      "Total time: 41.41572014093399 in minutes\n",
      "[Iter 300] Loss: 0.020525464788079262 Top 1: 0.8868003341687553\n",
      "[Iter 600] Loss: 0.021028190851211548 Top 1: 0.8819357530246141\n",
      "[Iter 900] Loss: 0.02065856009721756 Top 1: 0.8831665276619405\n",
      "[Iter 1200] Loss: 0.020671015605330467 Top 1: 0.8816968938920159\n",
      "[Iter 1500] Loss: 0.02087736502289772 Top 1: 0.8808154077038519\n",
      "[Iter 1800] Loss: 0.020814523100852966 Top 1: 0.8815131304710296\n",
      "[Iter 2100] Loss: 0.020872751250863075 Top 1: 0.8814755269739193\n",
      "[Iter 2400] Loss: 0.020827068015933037 Top 1: 0.8820204230488694\n",
      "[Iter 2700] Loss: 0.02081402763724327 Top 1: 0.8818190238029082\n",
      "[Iter 3000] Loss: 0.02073873020708561 Top 1: 0.8823455863965991\n",
      "[Iter 3300] Loss: 0.020624903962016106 Top 1: 0.883041600363719\n",
      "[Iter 3600] Loss: 0.020526720210909843 Top 1: 0.884194623879975\n",
      "[Iter 3900] Loss: 0.02043435163795948 Top 1: 0.8847855356799385\n",
      "Loss: 0.02034446969628334 Top 1: 0.8852513029413527\n",
      "Total time: 41.47703853050868 in minutes\n",
      "[Iter 300] Loss: 0.020836414769291878 Top 1: 0.8778195488721805\n",
      "[Iter 600] Loss: 0.0204851683229208 Top 1: 0.8789111389236546\n",
      "[Iter 900] Loss: 0.02046167477965355 Top 1: 0.8814289685849319\n",
      "[Iter 1200] Loss: 0.020116936415433884 Top 1: 0.8843548050865124\n",
      "[Iter 1500] Loss: 0.019995640963315964 Top 1: 0.8853593463398366\n",
      "[Iter 1800] Loss: 0.020066553726792336 Top 1: 0.8845352230095873\n",
      "[Iter 2100] Loss: 0.0200762078166008 Top 1: 0.8850184589734429\n",
      "[Iter 2400] Loss: 0.0200340673327446 Top 1: 0.8856673960612691\n",
      "[Iter 2700] Loss: 0.019923577085137367 Top 1: 0.8862647031582848\n",
      "[Iter 3000] Loss: 0.01993374526500702 Top 1: 0.88615903975994\n",
      "[Iter 3300] Loss: 0.019939882680773735 Top 1: 0.8860157611578389\n",
      "[Iter 3600] Loss: 0.019840894266963005 Top 1: 0.886920886295756\n",
      "[Iter 3900] Loss: 0.019765270873904228 Top 1: 0.8876707059049818\n",
      "Loss: 0.019791768863797188 Top 1: 0.8876175642484874\n",
      "Total time: 41.42326165040334 in minutes\n",
      "[Iter 300] Loss: 0.01858043670654297 Top 1: 0.8936925647451963\n",
      "[Iter 600] Loss: 0.01864791102707386 Top 1: 0.894659991656237\n",
      "[Iter 900] Loss: 0.01912066899240017 Top 1: 0.8919933277731443\n",
      "[Iter 1200] Loss: 0.01933171972632408 Top 1: 0.8907650614967688\n",
      "[Iter 1500] Loss: 0.01925066113471985 Top 1: 0.8909037852259463\n",
      "[Iter 1800] Loss: 0.019464239478111267 Top 1: 0.8897109906905655\n",
      "[Iter 2100] Loss: 0.019664647057652473 Top 1: 0.8888293438132666\n",
      "[Iter 2400] Loss: 0.01968003250658512 Top 1: 0.8890278211941232\n",
      "[Iter 2700] Loss: 0.01961352303624153 Top 1: 0.8892979531351302\n",
      "[Iter 3000] Loss: 0.01961500011384487 Top 1: 0.8895765608068684\n",
      "[Iter 3300] Loss: 0.01956995204091072 Top 1: 0.890031825414867\n",
      "[Iter 3600] Loss: 0.019645579159259796 Top 1: 0.8894387719663819\n",
      "[Iter 3900] Loss: 0.01972007378935814 Top 1: 0.8889049176123613\n",
      "Loss: 0.019636807963252068 Top 1: 0.8899988018930091\n",
      "Total time: 42.15801742871602 in minutes\n",
      "[Iter 300] Loss: 0.0181043092161417 Top 1: 0.8966165413533834\n",
      "[Iter 600] Loss: 0.018391037359833717 Top 1: 0.8965373383395911\n",
      "[Iter 900] Loss: 0.01843046210706234 Top 1: 0.8960244648318043\n",
      "[Iter 1200] Loss: 0.01862071268260479 Top 1: 0.8942568271836564\n",
      "[Iter 1500] Loss: 0.018930280581116676 Top 1: 0.8925295981323995\n",
      "[Iter 1800] Loss: 0.01906392350792885 Top 1: 0.8913436153953036\n",
      "[Iter 2100] Loss: 0.01908997818827629 Top 1: 0.8910325116112897\n",
      "[Iter 2400] Loss: 0.01920074038207531 Top 1: 0.8905387100135459\n",
      "[Iter 2700] Loss: 0.01920325867831707 Top 1: 0.8903167546540706\n",
      "[Iter 3000] Loss: 0.01927926577627659 Top 1: 0.8899516545803118\n",
      "[Iter 3300] Loss: 0.0193328820168972 Top 1: 0.8898992195195878\n",
      "[Iter 3600] Loss: 0.019347216933965683 Top 1: 0.8897860665416406\n",
      "[Iter 3900] Loss: 0.019365837797522545 Top 1: 0.8896262101686222\n",
      "Loss: 0.01940569095313549 Top 1: 0.8896243934583359\n",
      "Total time: 42.607085486253105 in minutes\n",
      "[Iter 300] Loss: 0.01967855729162693 Top 1: 0.8863826232247285\n",
      "[Iter 600] Loss: 0.019561469554901123 Top 1: 0.8900709219858156\n",
      "[Iter 900] Loss: 0.019747711718082428 Top 1: 0.8881706978037254\n",
      "[Iter 1200] Loss: 0.01967059075832367 Top 1: 0.8892015843235356\n",
      "[Iter 1500] Loss: 0.019343657419085503 Top 1: 0.8906536601634151\n",
      "[Iter 1800] Loss: 0.019363796338438988 Top 1: 0.8902320411282478\n",
      "[Iter 2100] Loss: 0.019312456250190735 Top 1: 0.8905263784685007\n",
      "[Iter 2400] Loss: 0.019244715571403503 Top 1: 0.8907210586641658\n",
      "[Iter 2700] Loss: 0.019224554300308228 Top 1: 0.8908493099935167\n",
      "[Iter 3000] Loss: 0.019280623644590378 Top 1: 0.8909102275568892\n",
      "[Iter 3300] Loss: 0.019290033727884293 Top 1: 0.8908274607865424\n",
      "[Iter 3600] Loss: 0.019335126504302025 Top 1: 0.8906543029797874\n",
      "[Iter 3900] Loss: 0.019257016479969025 Top 1: 0.8908283644290569\n",
      "Loss: 0.019214538857340813 Top 1: 0.890837476786677\n",
      "Total time: 41.79569900830587 in minutes\n",
      "[Iter 300] Loss: 0.019267989322543144 Top 1: 0.8943191311612364\n",
      "[Iter 600] Loss: 0.0195076372474432 Top 1: 0.8903838130997079\n",
      "[Iter 900] Loss: 0.01962682045996189 Top 1: 0.8895607450653322\n",
      "[Iter 1200] Loss: 0.019594397395849228 Top 1: 0.8893058161350844\n",
      "[Iter 1500] Loss: 0.019590510055422783 Top 1: 0.8899449724862432\n",
      "[Iter 1800] Loss: 0.019456366077065468 Top 1: 0.8900583576490204\n",
      "[Iter 2100] Loss: 0.01932748220860958 Top 1: 0.890615696081934\n",
      "[Iter 2400] Loss: 0.01923108845949173 Top 1: 0.8912681046160258\n",
      "[Iter 2700] Loss: 0.01916859671473503 Top 1: 0.8914513290728906\n",
      "[Iter 3000] Loss: 0.019203467294573784 Top 1: 0.8914728682170543\n",
      "[Iter 3300] Loss: 0.019297420978546143 Top 1: 0.890846404485868\n",
      "[Iter 3600] Loss: 0.019147101789712906 Top 1: 0.8916961867055636\n",
      "[Iter 3900] Loss: 0.019211795181035995 Top 1: 0.8917099442200423\n",
      "Loss: 0.019128158688545227 Top 1: 0.8920655364524052\n",
      "Total time: 41.839725629488626 in minutes\n",
      "[Iter 300] Loss: 0.018671415746212006 Top 1: 0.9010025062656641\n",
      "[Iter 600] Loss: 0.018756970763206482 Top 1: 0.8983103879849812\n",
      "[Iter 900] Loss: 0.019026905298233032 Top 1: 0.895954962468724\n",
      "[Iter 1200] Loss: 0.019036132842302322 Top 1: 0.8940483635605587\n",
      "[Iter 1500] Loss: 0.01908729039132595 Top 1: 0.8936134734033684\n",
      "[Iter 1800] Loss: 0.019130120053887367 Top 1: 0.8933236070584966\n",
      "[Iter 2100] Loss: 0.01929434947669506 Top 1: 0.8928784089555794\n",
      "[Iter 2400] Loss: 0.01935173198580742 Top 1: 0.8923882463269772\n",
      "[Iter 2700] Loss: 0.01920679397881031 Top 1: 0.8930026859312772\n",
      "[Iter 3000] Loss: 0.019162220880389214 Top 1: 0.8928898891389514\n",
      "[Iter 3300] Loss: 0.019119396805763245 Top 1: 0.8931007047056149\n",
      "[Iter 3600] Loss: 0.019029047340154648 Top 1: 0.8935715774119608\n",
      "[Iter 3900] Loss: 0.018998287618160248 Top 1: 0.8936173623132654\n",
      "Loss: 0.01907750591635704 Top 1: 0.8930539747199425\n",
      "Total time: 41.82386209964752 in minutes\n",
      "[Iter 300] Loss: 0.019178925082087517 Top 1: 0.8930659983291562\n",
      "[Iter 600] Loss: 0.019041523337364197 Top 1: 0.8937213183145599\n",
      "[Iter 900] Loss: 0.019179444760084152 Top 1: 0.894564915207117\n",
      "[Iter 1200] Loss: 0.018826302140951157 Top 1: 0.8958724202626641\n",
      "[Iter 1500] Loss: 0.019030695781111717 Top 1: 0.894072036018009\n",
      "[Iter 1800] Loss: 0.01914055272936821 Top 1: 0.8929762401000417\n",
      "[Iter 2100] Loss: 0.01903059147298336 Top 1: 0.8929974991068239\n",
      "[Iter 2400] Loss: 0.01884768344461918 Top 1: 0.8939251849536314\n",
      "[Iter 2700] Loss: 0.018887169659137726 Top 1: 0.8934889321107715\n",
      "[Iter 3000] Loss: 0.018890380859375 Top 1: 0.8938484621155289\n",
      "[Iter 3300] Loss: 0.019020933657884598 Top 1: 0.8927407744184285\n",
      "[Iter 3600] Loss: 0.018990319222211838 Top 1: 0.8931374591928875\n",
      "[Iter 3900] Loss: 0.01894010230898857 Top 1: 0.8935051612489582\n",
      "Loss: 0.018916696310043335 Top 1: 0.8936380518780327\n",
      "Total time: 41.82310500939687 in minutes\n",
      "[Iter 300] Loss: 0.01747898757457733 Top 1: 0.9012113617376776\n",
      "[Iter 600] Loss: 0.01816655322909355 Top 1: 0.898101793909053\n",
      "[Iter 900] Loss: 0.01833389326930046 Top 1: 0.8971365026410898\n",
      "[Iter 1200] Loss: 0.0184415802359581 Top 1: 0.896914738378153\n",
      "[Iter 1500] Loss: 0.018450962379574776 Top 1: 0.8970318492579623\n",
      "[Iter 1800] Loss: 0.018579276278614998 Top 1: 0.8966930665555093\n",
      "[Iter 2100] Loss: 0.018632255494594574 Top 1: 0.8972847445516255\n",
      "[Iter 2400] Loss: 0.018578246235847473 Top 1: 0.8978066062311139\n",
      "[Iter 2700] Loss: 0.018694866448640823 Top 1: 0.8967537278873762\n",
      "[Iter 3000] Loss: 0.01872335560619831 Top 1: 0.8962032174710344\n",
      "[Iter 3300] Loss: 0.018705295398831367 Top 1: 0.8964347957869213\n",
      "[Iter 3600] Loss: 0.018864642828702927 Top 1: 0.8959679099812461\n",
      "[Iter 3900] Loss: 0.01888151466846466 Top 1: 0.8958453548759376\n",
      "Loss: 0.018887337297201157 Top 1: 0.8953303780027556\n",
      "Total time: 41.75905910730362 in minutes\n",
      "[Iter 300] Loss: 0.018858348950743675 Top 1: 0.8951545530492899\n",
      "[Iter 600] Loss: 0.018673518672585487 Top 1: 0.8971631205673759\n",
      "[Iter 900] Loss: 0.01882977783679962 Top 1: 0.896371976647206\n",
      "[Iter 1200] Loss: 0.0187480878084898 Top 1: 0.8961329997915364\n",
      "[Iter 1500] Loss: 0.018782706931233406 Top 1: 0.895781223945306\n",
      "[Iter 1800] Loss: 0.01875033974647522 Top 1: 0.8945741281089343\n",
      "[Iter 2100] Loss: 0.018858404830098152 Top 1: 0.8946945337620579\n",
      "[Iter 2400] Loss: 0.018962770700454712 Top 1: 0.8943419818693341\n",
      "[Iter 2700] Loss: 0.018954992294311523 Top 1: 0.8947624340094471\n",
      "[Iter 3000] Loss: 0.01892414130270481 Top 1: 0.8950362590647661\n",
      "[Iter 3300] Loss: 0.0189395509660244 Top 1: 0.895108736834129\n",
      "[Iter 3600] Loss: 0.018927179276943207 Top 1: 0.895169132458151\n",
      "[Iter 3900] Loss: 0.018926821649074554 Top 1: 0.895476694236071\n",
      "Loss: 0.018940286710858345 Top 1: 0.8957047864374289\n",
      "Total time: 41.76028788089752 in minutes\n",
      "[Iter 300] Loss: 0.020095042884349823 Top 1: 0.8876357560568087\n",
      "[Iter 600] Loss: 0.019353264942765236 Top 1: 0.8919482686691698\n",
      "[Iter 900] Loss: 0.019361764192581177 Top 1: 0.8924103419516264\n",
      "[Iter 1200] Loss: 0.01891985349357128 Top 1: 0.8934229726912654\n",
      "[Iter 1500] Loss: 0.01900128275156021 Top 1: 0.8933216608304152\n",
      "[Iter 1800] Loss: 0.018901674076914787 Top 1: 0.8947478115881617\n",
      "[Iter 2100] Loss: 0.018875330686569214 Top 1: 0.8948136239133023\n",
      "[Iter 2400] Loss: 0.018918244168162346 Top 1: 0.8950192768573513\n",
      "[Iter 2700] Loss: 0.018914975225925446 Top 1: 0.8951097527090859\n",
      "[Iter 3000] Loss: 0.01887897029519081 Top 1: 0.8953279986663333\n",
      "[Iter 3300] Loss: 0.01887228898704052 Top 1: 0.8951466242327801\n",
      "[Iter 3600] Loss: 0.01882278174161911 Top 1: 0.8952559561019657\n",
      "[Iter 3900] Loss: 0.018761398270726204 Top 1: 0.8957171250881579\n",
      "Loss: 0.01880403608083725 Top 1: 0.8955250703887857\n",
      "Total time: 41.836493996779126 in minutes\n",
      "[Iter 300] Loss: 0.01985420659184456 Top 1: 0.8955722639933166\n",
      "[Iter 600] Loss: 0.01938118226826191 Top 1: 0.8958072590738423\n",
      "[Iter 900] Loss: 0.019220463931560516 Top 1: 0.8954684459271616\n",
      "[Iter 1200] Loss: 0.019386786967515945 Top 1: 0.8943089430894309\n",
      "[Iter 1500] Loss: 0.019351154565811157 Top 1: 0.8954894113723528\n",
      "[Iter 1800] Loss: 0.019277073442935944 Top 1: 0.8963109629012088\n",
      "[Iter 2100] Loss: 0.01911640726029873 Top 1: 0.896659521257592\n",
      "[Iter 2400] Loss: 0.019028540700674057 Top 1: 0.8966343649056997\n",
      "[Iter 2700] Loss: 0.019061479717493057 Top 1: 0.8958275446883394\n",
      "[Iter 3000] Loss: 0.019087160006165504 Top 1: 0.8957447695257148\n",
      "[Iter 3300] Loss: 0.019011499360203743 Top 1: 0.8959043722058043\n",
      "[Iter 3600] Loss: 0.01906850002706051 Top 1: 0.8954643328471209\n",
      "[Iter 3900] Loss: 0.019002096727490425 Top 1: 0.8956369814707956\n",
      "Loss: 0.01920001395046711 Top 1: 0.8946564428203438\n",
      "Total time: 41.81709958314896 in minutes\n"
     ]
    }
   ],
   "source": [
    "run_model(model_r152, train_generator, epochs=30, training=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6646e851-0137-43fb-8557-a1d343388788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'initial_max_pool': <tf.Tensor 'model/StatefulPartitionedCall:6' shape=(16, 64, 64, 128) dtype=float32>, 'block_group1': <tf.Tensor 'model/StatefulPartitionedCall:0' shape=(16, 64, 64, 512) dtype=float32>, 'initial_conv': <tf.Tensor 'model/StatefulPartitionedCall:5' shape=(16, 128, 128, 128) dtype=float32>, 'block_group2': <tf.Tensor 'model/StatefulPartitionedCall:1' shape=(16, 32, 32, 1024) dtype=float32>, 'block_group4': <tf.Tensor 'model/StatefulPartitionedCall:3' shape=(16, 8, 8, 4096) dtype=float32>, 'final_avg_pool': <tf.Tensor 'model/StatefulPartitionedCall:4' shape=(16, 4096) dtype=float32>, 'logits_sup': <tf.Tensor 'model/StatefulPartitionedCall:7' shape=(16, 1000) dtype=float32>, 'block_group3': <tf.Tensor 'model/StatefulPartitionedCall:2' shape=(16, 16, 16, 2048) dtype=float32>}\n",
      "[Iter 300] Loss: 0.005195227917283773 Top 1: 0.9775\n",
      "[Iter 600] Loss: 0.011027905158698559 Top 1: 0.9439583333333333\n",
      "[Iter 900] Loss: 0.02199152484536171 Top 1: 0.8808333333333334\n",
      "Loss: 0.020067987963557243 Top 1: 0.8918983700862896\n",
      "Total time: 10.516391571362814 in minutes\n"
     ]
    }
   ],
   "source": [
    "run_model(model_r152, valid_generator, epochs=1, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bc4896-4a58-41fb-87b7-cfceca77688c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a46409e-4d9a-4a68-a084-314d63a32d21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
